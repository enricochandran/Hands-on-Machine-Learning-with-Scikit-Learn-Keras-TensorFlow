{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Motivasi dan Kelemahan Dimensionality Reduction**\n",
        "\n",
        "Banyak masalah Machine Learning melibatkan ribuan, bahkan jutaan, fitur untuk setiap instance pelatihan. Jumlah fitur yang banyak ini tidak hanya membuat pelatihan sangat lambat tetapi juga mempersulit pencarian solusi yang baik, yang dikenal sebagai curse of dimensionality.\n",
        "\n",
        "* **Motivasi Utama (Keuntungan):**\n",
        "    * **Mempercepat Pelatihan:** Mengurangi jumlah fitur dapat mengubah masalah yang tidak dapat ditangani menjadi masalah yang dapat ditangani.\n",
        "    * **Visualisasi Data (Data Viz):** Mengurangi dimensi menjadi dua atau tiga memungkinkan visualisasi data high-dimensional pada grafik, membantu mendeteksi pola seperti cluster. Visualisasi juga penting untuk mengkomunikasikan kesimpulan kepada non-ilmuwan data.\n",
        "    * **Filter Noise dan Detail Tidak Perlu:** Dalam beberapa kasus, mengurangi dimensi data pelatihan dapat menyaring noise dan detail yang tidak perlu, sehingga menghasilkan kinerja yang lebih tinggi.\n",
        "\n",
        "* **Kelemahan Utama (Kerugian):**\n",
        "    * **Kehilangan Informasi:** Pengurangan dimensi menyebabkan hilangnya informasi, mirip seperti kompresi gambar JPEG yang menurunkan kualitas. Ini mungkin menyebabkan sistem berkinerja sedikit lebih buruk.\n",
        "    * **Pipa (Pipeline) Lebih Kompleks:** Proses ini membuat pipeline Anda sedikit lebih kompleks dan lebih sulit untuk dipelihara.\n",
        "    * **Tidak Selalu Meningkatkan Performa:** Meskipun dapat mempercepat pelatihan, tidak selalu menghasilkan solusi yang lebih baik atau lebih sederhana; itu semua tergantung pada dataset.\n",
        "\n",
        "**The Curse of Dimensionality**\n",
        "\n",
        "Curse of dimensionality mengacu pada masalah yang muncul ketika bekerja dengan data high-dimensional. Intuisi manusia seringkali gagal dalam membayangkan ruang high-dimensional.\n",
        "\n",
        "* **Perilaku Aneh dalam Ruang Dimensi Tinggi:**\n",
        "    * Dalam unit hypercube 10.000 dimensi, kemungkinan titik acak berada kurang dari 0.001 dari batas adalah lebih dari 99.999999%. Artinya, sebagian besar titik di hypercube berdimensi tinggi sangat dekat dengan batas.\n",
        "    * Jarak rata-rata antara dua titik yang dipilih secara acak dalam hypercube 1.000.000 dimensi adalah sekitar 408.25. Ini menunjukkan bahwa ada banyak ruang dalam dimensi tinggi, membuat dataset high-dimensional cenderung sangat sparse (jarang).\n",
        "    * Sparse dataset berarti instance pelatihan kemungkinan besar akan berjauhan satu sama lain. Akibatnya, instance baru kemungkinan besar akan jauh dari instance pelatihan apa pun, membuat prediksi menjadi kurang dapat diandalkan karena didasarkan pada ekstrapolasi yang lebih besar.\n",
        "    * Semakin banyak dimensi yang dimiliki training set, semakin besar risiko overfitting.\n",
        "    * Untuk mengatasi curse of dimensionality dengan meningkatkan ukuran training set agar mencapai kepadatan yang cukup, jumlah instance pelatihan yang diperlukan tumbuh secara eksponensial dengan jumlah dimensi.\n",
        "\n",
        "**Main Approaches for Dimensionality Reduction**\n",
        "\n",
        "Dua pendekatan utama untuk mengurangi dimensi adalah Projection dan Manifold Learning.\n",
        "\n",
        "1.  **Projection:**\n",
        "    * Dalam banyak masalah dunia nyata, instance pelatihan tidak tersebar secara seragam di semua dimensi. Banyak fitur hampir konstan atau sangat berkorelasi. Akibatnya, semua instance pelatihan berada di dalam (atau dekat dengan) subspace berdimensi jauh lebih rendah dari ruang high-dimensional.\n",
        "    * Projection melibatkan proyeksi setiap instance pelatihan secara tegak lurus ke subspace ini untuk mendapatkan dataset berdimensi baru. Contohnya adalah mengurangi dataset 3D menjadi 2D dengan memproyeksikannya ke sebuah bidang (Gambar 8-2, 8-3).\n",
        "    * Namun, projection tidak selalu merupakan pendekatan terbaik, terutama untuk dataset yang \"melengkung\" seperti Swiss roll (Gambar 8-4). Memproyeksikan Swiss roll ke bidang datar akan \"memadatkan\" lapisan-lapisan yang berbeda. Yang diinginkan adalah \"membuka gulungan\" Swiss roll tersebut.\n",
        "\n",
        "2.  **Manifold Learning:**\n",
        "    * Swiss roll adalah contoh 2D manifold. Secara umum, d-dimensional manifold adalah bagian dari ruang n-dimensional ($d < n$) yang secara lokal menyerupai d-dimensional hyperplane.\n",
        "    * Banyak algoritma pengurangan dimensi bekerja dengan memodelkan manifold di mana instance pelatihan berada; ini disebut Manifold Learning.\n",
        "    * Ini bergantung pada manifold assumption (atau manifold hypothesis), yang menyatakan bahwa sebagian besar dataset high-dimensional di dunia nyata terletak dekat dengan manifold berdimensi jauh lebih rendah. Asumsi ini sering diamati secara empiris.\n",
        "    * Contoh MNIST: gambar digit tulisan tangan memiliki kesamaan yang membuatnya \"terjepit\" ke dalam manifold berdimensi lebih rendah.\n",
        "    * Manifold assumption seringkali disertai dengan asumsi implisit lain: bahwa tugas yang dihadapi (misalnya, klasifikasi) akan lebih sederhana jika diekspresikan dalam ruang dimensi yang lebih rendah dari manifold. Namun, asumsi ini tidak selalu berlaku.\n",
        "\n",
        "**Principal Component Analysis (PCA)**\n",
        "\n",
        "PCA adalah algoritma pengurangan dimensi paling populer.\n",
        "\n",
        "* **Cara Kerja PCA:**\n",
        "    * Mengidentifikasi hyperplane yang paling dekat dengan data.\n",
        "    * Memproyeksikan data ke hyperplane tersebut.\n",
        "* **Mempertahankan Variansi:**\n",
        "    * PCA memilih sumbu (atau hyperplane) yang mempertahankan jumlah variansi maksimum. Ini meminimalkan jarak kuadrat rata-rata antara dataset asli dan proyeksikannya.\n",
        "    * **Principal Components (PC):** PCA mengidentifikasi sumbu yang menjelaskan jumlah variansi terbesar dalam training set sebagai PC pertama. Kemudian menemukan sumbu kedua, ortogonal terhadap yang pertama, yang menjelaskan variansi sisa terbesar, dan seterusnya. Sumbu ke-i disebut i-th principal component (PC) dari data.\n",
        "    * Untuk setiap PC, PCA menemukan vektor satuan (unit vector) yang berpusat nol yang menunjuk ke arah PC.\n",
        "\n",
        "* **Menemukan Principal Components (SVD):**\n",
        "    * PCA menggunakan teknik faktorisasi matriks standar yang disebut Singular Value Decomposition (SVD) untuk menguraikan matriks training set X menjadi perkalian matriks U $\\Sigma$ V$^{T}$. Matriks V berisi vektor satuan yang mendefinisikan semua principal component.\n",
        "    * **Persamaan 8-1. Principal components matrix:**\n",
        "        $$V=\\begin{pmatrix} | & | & & | \\\\ c_{1} & c_{2} & \\cdots & c_{n} \\\\ | & | & & | \\end{pmatrix}$$\n",
        "        Di mana $c_1, c_2, \\dots, c_n$ adalah principal component.\n",
        "    * **Contoh Kode NumPy untuk PCA:**\n",
        "        ```python\n",
        "        X_centered = X - X.mean(axis=0) # Centering the data\n",
        "        U, s, Vt = np.linalg.svd(X_centered)\n",
        "        c1 = Vt.T[:, 0] # First Principal Component\n",
        "        c2 = Vt.T[:, 1] # Second Principal Component\n",
        "        ```\n",
        "        PCA mengasumsikan dataset terpusat di sekitar titik asal. Kelas `PCA` Scikit-Learn secara otomatis menangani pemusatan data.\n",
        "\n",
        "* **Projecting Down to d Dimensions:**\n",
        "    * Setelah mengidentifikasi semua principal component, dimensi dataset dapat dikurangi menjadi d dimensi dengan memproyeksikannya ke hyperplane yang ditentukan oleh d principal component pertama. Ini memastikan bahwa proyeksi mempertahankan variansi sebanyak mungkin.\n",
        "    * **Persamaan 8-2. Projecting the training set down to d dimensions:**\n",
        "        $$X_{d-proj} = XW_{d}$$\n",
        "        Di mana $W_d$ adalah matriks yang berisi d kolom pertama dari V.\n",
        "    * **Contoh Kode NumPy untuk Proyeksi:**\n",
        "        ```python\n",
        "        W2 = Vt.T[:, :2] # Matrix with the first two principal components\n",
        "        X2D = X_centered.dot(W2) # Projecting the data onto 2D\n",
        "        ```\n",
        "\n",
        "* **Menggunakan Scikit-Learn:**\n",
        "    * Kelas `PCA` Scikit-Learn menggunakan dekomposisi SVD untuk mengimplementasikan PCA.\n",
        "    * **Contoh Kode Scikit-Learn:**\n",
        "        ```python\n",
        "        from sklearn.decomposition import PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        X2D = pca.fit_transform(X)\n",
        "        ```\n",
        "    * Atribut `components_` dari objek PCA yang telah di-fit berisi transpose dari $W_d$.\n",
        "\n",
        "* **Explained Variance Ratio:**\n",
        "    * Atribut `explained_variance_ratio_` menunjukkan proporsi variansi dataset yang terletak di sepanjang setiap principal component. Ini membantu menentukan berapa banyak informasi yang dipertahankan. Contohnya, 84.2% variansi terletak di PC pertama dan 14.6% di PC kedua.\n",
        "\n",
        "* **Memilih Jumlah Dimensi yang Tepat:**\n",
        "    * Alih-alih memilih jumlah dimensi secara arbitrer, lebih baik memilih jumlah dimensi yang menjumlahkan bagian variansi yang cukup besar (misalnya, 95%). Untuk visualisasi data, biasanya 2 atau 3 dimensi.\n",
        "    * **Contoh Kode untuk Memilih Dimensi:**\n",
        "        ```python\n",
        "        pca = PCA()\n",
        "        pca.fit(X_train)\n",
        "        cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "        d = np.argmax(cumsum >= 0.95) + 1 # Min dimensions to preserve 95% variance\n",
        "        ```\n",
        "    * Atau, dapat mengatur `n_components` sebagai float antara 0.0 dan 1.0, menunjukkan rasio variansi yang ingin dipertahankan:\n",
        "        ```python\n",
        "        pca = PCA(n_components=0.95)\n",
        "        X_reduced = pca.fit_transform(X_train)\n",
        "        ```\n",
        "    * Visualisasi kurva `explained_variance` sebagai fungsi jumlah dimensi dapat menunjukkan \"siku\" (elbow) di mana variansi yang dijelaskan berhenti tumbuh dengan cepat.\n",
        "\n",
        "* **PCA untuk Kompresi:**\n",
        "    * Setelah pengurangan dimensi, training set membutuhkan lebih sedikit ruang. Contohnya, dataset MNIST dapat dikompresi menjadi sekitar 150 fitur dari 784 sambil mempertahankan 95% variansi. Ini dapat mempercepat algoritma klasifikasi.\n",
        "    * Dataset yang direduksi dapat didekompresi kembali ke dimensi aslinya menggunakan transformasi invers PCA. Ini tidak akan mengembalikan data asli persis karena adanya kehilangan informasi, tetapi akan sangat mirip.\n",
        "    * Jarak kuadrat rata-rata antara data asli dan data yang direkonstruksi disebut reconstruction error.\n",
        "    * **Persamaan 8-3. PCA inverse transformation:**\n",
        "        $$X_{recovered} = X_{d-proj} W_{d}^{\\top}$$\n",
        "    * **Contoh Kode untuk Kompresi dan Dekompresi:**\n",
        "        ```python\n",
        "        pca = PCA(n_components=154)\n",
        "        X_reduced = pca.fit_transform(X_train)\n",
        "        X_recovered = pca.inverse_transform(X_reduced)\n",
        "        ```\n",
        "\n",
        "* **Randomized PCA:**\n",
        "    * Menggunakan algoritma stokastik yang cepat menemukan perkiraan d principal component pertama. Kompleksitas komputasinya adalah $O(m \\times d^2) + O(d^3)$, yang jauh lebih cepat daripada SVD penuh ketika d jauh lebih kecil dari n.\n",
        "    * **Contoh Kode:**\n",
        "        ```python\n",
        "        rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
        "        X_reduced = rnd_pca.fit_transform(X_train)\n",
        "        ```\n",
        "    * Secara default, `svd_solver` diatur ke \"auto\" di Scikit-Learn; ini akan menggunakan randomized PCA jika m atau n lebih besar dari 500 dan d kurang dari 80% dari m atau n, jika tidak, ia menggunakan pendekatan SVD penuh.\n",
        "\n",
        "* **Incremental PCA (IPCA):**\n",
        "    * Mengatasi masalah bahwa implementasi PCA sebelumnya membutuhkan seluruh training set agar muat dalam memori. IPCA memungkinkan pembagian training set menjadi mini-batch dan memprosesnya satu per satu.\n",
        "    * Berguna untuk training set yang besar dan untuk menerapkan PCA secara online.\n",
        "    * **Contoh Kode:**\n",
        "        ```python\n",
        "        from sklearn.decomposition import IncrementalPCA\n",
        "        n_batches = 100\n",
        "        inc_pca = IncrementalPCA(n_components=154)\n",
        "        for X_batch in np.array_split(X_train, n_batches):\n",
        "            inc_pca.partial_fit(X_batch)\n",
        "        X_reduced = inc_pca.transform(X_train)\n",
        "        ```\n",
        "    * Alternatifnya, dapat menggunakan kelas `memmap` NumPy untuk memanipulasi array besar yang disimpan di disk seolah-olah sepenuhnya dalam memori. Ini memungkinkan pemanggilan metode `fit()` biasa.\n",
        "\n",
        "**Kernel PCA (kPCA)**\n",
        "\n",
        "* Menggunakan kernel trick untuk melakukan proyeksi nonlinier yang kompleks untuk pengurangan dimensi.\n",
        "* Seringkali baik dalam mempertahankan cluster instance setelah proyeksi, atau bahkan \"membuka gulungan\" dataset yang terletak dekat dengan manifold yang melengkung.\n",
        "* **Contoh Kode:**\n",
        "    ```python\n",
        "    from sklearn.decomposition import KernelPCA\n",
        "    rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
        "    X_reduced = rbf_pca.fit_transform(X)\n",
        "    ```\n",
        "* **Memilih Kernel dan Menyetel Hyperparameter:**\n",
        "    * Sebagai algoritma pembelajaran tanpa pengawasan (unsupervised learning), tidak ada ukuran kinerja yang jelas. Namun, jika pengurangan dimensi adalah langkah persiapan untuk tugas pembelajaran dengan pengawasan (supervised learning), grid search dapat digunakan untuk memilih kernel dan hyperparameter yang menghasilkan kinerja terbaik pada tugas tersebut.\n",
        "    * **Contoh Kode Pipeline dan GridSearchCV:**\n",
        "        ```python\n",
        "        from sklearn.model_selection import GridSearchCV\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "        from sklearn.pipeline import Pipeline\n",
        "        clf = Pipeline([\n",
        "            (\"kpca\", KernelPCA(n_components=2)),\n",
        "            (\"log_reg\", LogisticRegression())\n",
        "        ])\n",
        "        param_grid = [{\n",
        "            \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
        "            \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
        "        }]\n",
        "        grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
        "        grid_search.fit(X, y)\n",
        "        print(grid_search.best_params_) # {'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n",
        "        ```\n",
        "    * Pendekatan unsupervised lainnya adalah memilih kernel dan hyperparameter yang menghasilkan reconstruction error terendah. Namun, rekonstruksi dengan kPCA tidak semudah PCA linier. Titik yang direkonstruksi terletak di feature space (mungkin infinite-dimensional), bukan ruang asli.\n",
        "    * Dimungkinkan untuk menemukan titik di ruang asli yang akan memetakan dekat dengan titik yang direkonstruksi, yang disebut reconstruction pre-image. Errornya disebut reconstruction pre-image error.\n",
        "    * Scikit-Learn dapat melakukan rekonstruksi ini jika `fit_inverse_transform=True` diatur.\n",
        "    * **Contoh Kode Rekonstruksi Pre-image:**\n",
        "        ```python\n",
        "        rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.043,\n",
        "                            fit_inverse_transform=True)\n",
        "        X_reduced = rbf_pca.fit_transform(X)\n",
        "        X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
        "        from sklearn.metrics import mean_squared_error\n",
        "        print(mean_squared_error(X, X_preimage))\n",
        "        ```\n",
        "\n",
        "**Locally Linear Embedding (LLE)**\n",
        "\n",
        "* Teknik pengurangan dimensi nonlinier (NLDR) yang kuat dan merupakan teknik Manifold Learning yang tidak bergantung pada proyeksi.\n",
        "* **Cara Kerja LLE:**\n",
        "    1.  Mengukur bagaimana setiap instance pelatihan berhubungan secara linier dengan tetangga terdekatnya. Ini melibatkan pencarian bobot $w_{i,j}$ sehingga jarak kuadrat antara $X^{(i)}$ dan $\\sum_{j=1}^{m}w_{i,j}x^{(j)}$ sekecil mungkin, dengan $w_{i,j}=0$ jika $X^{(j)}$ bukan tetangga terdekat.\n",
        "        * **Persamaan 8-4. LLE step one: linearly modeling local relationships:**\n",
        "            $$\\hat{W} = \\text{argmin}_W \\sum_{i=1}^m \\left(x^{(i)} - \\sum_{j=1}^m w_{i,j}x^{(j)}\\right)^2$$\n",
        "            Subject to:\n",
        "            $$w_{i,j} = 0 \\quad \\text{if } x^{(j)} \\text{ is not one of the k c.n. of } x^{(i)}$$           $$\\sum_{j=1}^m w_{i,j} = 1 \\quad \\text{for } i = 1,2,..., m$$\n",
        "    2.  Memetakan instance pelatihan ke ruang d-dimensional ($d < n$) sambil mempertahankan hubungan lokal ini sebanyak mungkin.\n",
        "        * **Persamaan 8-5. LLE step two: reducing dimensionality while preserving relationships:**\n",
        "            $$\\hat{Z} = \\text{argmin} \\sum_{i=1}^m \\left(z^{(i)} - \\sum_{j=1}^m \\hat{w}_{i,j}z^{(j)}\\right)^2$$\n",
        "* Sangat baik dalam \"membuka gulungan\" manifold yang melengkung, terutama jika tidak ada terlalu banyak noise.\n",
        "* **Contoh Kode Scikit-Learn:**\n",
        "    ```python\n",
        "    from sklearn.manifold import LocallyLinearEmbedding\n",
        "    lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "    X_reduced = lle.fit_transform(X)\n",
        "    ```\n",
        "* Kompleksitas komputasi LLE: $O(m \\log(m)n \\log(k))$ untuk menemukan tetangga terdekat, $O(mnk^3)$ untuk mengoptimalkan bobot, dan $O(dm^2)$ untuk membangun representasi dimensi rendah. Faktor $m^2$ dalam suku terakhir menyebabkan algoritma ini tidak scale dengan baik untuk dataset yang sangat besar.\n",
        "\n",
        "**Teknik Pengurangan Dimensi Lainnya**\n",
        "\n",
        "Beberapa teknik populer lainnya yang tersedia di Scikit-Learn:\n",
        "\n",
        "* **Random Projections:** Memproyeksikan data ke ruang berdimensi lebih rendah menggunakan proyeksi linier acak. Ini kemungkinan besar mempertahankan jarak dengan baik, didukung oleh lemma Johnson-Lindenstrauss. Kualitas pengurangan dimensi tergantung pada jumlah instance dan dimensi target, tetapi tidak pada dimensi awal.\n",
        "* **Multidimensional Scaling (MDS):** Mengurangi dimensi sambil mencoba mempertahankan jarak antara instance.\n",
        "* **Isomap:** Membuat grafik dengan menghubungkan setiap instance ke tetangga terdekatnya, kemudian mengurangi dimensi sambil mencoba mempertahankan jarak geodesic antara instance.\n",
        "* **t-Distributed Stochastic Neighbor Embedding (t-SNE):** Mengurangi dimensi sambil mencoba menjaga instance yang serupa tetap dekat dan instance yang tidak serupa terpisah. Paling sering digunakan untuk visualisasi, terutama untuk memvisualisasikan cluster instance dalam ruang high-dimensional (misalnya, gambar MNIST dalam 2D).\n",
        "* **Linear Discriminant Analysis (LDA):** Merupakan algoritma klasifikasi, tetapi selama pelatihan, ia mempelajari sumbu paling diskriminatif antara kelas-kelas, dan sumbu-sumbu ini kemudian dapat digunakan untuk mendefinisikan hyperplane tempat data akan diproyeksikan. Keuntungan LDA adalah proyeksi akan menjaga kelas-kelas sejauh mungkin, sehingga LDA adalah teknik yang baik untuk mengurangi dimensi sebelum menjalankan algoritma klasifikasi lain."
      ],
      "metadata": {
        "id": "PdU8UoxzuRE-"
      }
    }
  ]
}