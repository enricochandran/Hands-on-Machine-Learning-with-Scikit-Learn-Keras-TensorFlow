{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**\n",
        "Model Linear Regression memprediksi nilai dengan menjumlahkan fitur input yang telah diberi bobot, ditambah dengan *bias term* (atau *intercept term*). Persamaan prediksinya adalah $\\hat{y}=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+\\cdot\\cdot\\cdot+\\theta_{n}x_{n}$. Dalam bentuk vektor, ini menjadi $\\hat{y}=h_{\\theta}(x)=\\theta\\cdot x$.\n",
        "\n",
        "Untuk melatih model Linear Regression, tujuannya adalah menemukan parameter $\\theta$ yang meminimalkan *Mean Squared Error* (MSE). Fungsi biaya MSE untuk Linear Regression adalah $MSE(X,h_{\\theta})=\\frac{1}{m}\\sum_{i=1}^{m}(\\theta^{T}x^{(i)}-y^{(i)})^{2}$.\n",
        "\n",
        "Ada dua cara utama untuk melatih model Linear Regression:\n",
        "\n",
        "1.  **Normal Equation**: Ini adalah solusi *closed-form* yang secara langsung menghitung parameter model yang meminimalkan fungsi biaya. Persamaannya adalah $\\hat{\\theta}=(X^{\\top}X)^{-1}X^{\\top}y$.\n",
        "    * **Contoh Kode**:\n",
        "        ```python\n",
        "        import numpy as np\n",
        "\n",
        "        X = 2 * np.random.rand(100, 1)\n",
        "        y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "        X_b = np.c_[np.ones((100, 1)), X] # Menambahkan x0=1 ke setiap instance\n",
        "        theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "        print(theta_best)\n",
        "        ```\n",
        "        Kode ini menghasilkan data linear, menambahkan *bias term* ($x_0=1$) ke setiap *instance* X, dan kemudian menghitung `theta_best` (parameter optimal) menggunakan Normal Equation. Output `theta_best` akan mendekati `[[4.], [3.]]` meskipun ada *noise*.\n",
        "\n",
        "    * **Kompleksitas Komputasi**: Menghitung invers dari matriks $X^TX$ (ukuran $(n+1) \\times (n+1)$) memiliki kompleksitas sekitar $O(n^{2.4})$ hingga $O(n^3)$. Ini menjadi sangat lambat ketika jumlah fitur ($n$) sangat besar (misalnya, 100.000). Namun, Normal Equation dan SVD (Singular Value Decomposition, yang digunakan oleh `LinearRegression` Scikit-Learn) bersifat linear terhadap jumlah *instance* pelatihan ($O(m)$), sehingga efisien untuk *dataset* pelatihan besar selama mereka dapat masuk ke memori.\n",
        "\n",
        "2.  **Gradient Descent (GD)**: Ini adalah pendekatan optimasi iteratif yang secara bertahap menyesuaikan parameter model untuk meminimalkan fungsi biaya.\n",
        "    * **Ide Umum**: Algoritma ini mengukur gradien lokal dari fungsi *error* terhadap vektor parameter $\\theta$ dan bergerak ke arah gradien yang menurun. Ketika gradien nol, minimum telah tercapai.\n",
        "    * **Learning Rate ($\\eta$)**: Ukuran langkah ditentukan oleh *learning rate*. Jika terlalu kecil, konvergensi akan lambat. Jika terlalu besar, algoritma dapat menyimpang.\n",
        "    * **Jenis-jenis Gradient Descent**:\n",
        "        * **Batch Gradient Descent**: Menghitung gradien berdasarkan seluruh *training set* pada setiap langkah. Ini lambat pada *training set* yang sangat besar, tetapi skalanya baik dengan jumlah fitur. Persamaan turunan parsial dari fungsi biaya terhadap parameter $\\theta_j$ adalah $\\frac{\\partial}{\\partial\\theta_{j}}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^{m}(\\theta^{T}x^{(i)}-y^{(i)})x_{j}^{(i)}$. Vektor gradiennya adalah $\\nabla_{\\theta}MSE(\\theta)$. Langkah *update* parameter adalah $\\theta^{(\\text{next step})} = \\theta - \\eta \\nabla_{\\theta}MSE(\\theta)$.\n",
        "            * **Contoh Kode Batch GD**:\n",
        "                ```python\n",
        "                eta = 0.1 # learning rate\n",
        "                n_iterations = 1000\n",
        "                m = 100\n",
        "                theta = np.random.randn(2,1) # inisialisasi random\n",
        "\n",
        "                for iteration in range(n_iterations):\n",
        "                    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "                    theta = theta - eta * gradients\n",
        "                print(theta)\n",
        "                ```\n",
        "                Kode ini mengimplementasikan Batch Gradient Descent dan akan menghasilkan `theta` yang sangat mirip dengan yang ditemukan oleh Normal Equation.\n",
        "        * **Stochastic Gradient Descent (SGD)**: Memilih *instance* acak dari *training set* pada setiap langkah dan menghitung gradien hanya berdasarkan *instance* tunggal itu. Ini jauh lebih cepat karena memanipulasi sangat sedikit data per iterasi dan memungkinkan pelatihan pada *training set* yang sangat besar (*out-of-core*). Namun, karena sifatnya yang stokastik, fungsi biaya akan memantul naik-turun dan tidak akan benar-benar menetap di minimum. Randomness ini dapat membantu keluar dari *local minima*. Untuk konvergensi yang lebih baik, *learning rate* harus dikurangi secara bertahap (*learning schedule*).\n",
        "            * **Contoh Kode SGD**:\n",
        "                ```python\n",
        "                n_epochs = 50\n",
        "                t0, t1 = 5, 50 # hyperparameters learning schedule\n",
        "\n",
        "                def learning_schedule(t):\n",
        "                    return t0 / (t + t1)\n",
        "\n",
        "                theta = np.random.randn(2,1) # inisialisasi random\n",
        "\n",
        "                for epoch in range(n_epochs):\n",
        "                    for i in range(m):\n",
        "                        random_index = np.random.randint(m)\n",
        "                        xi = X_b[random_index:random_index+1]\n",
        "                        yi = y[random_index:random_index+1]\n",
        "                        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "                        eta = learning_schedule(epoch * m + i)\n",
        "                        theta = theta - eta * gradients\n",
        "                print(theta)\n",
        "                ```\n",
        "                Kode ini menunjukkan implementasi SGD dengan *learning schedule* sederhana.\n",
        "        * **Mini-batch Gradient Descent**: Menghitung gradien pada kumpulan *instance* acak yang lebih kecil (*mini-batches*). Ini memanfaatkan optimasi perangkat keras untuk operasi matriks (terutama GPU) dan kemajuannya lebih stabil dibandingkan SGD, tetapi mungkin lebih sulit untuk keluar dari *local minima*.\n",
        "\n",
        "**Polynomial Regression**\n",
        "Polynomial Regression memungkinkan model linear untuk menyesuaikan data non-linear dengan menambahkan pangkat dari setiap fitur sebagai fitur baru.\n",
        "* **Contoh Kode**:\n",
        "    ```python\n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    import numpy as np\n",
        "\n",
        "    m = 100\n",
        "    X = 6 * np.random.rand(m, 1) - 3\n",
        "    y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "    poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    X_poly = poly_features.fit_transform(X)\n",
        "    print(X[0])\n",
        "    print(X_poly[0])\n",
        "\n",
        "    lin_reg = LinearRegression()\n",
        "    lin_reg.fit(X_poly, y)\n",
        "    print(lin_reg.intercept_, lin_reg.coef_)\n",
        "    ```\n",
        "    Kode ini menghasilkan data non-linear, menambahkan fitur kuadratnya (derajat 2) untuk membentuk `X_poly`, dan kemudian melatih model `LinearRegression` pada `X_poly`.\n",
        "\n",
        "**Learning Curves**\n",
        "*Learning curves* adalah plot kinerja model pada *training set* dan *validation set* sebagai fungsi dari ukuran *training set*. Ini membantu mendeteksi *overfitting* atau *underfitting*.\n",
        "* **Underfitting**: Kedua kurva (pelatihan dan validasi) mencapai *plateau*, dekat satu sama lain, dan relatif tinggi. Menambahkan lebih banyak contoh pelatihan tidak akan membantu.\n",
        "* **Overfitting**: *Error* pada data pelatihan jauh lebih rendah daripada *error* pada data validasi, menunjukkan adanya kesenjangan antara kedua kurva. Menambahkan lebih banyak data pelatihan dapat membantu mengurangi *overfitting*.\n",
        "\n",
        "**Bias/Variance Trade-off**\n",
        "*Generalization error* model dapat dipecah menjadi tiga komponen:\n",
        "* **Bias**: Kesalahan karena asumsi yang salah (misalnya, mengasumsikan data linear padahal kuadratik). Model dengan *high-bias* cenderung *underfit*.\n",
        "* **Variance**: Kesalahan karena sensitivitas berlebihan model terhadap variasi kecil dalam data pelatihan. Model dengan banyak *degrees of freedom* (misalnya, polinomial tingkat tinggi) cenderung memiliki *high variance* dan *overfit*.\n",
        "* **Irreducible error**: Kesalahan karena *noise* dalam data itu sendiri.\n",
        "\n",
        "Meningkatkan kompleksitas model biasanya meningkatkan *variance* dan mengurangi *bias*, dan sebaliknya. Ini disebut *trade-off*.\n",
        "\n",
        "**Regularized Linear Models**\n",
        "Regularisasi adalah cara untuk mengurangi *overfitting* dengan membatasi model. Untuk model linear, ini biasanya dicapai dengan membatasi bobot model.\n",
        "\n",
        "1.  **Ridge Regression (L2 Regularization)**: Menambahkan *regularization term* $\\alpha\\frac{1}{2}\\Sigma_{i=1}^{n}\\theta_{i}^{2}$ ke fungsi biaya MSE. Ini memaksa algoritma pembelajaran untuk menjaga bobot model sekecil mungkin. *Hyperparameter* $\\alpha$ mengontrol kekuatan regularisasi. $\\theta_0$ (bias term) tidak diregularisasi. Penting untuk menskalakan data sebelum melakukan Ridge Regression.\n",
        "    * **Contoh Kode**:\n",
        "        ```python\n",
        "        from sklearn.linear_model import Ridge\n",
        "        ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
        "        ridge_reg.fit(X, y)\n",
        "        print(ridge_reg.predict([[1.5]]))\n",
        "\n",
        "        from sklearn.linear_model import SGDRegressor\n",
        "        sgd_reg = SGDRegressor(penalty=\"l2\") # Menggunakan l2 regularization\n",
        "        sgd_reg.fit(X, y.ravel())\n",
        "        print(sgd_reg.predict([[1.5]]))\n",
        "        ```\n",
        "        `Ridge` menggunakan solusi *closed-form*, sedangkan `SGDRegressor` dengan `penalty=\"l2\"` menggunakan Gradient Descent dengan regularisasi L2.\n",
        "\n",
        "2.  **Lasso Regression (L1 Regularization)**: Menambahkan *regularization term* $\\alpha\\Sigma_{i=1}^{n}|\\theta_{i}|$ ke fungsi biaya. Ciri penting dari Lasso Regression adalah ia cenderung menghilangkan bobot fitur yang paling tidak penting (mengaturnya menjadi nol), sehingga melakukan seleksi fitur otomatis dan menghasilkan model yang *sparse*.\n",
        "    * **Contoh Kode**:\n",
        "        ```python\n",
        "        from sklearn.linear_model import Lasso\n",
        "        lasso_reg = Lasso(alpha=0.1)\n",
        "        lasso_reg.fit(X, y)\n",
        "        print(lasso_reg.predict([[1.5]]))\n",
        "        ```\n",
        "        Sama seperti Ridge, `SGDRegressor` juga dapat digunakan dengan `penalty=\"l1\"`.\n",
        "\n",
        "3.  **Elastic Net**: Gabungan antara Ridge dan Lasso Regression. *Regularization term* adalah campuran dari kedua istilah regularisasi tersebut, dikontrol oleh rasio campuran $r$. Ketika $r=0$, Elastic Net setara dengan Ridge Regression, dan ketika $r=1$, setara dengan Lasso Regression. Secara umum, Elastic Net lebih disukai daripada Lasso karena Lasso dapat berperilaku tidak menentu ketika jumlah fitur lebih besar dari jumlah *instance* pelatihan atau ketika beberapa fitur sangat berkorelasi.\n",
        "    * **Contoh Kode**:\n",
        "        ```python\n",
        "        from sklearn.linear_model import ElasticNet\n",
        "        elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5) # l1_ratio adalah r\n",
        "        elastic_net.fit(X, y)\n",
        "        print(elastic_net.predict([[1.5]]))\n",
        "        ```\n",
        "\n",
        "**Early Stopping**\n",
        "*Early stopping* adalah cara lain untuk meregularisasi algoritma pembelajaran iteratif seperti Gradient Descent. Ini melibatkan penghentian pelatihan segera setelah *validation error* mencapai minimumnya. Ketika *error* validasi mulai naik, itu menunjukkan bahwa model telah mulai *overfit* data pelatihan.\n",
        "* **Contoh Kode**:\n",
        "    ```python\n",
        "    from sklearn.base import clone\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "    from sklearn.linear_model import SGDRegressor\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    import numpy as np\n",
        "\n",
        "    # Data persiapan (contoh sederhana)\n",
        "    X = 2 * np.random.rand(100, 1)\n",
        "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "    X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "    sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                           penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "    minimum_val_error = float(\"inf\")\n",
        "    best_epoch = None\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(1000):\n",
        "        sgd_reg.fit(X_train_poly_scaled, y_train.ravel()) # .ravel() needed for y\n",
        "        y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "        val_error = mean_squared_error(y_val, y_val_predict)\n",
        "\n",
        "        if val_error < minimum_val_error:\n",
        "            minimum_val_error = val_error\n",
        "            best_epoch = epoch\n",
        "            best_model = clone(sgd_reg)\n",
        "    print(f\"Best epoch: {best_epoch}, Minimum validation error: {np.sqrt(minimum_val_error)}\")\n",
        "    # The best_model now holds the model state at the minimum validation error\n",
        "    ```\n",
        "    Dalam kode ini, `warm_start=True` memungkinkan metode `fit()` untuk melanjutkan pelatihan dari keadaan sebelumnya, bukan memulai dari awal.\n",
        "\n",
        "**Logistic Regression**\n",
        "Logistic Regression digunakan untuk memperkirakan probabilitas suatu *instance* termasuk dalam kelas tertentu (klasifikasi biner).\n",
        "* **Estimasi Probabilitas**: Model menghitung jumlah fitur input yang diberi bobot (ditambah *bias term*), kemudian menghasilkan *logistic* (fungsi sigmoid) dari hasil ini. Fungsi *logistic* adalah $\\sigma(t)=\\frac{1}{1+exp(-t)}$.\n",
        "* **Prediksi**: Jika probabilitas yang diperkirakan ($\\hat{p}$) lebih besar dari 50%, model memprediksi *instance* tersebut termasuk dalam kelas positif (label \"1\"), jika tidak, kelas negatif (label \"0\").\n",
        "* **Fungsi Biaya**: Fungsi biaya untuk satu *instance* pelatihan adalah $c(\\theta)=\\begin{cases}-log(\\hat{p})&if~y=1\\\\ -log(1-\\hat{p})if~y=0\\end{cases}$. Untuk seluruh *training set*, fungsi biayanya adalah *log loss* (Cross Entropy) $J(\\theta)=-\\frac{1}{m}\\Sigma_{i=1}^{m}[y^{(i)}log(\\hat{p}^{(i)})+(1-y^{(i)})log(1-\\hat{p}^{(i)})]$.\n",
        "* **Pelatihan**: Tidak ada solusi *closed-form* (seperti Normal Equation) untuk Logistic Regression, tetapi fungsi biaya bersifat konveks, sehingga Gradient Descent dapat menemukan minimum global. Turunan parsial dari fungsi biaya adalah $\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(\\theta^{T}x^{(i)})-y^{(i)})x_{j}^{(i)}$.\n",
        "* **Decision Boundaries**: Logistic Regression menghasilkan *decision boundary* linear.\n",
        "* **Regularisasi**: Scikit-Learn `LogisticRegression` secara default menambahkan penalti $l_2$. *Hyperparameter* `C` mengontrol kekuatan regularisasi, di mana nilai `C` yang lebih tinggi berarti lebih sedikit regularisasi.\n",
        "\n",
        "**Softmax Regression (Multinomial Logistic Regression)**\n",
        "Softmax Regression adalah generalisasi Logistic Regression untuk mendukung banyak kelas secara langsung.\n",
        "* **Cara Kerja**: Model menghitung skor $s_k(x)$ untuk setiap kelas $k$ (mirip dengan prediksi Linear Regression: $s_k(x) = x^T\\theta^{(k)}$). Kemudian, probabilitas setiap kelas diperkirakan dengan menerapkan fungsi *softmax* ke skor tersebut. Fungsi *softmax* adalah $\\hat{P}_{k}=\\sigma(s(x))_{k}=\\frac{exp(s_{k}(x))}{\\Sigma_{j=1}^{K}exp(s_{j}(x))}$.\n",
        "* **Prediksi**: Klasifier Softmax Regression memprediksi kelas dengan probabilitas tertinggi (atau skor tertinggi). Ini hanya dapat digunakan dengan kelas-kelas yang saling eksklusif.\n",
        "* **Fungsi Biaya**: Tujuannya adalah meminimalkan fungsi biaya *cross entropy* $J(\\Theta)=-\\frac{1}{m}\\Sigma_{i=1}^{m}\\Sigma_{k=1}^{K}y_{k}^{(i)}log(\\hat{p}_{k}^{(i)})$. *Gradient vector* untuk kelas $k$ adalah $\\nabla_{\\theta^{(k)}}J(\\Theta)=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_{k}^{(i)}-y_{k}^{(i)})x^{(i)}$.\n",
        "* **Contoh Kode**:\n",
        "    ```python\n",
        "    from sklearn import datasets\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import numpy as np\n",
        "\n",
        "    iris = datasets.load_iris()\n",
        "    X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
        "    y = iris[\"target\"]\n",
        "\n",
        "    softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
        "    softmax_reg.fit(X, y)\n",
        "\n",
        "    print(softmax_reg.predict([[5, 2]]))\n",
        "    print(softmax_reg.predict_proba([[5, 2]]))\n",
        "    ```\n",
        "    Ini melatih model Softmax Regression pada *iris dataset* untuk mengklasifikasikan bunga ke dalam tiga spesies. `multi_class=\"multinomial\"` dan `solver=\"lbfgs\"` diatur untuk mengaktifkan Softmax Regression.\n",
        "\n",
        "**Latihan dan Penjelasan Teoritis**\n",
        "\n",
        "1.  **Algoritma Pelatihan Linear Regression yang dapat digunakan dengan *training set* yang memiliki jutaan fitur?**\n",
        "    * Algoritma yang cocok adalah **Gradient Descent (GD) variants** seperti **Batch Gradient Descent**, **Stochastic Gradient Descent (SGD)**, atau **Mini-batch Gradient Descent**.\n",
        "    * **Penjelasan Teoritis**: Normal Equation dan SVD (digunakan oleh `LinearRegression` Scikit-Learn) melibatkan inversi matriks $X^TX$ yang memiliki kompleksitas komputasi $O(n^{2.4})$ hingga $O(n^3)$ di mana $n$ adalah jumlah fitur. Ini menjadi sangat lambat ketika $n$ besar (misalnya, 100.000). Sebaliknya, Gradient Descent skalanya baik dengan jumlah fitur; melatih model Linear Regression dengan ratusan ribu fitur jauh lebih cepat menggunakan Gradient Descent dibandingkan Normal Equation atau dekomposisi SVD. SGD dan Mini-batch GD bahkan memiliki dukungan *out-of-core*, yang berarti mereka dapat menangani *training set* yang terlalu besar untuk masuk ke memori.\n",
        "\n",
        "2.  **Fitur dalam *training set* memiliki skala yang sangat berbeda. Algoritma mana yang mungkin menderita, dan bagaimana? Apa yang bisa dilakukan?**\n",
        "    * **Algoritma yang Menderita**: **Gradient Descent (GD) variants** (Batch GD, Stochastic GD, Mini-batch GD) dan **Regularized Linear Models** (Ridge, Lasso, Elastic Net).\n",
        "    * **Bagaimana Mereka Menderita**:\n",
        "        * **Gradient Descent**: Jika fitur memiliki skala yang sangat berbeda, fungsi biaya akan memiliki bentuk seperti mangkuk yang sangat memanjang (oval), bukan mangkuk bundar. Ini menyebabkan Gradient Descent bergerak hampir tegak lurus dengan arah menuju minimum global pada awalnya, dan kemudian \"berbaris\" lama di lembah yang hampir datar, sehingga membutuhkan waktu sangat lama untuk konvergen.\n",
        "        * **Regularized Linear Models (Ridge, Lasso, Elastic Net)**: Model-model ini sensitif terhadap skala fitur input. Karena *regularization term* menambahkan penalti berdasarkan ukuran bobot fitur ($\\Sigma\\theta_i^2$ atau $\\Sigma|\\theta_i|$), fitur dengan skala yang lebih besar secara inheren akan memiliki bobot yang cenderung lebih kecil (atau sebaliknya jika tidak diskalakan), sehingga regularisasi akan mempengaruhi fitur-fitur tersebut secara tidak proporsional.\n",
        "    * **Apa yang Bisa Dilakukan**: Pastikan semua fitur memiliki skala yang serupa. Ini dapat dilakukan dengan menggunakan **Feature Scaling** teknik seperti **Standardization** (menggunakan `StandardScaler` Scikit-Learn) atau **Normalization**.\n",
        "\n",
        "3.  **Dapatkah Gradient Descent terjebak di *local minimum* ketika melatih model Logistic Regression?**\n",
        "    * **Tidak, tidak seharusnya**.\n",
        "    * **Penjelasan Teoritis**: Fungsi biaya Logistic Regression (log loss) adalah **fungsi konveks**. Fungsi konveks memiliki properti penting bahwa mereka hanya memiliki satu minimum global dan tidak ada *local minima* lainnya. Oleh karena itu, Gradient Descent (dengan *learning rate* yang tidak terlalu besar dan waktu yang cukup lama) dijamin akan mendekati minimum global.\n",
        "\n",
        "4.  **Apakah semua algoritma Gradient Descent mengarah pada model yang sama, asalkan Anda membiarkannya berjalan cukup lama?**\n",
        "    * **Tidak selalu, tapi idealnya mendekati**.\n",
        "    * **Penjelasan Teoritis**:\n",
        "        * **Batch Gradient Descent**: Akan konvergen ke minimum global karena fungsi biaya Linear Regression adalah konveks dan halus. Jika dijalankan cukup lama dan dengan *learning rate* yang tepat, ia akan mencapai solusi optimal.\n",
        "        * **Stochastic Gradient Descent (SGD) dan Mini-batch Gradient Descent**: Karena sifat stokastik mereka, fungsi biaya akan memantul naik-turun dan tidak akan sepenuhnya menetap di minimum. Mereka akan berakhir sangat dekat dengan minimum global, tetapi tidak akan pernah berhenti tepat di sana kecuali *learning rate* berkurang ke nol. Namun, jika *learning schedule* yang baik digunakan (yaitu, secara bertahap mengurangi *learning rate*), mereka dapat mendekati minimum global secara arbitrer.\n",
        "        * **Perbedaan Utama**: SGD dan Mini-batch GD mungkin memiliki peluang lebih baik untuk keluar dari *local minima* pada fungsi biaya yang tidak konveks (yang bukan kasus untuk Linear/Logistic Regression). Jadi, meskipun untuk Linear/Logistic Regression mereka semua akan menemukan minimum global (atau sangat dekat dengannya), perilaku konvergensi dan lintasan mereka berbeda.\n",
        "\n",
        "5.  **Misalkan Anda menggunakan Batch Gradient Descent dan Anda memplot *validation error* pada setiap *epoch*. Jika Anda melihat bahwa *validation error* secara konsisten naik, apa yang kemungkinan terjadi? Bagaimana Anda bisa memperbaikinya?**\n",
        "    * **Apa yang Terjadi**: Jika *validation error* secara konsisten naik, kemungkinan besar **learning rate ($\\eta$) terlalu tinggi**. Ini menyebabkan algoritma \"melompati\" minimum dan menyimpang, dengan nilai fungsi biaya yang semakin besar pada setiap langkah.\n",
        "    * **Bagaimana Memperbaikinya**:\n",
        "        * **Kurangi *learning rate*** ($\\eta$). Anda bisa mencoba *grid search* untuk menemukan *learning rate* yang optimal.\n",
        "        * Pastikan fitur-fitur diskalakan dengan benar, karena skala yang berbeda juga dapat memperburuk masalah konvergensi pada GD.\n",
        "        * Jika model terlalu kompleks (misalnya, derajat polinomial terlalu tinggi), itu mungkin sudah *overfit* bahkan dengan *learning rate* yang tepat. Dalam kasus tersebut, **regularisasi** (Ridge, Lasso, Elastic Net) atau **mengurangi kompleksitas model** perlu dipertimbangkan.\n",
        "\n",
        "6.  **Apakah ide yang baik untuk segera menghentikan Mini-batch Gradient Descent ketika *validation error* naik?**\n",
        "    * **Tidak, itu bukan ide yang baik untuk segera menghentikannya**.\n",
        "    * **Penjelasan Teoritis**: Dengan Stochastic Gradient Descent dan Mini-batch Gradient Descent, kurva *validation error* tidak semulus Batch Gradient Descent. Mereka akan **memantul naik dan turun** karena sifat stokastik algoritma. Jika Anda berhenti segera setelah *error* validasi naik untuk pertama kalinya, Anda mungkin berhenti terlalu dini dan tidak mencapai minimum global.\n",
        "    * **Apa yang Seharusnya Dilakukan**: Solusi yang lebih baik untuk *early stopping* dengan SGD/Mini-batch GD adalah **berhenti hanya setelah *validation error* berada di atas minimum selama beberapa waktu** (misalnya, beberapa *epoch*). Ini memberi keyakinan bahwa model tidak akan berkinerja lebih baik lagi. Kemudian, **kembalikan parameter model ke titik di mana *validation error* adalah minimum**. Ini sering diimplementasikan dengan melacak *best validation error* yang diamati dan menyimpan parameter model yang sesuai."
      ],
      "metadata": {
        "id": "DIn2QDf1ppdP"
      }
    }
  ]
}