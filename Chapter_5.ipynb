{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVM) adalah model Machine Learning yang serbaguna dan kuat, mampu melakukan klasifikasi linear atau nonlinear, regresi, dan deteksi outlier. SVM sangat cocok untuk klasifikasi dataset kompleks berukuran kecil atau menengah.\n",
        "\n",
        "**1. Klasifikasi SVM Linear**\n",
        "Ide dasar di balik SVM adalah menemukan \"jalan\" terlebar yang mungkin (margin) antara kelas-kelas yang berbeda. Ini disebut klasifikasi margin besar (*large margin classification*).\n",
        "\n",
        "* **Large Margin Classification**: SVM classifier tidak hanya memisahkan dua kelas tetapi juga menjaga jarak sejauh mungkin dari *training instance* terdekat. *Instance* yang berada di tepi \"jalan\" ini disebut *support vector*.\n",
        "* **Sensitivitas terhadap Skala Fitur**: SVM sensitif terhadap skala fitur. Penskalaan fitur (misalnya, menggunakan `StandardScaler` dari Scikit-Learn) dapat meningkatkan *decision boundary* secara signifikan.\n",
        "* **Hard Margin vs. Soft Margin Classification**:\n",
        "    * **Hard Margin Classification**: Mensyaratkan semua *instance* berada di luar \"jalan\" dan di sisi yang benar. Masalah utamanya adalah hanya berfungsi jika data dapat dipisahkan secara linear dan sangat sensitif terhadap outlier.\n",
        "    * **Soft Margin Classification**: Model yang lebih fleksibel yang bertujuan untuk menyeimbangkan ukuran \"jalan\" (margin) dengan jumlah *margin violation* (*instance* yang berada di tengah \"jalan\" atau di sisi yang salah). Hiperparameter `C` mengontrol *trade-off* ini. Nilai `C` yang rendah menghasilkan margin yang lebih besar tetapi dengan lebih banyak *margin violation*, yang cenderung menggeneralisasi lebih baik jika model *overfitting*.\n",
        "\n",
        "**Contoh Kode Linear SVM Classifier (Scikit-Learn)**:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
        "\n",
        "svm_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
        "])\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "# Contoh prediksi\n",
        "print(svm_clf.predict([[5.5, 1.7]])) # Output: array([1.])\n",
        "```\n",
        "\n",
        "Kode ini memuat dataset iris, menskalakan fitur, dan melatih model SVM linear menggunakan kelas `LinearSVC` untuk mendeteksi bunga Iris virginica.\n",
        "\n",
        "* `LinearSVC` tidak menghasilkan probabilitas kelas.\n",
        "* Alternatif `LinearSVC` adalah kelas `SVC` dengan `kernel=\"linear\"` atau `SGDClassifier` dengan `loss=\"hinge\"`. `SGDClassifier` berguna untuk tugas klasifikasi *online* atau dataset besar.\n",
        "* Penting untuk menskalakan data saat menggunakan `LinearSVC` karena ia meregulasi bias term. Juga, pastikan `loss` disetel ke \"hinge\" dan `dual` disetel ke `False` (kecuali jika fitur lebih banyak dari *instance* pelatihan).\n",
        "\n",
        "**2. Klasifikasi SVM Nonlinear**\n",
        "Ketika dataset tidak dapat dipisahkan secara linear, ada dua pendekatan utama:\n",
        "\n",
        "* **Menambahkan Fitur Polinomial**: Menambahkan fitur baru, seperti fitur polinomial, dapat membuat dataset terpisah secara linear.\n",
        "    * **Contoh Kode (PolynomialFeatures dengan LinearSVC)**:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.datasets import make_moons\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.preprocessing import PolynomialFeatures\n",
        "    from sklearn.svm import LinearSVC\n",
        "\n",
        "    X, y = make_moons(n_samples=100, noise=0.15)\n",
        "    polynomial_svm_clf = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
        "    ])\n",
        "    polynomial_svm_clf.fit(X, y)\n",
        "    ```\n",
        "\n",
        "    Pendekatan ini dapat bekerja dengan baik tetapi menjadi lambat dengan derajat polinomial yang tinggi karena menciptakan banyak fitur.\n",
        "\n",
        "* **Kernel Trick**: Teknik matematis yang memungkinkan SVM untuk mencapai hasil yang sama seolah-olah Anda telah menambahkan banyak fitur polinomial (bahkan dengan derajat sangat tinggi) tanpa benar-benar menambahkannya. Ini mencegah ledakan kombinatorial jumlah fitur.\n",
        "    * **Polynomial Kernel**: Diimplementasikan oleh kelas `SVC` dengan `kernel=\"poly\"`.\n",
        "        * `degree`: Mengontrol derajat polinomial. Kurangi jika *overfitting*, tingkatkan jika *underfitting*.\n",
        "        * `coef0`: Mengontrol seberapa besar model dipengaruhi oleh polinomial derajat tinggi versus derajat rendah.\n",
        "    * **Similarity Features (Gaussian RBF Kernel)**: Menambahkan fitur yang dihitung menggunakan fungsi kemiripan, seperti Gaussian Radial Basis Function (RBF).\n",
        "        * Rumus Gaussian RBF: $\\phi_{\\gamma}(x,l)=exp(-\\gamma||x-l||^{2})$.\n",
        "        * Kernel RBF Gaussian memungkinkan untuk mendapatkan hasil yang mirip seolah-olah Anda menambahkan banyak fitur kemiripan.\n",
        "    * **Contoh Kode (SVC dengan Gaussian RBF Kernel)**:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.svm import SVC\n",
        "\n",
        "    # X dan y dari make_moons() sebelumnya\n",
        "    rbf_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "    ])\n",
        "    rbf_kernel_svm_clf.fit(X, y)\n",
        "    ```\n",
        "\n",
        "    * `gamma` ($\\gamma$): Bertindak sebagai hiperparameter regularisasi. Meningkatkan `gamma` membuat kurva berbentuk lonceng lebih sempit, mengurangi jangkauan pengaruh setiap *instance*, menghasilkan *decision boundary* yang lebih tidak teratur (cenderung *overfitting*). Mengurangi `gamma` membuat kurva lebih lebar, *instance* memiliki jangkauan pengaruh yang lebih besar, dan *decision boundary* menjadi lebih halus (cenderung *underfitting*).\n",
        "\n",
        "**Memilih Kernel**:\n",
        "* Selalu coba *linear kernel* terlebih dahulu (`LinearSVC` lebih cepat dari `SVC(kernel=\"linear\")`), terutama jika dataset pelatihan sangat besar atau memiliki banyak fitur.\n",
        "* Jika dataset pelatihan tidak terlalu besar, coba *Gaussian RBF kernel* karena seringkali bekerja dengan baik.\n",
        "* Bereksperimenlah dengan *cross-validation* dan *grid search* untuk kernel lainnya, terutama jika ada kernel yang khusus untuk struktur data Anda.\n",
        "\n",
        "**3. Kompleksitas Komputasi**\n",
        "* **`LinearSVC`**: Berbasis library `liblinear`. Tidak mendukung *kernel trick*. Skalanya hampir linear dengan jumlah *training instance* ($m$) dan jumlah fitur ($n$), dengan kompleksitas waktu pelatihan sekitar $O(m \\times n)$. Sensitif terhadap *tolerance hyperparameter* ($\\epsilon$).\n",
        "* **`SVC`**: Berbasis library `libsvm`. Mendukung *kernel trick*. Kompleksitas waktu pelatihan biasanya antara $O(m^2 \\times n)$ dan $O(m^3 \\times n)$. Menjadi sangat lambat jika jumlah *training instance* besar. Cocok untuk dataset pelatihan kompleks berukuran kecil atau menengah. Skala baik dengan jumlah fitur, terutama fitur *sparse*.\n",
        "* **`SGDClassifier`**: Skalanya $O(m \\times n)$ dan mendukung *out-of-core training* (dapat menangani dataset yang tidak muat di memori).\n",
        "\n",
        "**4. Regresi SVM**\n",
        "Algoritma SVM juga mendukung regresi linear dan nonlinear.\n",
        "* Tujuannya adalah untuk menyesuaikan *instance* sebanyak mungkin di dalam \"jalan\" sambil membatasi *margin violation*.\n",
        "* Lebar \"jalan\" dikontrol oleh hiperparameter $\\epsilon$. Model dikatakan $\\epsilon$-insensitive karena menambahkan lebih banyak *training instance* di dalam margin tidak memengaruhi prediksi model.\n",
        "* **`LinearSVR`**: Digunakan untuk regresi SVM linear.\n",
        "* **`SVR`**: Digunakan untuk regresi SVM *kernelized* (nonlinear).\n",
        "\n",
        "**Contoh Kode Linear SVM Regression (Scikit-Learn)**:\n",
        "\n",
        "```python\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "# Asumsikan X dan y sudah diskalakan dan terpusat\n",
        "svm_reg = LinearSVR(epsilon=1.5)\n",
        "svm_reg.fit(X, y)\n",
        "```\n",
        "\n",
        "**Contoh Kode Nonlinear SVM Regression (Scikit-Learn)**:\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Asumsikan X dan y\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg.fit(X, y)\n",
        "```\n",
        "\n",
        "**5. Di Balik Layar (Teoritis)**\n",
        "* **Fungsi Keputusan (*Decision Function*)**: Untuk SVM classifier linear, kelas dari *instance* baru $x$ diprediksi dengan menghitung fungsi keputusan $w^{\\top}x+b$. Jika hasilnya positif, kelas yang diprediksi adalah kelas positif (1), jika tidak, itu adalah kelas negatif (0).\n",
        "    * *Decision boundary* adalah kumpulan titik di mana fungsi keputusan sama dengan 0.\n",
        "    * *Dashed line* pada plot menunjukkan titik-titik di mana fungsi keputusan sama dengan 1 atau -1: mereka paralel dan berjarak sama dari *decision boundary*, membentuk margin di sekitarnya.\n",
        "* **Tujuan Pelatihan (*Training Objective*)**:\n",
        "    * Tujuan utama adalah meminimalkan $||w||$ (norma vektor bobot) untuk mendapatkan margin yang lebih besar.\n",
        "    * **Hard Margin Linear SVM Classifier Objective**: Meminimalkan $\\frac{1}{2}w^{\\top}w$ (setara dengan meminimalkan $\\frac{1}{2}||w||^2$, yang memiliki turunan yang lebih sederhana $w$). Dibatasi oleh $t^{(i)}(w^{\\top}x^{(i)}+b)\\ge1$ untuk semua *instance* (di mana $t^{(i)}$ adalah -1 untuk *instance* negatif dan 1 untuk *instance* positif).\n",
        "    * **Soft Margin Linear SVM Classifier Objective**: Memperkenalkan *slack variable* $\\zeta^{(i)}\\ge0$ untuk setiap *instance* yang mengukur seberapa banyak *instance* ke-i diizinkan melanggar margin. Tujuannya adalah meminimalkan $\\frac{1}{2}w^{\\top}w+C\\sum_{i=1}^{m}\\zeta^{(i)}$ dengan batasan $t^{(i)}(w^{\\top}x^{(i)}+b)\\ge1-\\zeta^{(i)}$ dan $\\zeta^{(i)}\\ge0$. `C` mengatur *trade-off* antara memperkecil *slack variable* dan memperbesar margin.\n",
        "* **Quadratic Programming (QP)**: Masalah *hard margin* dan *soft margin* keduanya merupakan masalah optimisasi kuadrat cembung dengan batasan linear, dikenal sebagai masalah *Quadratic Programming*. QP *solver* dapat digunakan untuk menyelesaikan masalah ini.\n",
        "* **Masalah Dual (*Dual Problem*)**: SVM dapat diselesaikan dengan memecahkan masalah primal atau masalah dualnya, karena keduanya memiliki solusi yang sama di bawah kondisi tertentu. Masalah dual lebih cepat diselesaikan ketika jumlah *training instance* lebih kecil dari jumlah fitur. Yang lebih penting, masalah dual memungkinkan *kernel trick*.\n",
        "    * **Bentuk Dual dari Linear SVM Objective**:\n",
        "        $\\text{minimize} \\frac{1}{2}\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha^{(i)}\\alpha^{(j)}t^{(i)}t^{(j)}x^{(i)\\tau}x^{(j)}-\\sum_{i=1}^{m}\\alpha^{(i)}$\n",
        "        subject to $\\alpha^{(i)}\\ge0$ for $i=1,2,\\cdot\\cdot\\cdot,m$\n",
        "    * Setelah menemukan $\\hat{\\alpha}$ yang meminimalkan persamaan ini, $\\hat{w}$ dan $\\hat{b}$ dapat dihitung menggunakan persamaan di Equation 5-7.\n",
        "* **Kernelized SVMs**:\n",
        "    * **Tantangan Nonlinearitas**: Ketika data tidak dapat dipisahkan secara linear di ruang fitur asli, salah satu solusinya adalah memetakan data ke ruang fitur dimensi yang lebih tinggi di mana data mungkin menjadi terpisah secara linear. Namun, pemetaan eksplisit ke ruang dimensi tinggi dapat sangat mahal secara komputasi dan bahkan tidak mungkin jika dimensinya tak terbatas.\n",
        "    * **Solusi Kernel Trick**: *Kernel trick* adalah teknik brilian yang menghindari kebutuhan untuk secara eksplisit melakukan transformasi ini. Sebaliknya, ia menggantikan *dot product* di ruang fitur dimensi tinggi dengan fungsi *kernel* yang dapat dihitung langsung dari vektor di ruang fitur asli.\n",
        "        * Ini bekerja karena algoritma SVM, dalam bentuk dualnya, hanya bergantung pada *dot product* antara *training instance*. Dengan mengganti *dot product* ini dengan fungsi *kernel*, kita secara efektif melatih SVM di ruang fitur dimensi tinggi tanpa pernah benar-benar bekerja di ruang itu.\n",
        "    * **Contoh Kernel**:\n",
        "        * **Polynomial Kernel**: $K(a,b)=(ya^{\\top}b+r)^{d}$. Ini cocok untuk dataset yang dapat dipisahkan secara linear setelah transformasi polinomial.\n",
        "        * **Gaussian RBF Kernel**: $K(a,b)=exp(-\\gamma||a-b||^{2})$. Kernel ini secara implisit memetakan *instance* ke ruang dimensi tak terbatas, sangat efektif untuk menangani hubungan nonlinear yang kompleks. Parameter `gamma` mengontrol \"luas\" jangkauan pengaruh setiap *instance*.\n",
        "* **Online SVMs**: Untuk SVM classifier linear, salah satu metode untuk mengimplementasikan SVM *online* adalah menggunakan *Gradient Descent* (misalnya, dengan `SGDClassifier`) untuk meminimalkan fungsi biaya.\n",
        "    * **Fungsi Biaya Linear SVM Classifier**:\n",
        "        $I(w,b)=\\frac{1}{2}w^{T}w+C\\sum_{i=1}^{m}max(0,1-t^{(i)}(w^{T}x^{(i)}+b))$\n",
        "        * Suku pertama (`$\\frac{1}{2}w^{T}w$`) mendorong model untuk memiliki vektor bobot `w` yang kecil (margin yang lebih besar).\n",
        "        * Suku kedua (`$C\\sum_{i=1}^{m}max(0,1-t^{(i)}(w^{T}x^{(i)}+b))$`) menghitung total *margin violation* dan meminimalkannya.\n",
        "    * **Hinge Loss**: Fungsi $max(0,1-t)$ disebut fungsi *hinge loss*.\n",
        "\n",
        "Secara keseluruhan, SVM adalah algoritma yang canggih yang memanfaatkan konsep geometris margin maksimal dan teknik matematika cerdas seperti *kernel trick* dan dualitas untuk menangani masalah klasifikasi dan regresi, baik linear maupun nonlinear, secara efisien."
      ],
      "metadata": {
        "id": "xceARzpjrYvK"
      }
    }
  ]
}