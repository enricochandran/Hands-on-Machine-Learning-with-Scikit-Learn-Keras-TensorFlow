{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning (RL) adalah salah satu bidang Machine Learning yang paling menarik dan juga salah satu yang tertua, sudah ada sejak tahun 1950-an. Namun, revolusi terjadi pada tahun 2013 ketika DeepMind mendemonstrasikan sistem yang dapat belajar memainkan hampir semua game Atari dari awal, bahkan mengungguli manusia dalam banyak game, hanya menggunakan input piksel mentah dan tanpa pengetahuan awal tentang aturan game.\n",
        "\n",
        "**Tujuan RL**: Dalam Reinforcement Learning, sebuah *agent* perangkat lunak membuat observasi dan mengambil tindakan dalam suatu *environment*, dan sebagai imbalannya menerima *rewards*. Tujuannya adalah belajar bertindak sedemikian rupa untuk memaksimalkan *rewards* yang diharapkan dari waktu ke waktu.\n",
        "\n",
        "**Komponen Utama RL**:\n",
        "* **Agent**: Program yang mengontrol tindakan.\n",
        "* **Environment**: Dunia tempat agent beroperasi. Ini bisa berupa dunia fisik (misalnya, robot), simulasi (misalnya, game Atari, papan catur), atau sistem abstrak (misalnya, pasar saham, termostat pintar).\n",
        "* **Actions**: Tindakan yang dapat diambil oleh agent dalam environment.\n",
        "* **Observations**: Informasi yang diterima agent dari environment.\n",
        "* **Rewards**: Umpan balik positif atau negatif yang diterima agent dari environment sebagai konsekuensi dari tindakannya. Tujuan agent adalah memaksimalkan *reward* total.\n",
        "\n",
        "**Contoh Aplikasi RL**:\n",
        "* Mengendalikan robot, di mana lingkungan adalah dunia nyata dan *rewards* dapat berupa mendekati target atau menghindari pemborosan waktu.\n",
        "* Mengendalikan Ms. Pac-Man, di mana lingkungan adalah simulasi game Atari dan *rewards* adalah poin game.\n",
        "* Memainkan permainan papan seperti Go.\n",
        "* Mengendalikan termostat pintar, dengan *rewards* untuk suhu yang tepat dan penghematan energi.\n",
        "* Perdagangan saham otomatis, dengan *rewards* berupa keuntungan dan kerugian moneter.\n",
        "* Mobil tanpa pengemudi, sistem rekomendasi, penempatan iklan, atau kontrol perhatian sistem klasifikasi gambar.\n",
        "\n",
        "**Policy (Kebijakan)**:\n",
        "* Algoritma yang digunakan agent untuk menentukan tindakannya disebut *policy*.\n",
        "* *Policy* dapat berupa neural network yang mengambil observasi sebagai input dan mengeluarkan tindakan yang harus diambil.\n",
        "* *Policy* bisa bersifat deterministik atau stokastik (melibatkan randomness). Contoh stokastik adalah robot penyedot debu yang bergerak maju atau berputar secara acak.\n",
        "\n",
        "**Policy Search**:\n",
        "* Mencoba berbagai nilai parameter untuk *policy* dan memilih kombinasi terbaik (pendekatan *brute-force*).\n",
        "* Menggunakan *genetic algorithms* untuk mengeksplorasi ruang *policy*. Ini melibatkan pembuatan generasi *policy*, memilih yang terbaik, dan membiarkan mereka \"bereproduksi\" dengan variasi acak.\n",
        "* Menggunakan teknik optimasi seperti *policy gradients*, di mana gradien *rewards* terhadap parameter *policy* dievaluasi untuk menyesuaikan parameter menuju *rewards* yang lebih tinggi.\n",
        "\n",
        "**OpenAI Gym**:\n",
        "* Toolkit yang menyediakan berbagai *simulated environments* untuk melatih agent RL. Ini membantu mengatasi tantangan pelatihan di dunia nyata yang lambat, mahal, dan tidak dapat dibatalkan.\n",
        "* Contoh lingkungan: CartPole, di mana sebuah gerobak harus menyeimbangkan tiang di atasnya.\n",
        "* `env.reset()`: Menginisialisasi environment dan mengembalikan observasi pertama.\n",
        "* `env.step(action)`: Mengeksekusi tindakan dan mengembalikan observasi baru, *reward*, status `done` (episode selesai), dan `info` (informasi tambahan).\n",
        "* `env.render()`: Menampilkan environment.\n",
        "* `env.action_space`: Menjelaskan tindakan yang mungkin.\n",
        "* `env.observation_spec()`: Mengembalikan spesifikasi observasi (bentuk, tipe data, dll.).\n",
        "\n",
        "**Neural Network Policies**:\n",
        "* Neural network dapat digunakan sebagai *policy*, mengambil observasi sebagai input dan mengeluarkan probabilitas untuk setiap tindakan.\n",
        "* Contoh untuk CartPole: satu neuron output dengan fungsi aktivasi sigmoid untuk probabilitas tindakan 0 (kiri), dan $1-p$ untuk tindakan 1 (kanan).\n",
        "* Memilih tindakan secara acak berdasarkan probabilitas (daripada memilih tindakan dengan skor tertinggi) membantu agent menyeimbangkan eksplorasi (mencoba tindakan baru) dan eksploitasi (menggunakan tindakan yang diketahui berhasil).\n",
        "\n",
        "**Credit Assignment Problem**:\n",
        "* Masalah dalam RL di mana sulit untuk mengetahui tindakan mana yang harus diberi kredit (atau disalahkan) ketika agent menerima *reward* yang jarang dan tertunda.\n",
        "* **Solusi**: Mengevaluasi tindakan berdasarkan jumlah semua *rewards* yang datang setelahnya, biasanya menerapkan *discount factor* ($\\gamma$) pada setiap langkah. Jumlah *rewards* yang didiskon ini disebut *return* tindakan.\n",
        "* *Discount factor* ($\\gamma$): Nilai antara 0 dan 1. Jika $\\gamma$ mendekati 0, *rewards* di masa depan tidak terlalu berarti dibandingkan *rewards* langsung. Jika $\\gamma$ mendekati 1, *rewards* di masa depan hampir sama pentingnya dengan *rewards* langsung.\n",
        "* *Action advantage*: Mengukur seberapa baik atau buruk suatu tindakan dibandingkan dengan tindakan lain yang mungkin, rata-rata. Ini dihitung dengan menormalkan semua *returns* tindakan (mengurangi *mean* dan membagi dengan *standard deviation*).\n",
        "\n",
        "**Policy Gradients (PG)**:\n",
        "* Algoritma yang mengoptimalkan parameter *policy* dengan mengikuti gradien menuju *rewards* yang lebih tinggi.\n",
        "* **Algoritma REINFORCE (salah satu varian PG)**:\n",
        "    1.  Biarkan neural network *policy* memainkan game beberapa kali; pada setiap langkah, hitung gradien yang akan membuat tindakan yang dipilih lebih mungkin. Jangan terapkan gradien ini dulu.\n",
        "    2.  Setelah beberapa episode, hitung *advantage* setiap tindakan.\n",
        "    3.  Jika *advantage* positif, kalikan vektor gradien dengan *advantage* untuk membuat tindakan lebih mungkin di masa depan. Jika negatif, kalikan dengan kebalikannya untuk membuat tindakan sedikit kurang mungkin.\n",
        "    4.  Hitung *mean* dari semua vektor gradien yang dihasilkan dan gunakan untuk melakukan langkah Gradient Descent.\n",
        "\n",
        "**Implementasi PG (dengan `tf.keras`)**:\n",
        "* `play_one_step(env, obs, model, loss_fn)`: Fungsi yang memainkan satu langkah, menghitung *loss* dan gradien (yang akan diubah nanti).\n",
        "    * Menggunakan `tf.GradientTape()` untuk merekam operasi dan menghitung gradien.\n",
        "    * `model(obs[np.newaxis])`: Memprediksi probabilitas tindakan.\n",
        "    * Memilih tindakan secara stokastik (`tf.random.uniform`).\n",
        "    * Menghitung *loss* menggunakan `loss_fn` (misalnya `binary_crossentropy`).\n",
        "* `play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn)`: Memainkan beberapa episode, mengumpulkan *rewards* dan gradien.\n",
        "* `discount_rewards(rewards, discount_factor)`: Menghitung jumlah *rewards* masa depan yang didiskon.\n",
        "* `discount_and_normalize_rewards(all_rewards, discount_factor)`: Menormalkan *rewards* yang didiskon untuk mendapatkan *action advantages*.\n",
        "* **Loop Pelatihan**:\n",
        "    * Dalam setiap iterasi, `play_multiple_episodes()` dijalankan.\n",
        "    * `discount_and_normalize_rewards()` menghitung *action advantages*.\n",
        "    * Gradien dihitung sebagai *weighted mean* dari gradien yang terkumpul, dengan bobot berdasarkan *advantage*.\n",
        "    * `optimizer.apply_gradients()` menerapkan gradien untuk memperbarui model.\n",
        "\n",
        "**Markov Decision Processes (MDPs)**:\n",
        "* MDPs mirip dengan Markov chains (proses stokastik tanpa memori), tetapi dengan tambahan bahwa agent dapat memilih salah satu dari beberapa tindakan yang mungkin di setiap langkah, dan probabilitas transisi tergantung pada tindakan yang dipilih.\n",
        "* Beberapa transisi state menghasilkan *reward*. Tujuan agent adalah menemukan *policy* yang akan memaksimalkan *reward* dari waktu ke waktu.\n",
        "* **Optimal state value ($V^{*}(s)$)**: Jumlah semua *rewards* masa depan yang didiskon yang dapat diharapkan agent rata-rata setelah mencapai state $s$, dengan asumsi bertindak secara optimal.\n",
        "* **Bellman Optimality Equation**: $V^{*}(s)=max_{a}\\sum_{s}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+y\\cdot V^{*}(s^{\\prime})]$. Menjelaskan bahwa nilai optimal state saat ini sama dengan *reward* rata-rata setelah mengambil satu tindakan optimal, ditambah nilai optimal yang diharapkan dari semua state berikutnya yang dapat dicapai tindakan tersebut.\n",
        "    * $T(s,a,s^{\\prime})$: Probabilitas transisi dari state $s$ ke $s^{\\prime}$ diberikan tindakan $a$.\n",
        "    * $R(s,a,s^{\\prime})$: *Reward* dari transisi $s \\rightarrow s^{\\prime}$ dengan tindakan $a$.\n",
        "    * $\\gamma$: *Discount factor*.\n",
        "* **Value Iteration algorithm**: Iteratif memperbarui estimasi nilai state ($V_k(s)$) menggunakan Bellman Optimality Equation. Dijamin konvergen ke nilai state optimal.\n",
        "* **Optimal state-action values (Q-Values)**: $Q^{*}(s,a)$ adalah jumlah *rewards* masa depan yang didiskon yang dapat diharapkan agent rata-rata setelah mencapai state $s$ dan memilih tindakan $a$, tetapi sebelum melihat hasil tindakan ini, dengan asumsi bertindak secara optimal setelahnya.\n",
        "* **Q-Value Iteration algorithm**: Iteratif memperbarui estimasi Q-Value.\n",
        "* **Optimal Policy ($\\pi^{*}(s)$)**: Setelah Q-Values optimal, *policy* optimal adalah memilih tindakan dengan Q-Value tertinggi untuk state tersebut: $\\pi^{*}(s)=$ argmax $Q^{*}(s,a)$.\n",
        "\n",
        "**Temporal Difference Learning (TD Learning)**:\n",
        "* Mirip dengan Value Iteration, tetapi disesuaikan untuk mengambil keuntungan dari pengetahuan parsial agent tentang MDP (transisi dan *rewards* awalnya tidak diketahui).\n",
        "* Agent menggunakan *exploration policy* untuk menjelajahi MDP.\n",
        "* Menggunakan running average dari *rewards* langsung ditambah *rewards* yang diharapkan nanti.\n",
        "* $V_{k+1}(s)\\leftarrow(1-\\alpha)V_{k}(s)+\\alpha(r+y\\cdot V_{k}(s^{\\prime}))$ atau $V_{k+1}(s)\\leftarrow V_{k}(s)+\\alpha\\cdot\\delta_{k}(s,r,s^{\\prime})$.\n",
        "    * $\\alpha$: *Learning rate*.\n",
        "    * $r+\\gamma\\cdot V_{k}(s^{\\prime})$: *TD target*.\n",
        "    * $\\delta_{k}(s,r,s^{\\prime})$: *TD error*.\n",
        "\n",
        "**Q-Learning**:\n",
        "* Adaptasi dari Q-Value Iteration ketika probabilitas transisi dan *rewards* awalnya tidak diketahui.\n",
        "* Bekerja dengan mengamati agent bermain dan secara bertahap meningkatkan estimasi Q-Values.\n",
        "* Setelah estimasi Q-Value akurat, *policy* optimal adalah memilih tindakan dengan Q-Value tertinggi (kebijakan *greedy*).\n",
        "* $Q(s,a)\\leftarrow r+y\\cdot max_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})$.\n",
        "* **Off-policy algorithm**: *Policy* yang dilatih tidak harus sama dengan *policy* yang dieksekusi (misalnya, *exploration policy* bisa acak, sementara *policy* yang dilatih memilih Q-Value tertinggi).\n",
        "* **On-policy algorithm**: Seperti Policy Gradients, menjelajahi dunia menggunakan *policy* yang dilatih.\n",
        "\n",
        "**Exploration Policies**:\n",
        "* Penting untuk Q-Learning agar *exploration policy* menjelajahi MDP secara menyeluruh.\n",
        "* **$\\epsilon$-greedy policy**: Bertindak secara acak dengan probabilitas $\\epsilon$, atau secara *greedy* (memilih tindakan dengan Q-Value tertinggi) dengan probabilitas $1-\\epsilon$. Umumnya, $\\epsilon$ dimulai tinggi dan berkurang secara bertahap.\n",
        "* **Exploration function**: Mendorong *policy* untuk mencoba tindakan yang belum banyak dicoba sebelumnya dengan menambahkan bonus ke estimasi Q-Value, seperti $f(Q,N)=Q+\\kappa/(1+N)$, di mana $N$ adalah jumlah kali tindakan $a^{\\prime}$ dipilih di state $s^{\\prime}$ dan $\\kappa$ adalah *curiosity hyperparameter*.\n",
        "\n",
        "**Approximate Q-Learning dan Deep Q-Learning (DQN)**:\n",
        "* Masalah utama Q-Learning adalah tidak skalabel untuk MDPs besar.\n",
        "* **Approximate Q-Learning**: Menemukan fungsi $Q_{\\theta}(s,a)$ yang mengaproksimasi Q-Value dari pasangan state-action menggunakan sejumlah parameter yang dapat dikelola.\n",
        "* **Deep Q-Network (DQN)**: Deep Neural Network (DNN) digunakan untuk mengestimasi Q-Values.\n",
        "* **Pelatihan DQN**: Meminimalkan *squared error* (atau Huber loss) antara Q-Value yang diestimasi dan *target Q-Value*.\n",
        "* **Target Q-Value**: $Q_{target}(s,a)=r+\\gamma\\cdot max_{a^{\\prime}}Q_{\\theta}(s^{\\prime},a^{\\prime})$.\n",
        "\n",
        "**Implementasi Deep Q-Learning (dengan `tf.keras`)**:\n",
        "* Membuat DQN (model Sequential dengan `Dense` layers).\n",
        "* `epsilon_greedy_policy(state, epsilon=0)`: Memilih tindakan secara acak atau berdasarkan Q-Value tertinggi.\n",
        "* **Replay Buffer (Replay Memory)**: Menyimpan semua pengalaman (state, action, reward, next_state, done) dan mengambil *random training batch* darinya di setiap iterasi pelatihan. Ini mengurangi korelasi antara pengalaman dan membantu pelatihan. Menggunakan `collections.deque` atau *circular buffer*.\n",
        "* `sample_experiences(batch_size)`: Mengambil *batch* pengalaman acak dari *replay buffer*.\n",
        "* `play_one_step(env, state, epsilon)`: Memainkan satu langkah menggunakan *epsilon-greedy policy* dan menyimpan pengalaman di *replay buffer*.\n",
        "* `training_step(batch_size)`: Mengambil *batch* pengalaman, menghitung Q-Values target, menghitung *loss*, dan melakukan langkah Gradient Descent.\n",
        "    * `model.predict(next_states)`: Memprediksi Q-Value untuk setiap tindakan di *next state*.\n",
        "    * `np.max(next_Q_values, axis=1)`: Mengambil Q-Value maksimum dari *next state* (asumsi optimal).\n",
        "    * `target_Q_values = (rewards + (1 - dones) * discount_factor * max_next_Q_values)`: Menghitung Q-Value target.\n",
        "    * `tf.one_hot(actions, n_outputs)`: Membuat *mask* untuk memilih Q-Value dari tindakan yang benar-benar dipilih.\n",
        "    * Menghitung *loss* (Mean Squared Error) antara target dan prediksi.\n",
        "    * `optimizer.apply_gradients()`: Menerapkan gradien.\n",
        "* **Catastrophic Forgetting**: Salah satu masalah besar dalam RL di mana pembelajaran di satu bagian environment dapat merusak apa yang dipelajari sebelumnya di bagian lain. Hal ini karena pengalaman yang sangat berkorelasi dan environment belajar yang terus berubah. Dapat dikurangi dengan meningkatkan ukuran *replay buffer* atau mengurangi *learning rate*.\n",
        "\n",
        "**Deep Q-Learning Variants**:\n",
        "* **Fixed Q-Value Targets**: Menggunakan dua DQNs: *online model* (untuk memprediksi dan bergerak) dan *target model* (untuk menentukan target Q-Value). *Target model* adalah klon dari *online model* dan diperbarui secara berkala (lebih jarang). Ini menstabilkan pelatihan.\n",
        "* **Double DQN (DDQN)**: Mengatasi *overestimation* Q-Value oleh *target model*. Menggunakan *online model* untuk memilih tindakan terbaik untuk *next state*, dan *target model* hanya untuk mengestimasi Q-Value untuk tindakan terbaik tersebut.\n",
        "* **Prioritized Experience Replay (PER)**: Mengambil sampel pengalaman penting lebih sering dari *replay buffer*. Pengalaman penting didefinisikan oleh besarnya *TD error*. Pengalaman dengan *TD error* besar diberi prioritas lebih tinggi. Untuk mengompensasi bias sampling, bobot pelatihan pengalaman disesuaikan.\n",
        "* **Dueling DQN (DDQN, jangan bingung dengan Double DQN)**: Memisahkan estimasi Q-Value menjadi nilai state ($V(s)$) dan *advantage* tindakan ($A(s,a)$), dengan $Q(s,a)=V(s)+A(s,a)$. Model mengestimasi keduanya. Keunggulan tindakan terbaik diatur ke 0.\n",
        "\n",
        "**TF-Agents Library**:\n",
        "* Pustaka Reinforcement Learning berbasis TensorFlow, dikembangkan di Google.\n",
        "* Menyediakan banyak environment siap pakai (termasuk *wrapper* untuk OpenAI Gym), serta dukungan untuk PyBullet, DM Control, dan Unity ML-Agents.\n",
        "* Mengimplementasikan banyak algoritma RL (REINFORCE, DQN, DDQN) dan komponen RL (buffer replay, metrik).\n",
        "* **TF-Agents Environments**: Mirip dengan OpenAI Gym, tetapi `reset()` dan `step()` mengembalikan objek `TimeStep`.\n",
        "* **Environment Specifications (`observation_spec()`, `action_spec()`, `time_step_spec()`)**: Memberikan detail tentang bentuk, tipe data, dan rentang nilai observasi, tindakan, dan *time steps*.\n",
        "* **Environment Wrappers**: Menambahkan fungsionalitas tambahan ke environment (misalnya, `ActionRepeat`, `TimeLimit`, `VideoWrapper`).\n",
        "* **Atari Preprocessing**: Wrapper khusus untuk environment Atari yang melakukan:\n",
        "    * **Grayscale dan downsampling**: Observasi diubah menjadi *grayscale* dan diperkecil (misalnya $84 \\times 84$ piksel).\n",
        "    * **Max pooling**: *Max-pooling* dua *frame* terakhir untuk menghilangkan *flickering*.\n",
        "    * **Frame skipping**: Agent hanya melihat setiap $n$ *frame* game, tindakan diulang, dan *rewards* dikumpulkan. Ini mempercepat game dari perspektif agent.\n",
        "    * **End on life lost**: Mengakhiri game segera jika kehilangan nyawa (opsional).\n",
        "* **`FrameStack4`**: Wrapper yang menghasilkan observasi yang terdiri dari empat *frame* yang ditumpuk di sepanjang dimensi *channel*. Ini membantu agent memahami arah dan kecepatan objek.\n",
        "* **`TFPyEnvironment`**: Membungkus environment Python agar dapat digunakan dalam grafik TensorFlow.\n",
        "\n",
        "**Arsitektur Pelatihan TF-Agents**:\n",
        "* **Collection (Pengumpulan)**:\n",
        "    * **Driver**: Menjelajahi environment menggunakan *collect policy* untuk memilih tindakan.\n",
        "    * **Collect Policy**: *Policy* yang digunakan driver untuk memilih tindakan.\n",
        "    * **Trajectories**: Representasi ringkas dari transisi dari satu *time step* ke *time step* berikutnya, atau urutan transisi.\n",
        "    * **Observer**: Fungsi yang menerima *trajectory* dan menyimpannya (misalnya, ke *replay buffer*) atau menghitung metrik.\n",
        "    * **Replay Buffer**: Tempat penyimpanan *trajectories* yang dikumpulkan.\n",
        "* **Training (Pelatihan)**:\n",
        "    * **Agent**: Mengambil *batch* *trajectories* dari *replay buffer* dan melatih *networks*.\n",
        "    * **Networks**: Model neural network (misalnya, Q-Network) yang digunakan oleh agent dan *collect policy*.\n",
        "    * **Dataset**: Cara untuk mengambil *batch* *trajectories* dari *replay buffer* secara efisien (menggunakan `tf.data.Dataset`).\n",
        "\n",
        "**Membuat Deep Q-Network dengan TF-Agents**:\n",
        "* Menggunakan `tf_agents.networks.q_network.QNetwork`.\n",
        "* Menerima `observation_spec()` dan `action_spec()`.\n",
        "* Memiliki `preprocessing_layers` (misalnya, `Lambda` layer untuk normalisasi).\n",
        "* Memiliki `conv_layer_params` (lapisan konvolusi) dan `fc_layer_params` (lapisan dense).\n",
        "* Terdiri dari *encoding network* (memproses observasi) dan lapisan output dense (satu Q-Value per tindakan).\n",
        "\n",
        "**Membuat DQN Agent dengan TF-Agents**:\n",
        "* Menggunakan `tf_agents.agents.dqn.dqn_agent.DqnAgent`.\n",
        "* Membutuhkan `time_step_spec()`, `action_spec()`, `q_network`, `optimizer`, `target_update_period` (periode update *target model*), `td_errors_loss_fn`, `gamma` (discount factor), `train_step_counter`, dan `epsilon_greedy`.\n",
        "* `epsilon_greedy` bisa menggunakan `keras.optimizers.schedules.PolynomialDecay` untuk mengatur nilai $\\epsilon$ yang menurun seiring waktu.\n",
        "\n",
        "**Membuat Replay Buffer dan Observer yang Sesuai**:\n",
        "* Menggunakan `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`.\n",
        "* Parameter: `data_spec` (dari `agent.collect_data_spec`), `batch_size`, `max_length`.\n",
        "* `replay_buffer_observer = replay_buffer.add_batch`.\n",
        "\n",
        "**Membuat Metrik Pelatihan**:\n",
        "* Menggunakan `tf_agents.metrics` (misalnya `NumberOfEpisodes`, `EnvironmentSteps`, `AverageReturnMetric`, `AverageEpisodeLengthMetric`).\n",
        "* `AverageReturnMetric` menghitung jumlah *rewards* yang tidak didiskon untuk setiap episode.\n",
        "* Metrik dapat dicetak dengan `log_metrics(train_metrics)`.\n",
        "\n",
        "**Membuat Collect Driver**:\n",
        "* Menggunakan `tf_agents.drivers.dynamic_step_driver.DynamicStepDriver` (untuk sejumlah langkah) atau `DynamicEpisodeDriver` (untuk sejumlah episode).\n",
        "* Parameter: environment, *collect policy* agent, daftar *observers*, `num_steps`.\n",
        "* **Warming up replay buffer**: Mengisi *replay buffer* dengan pengalaman yang dikumpulkan menggunakan *policy* acak (`RandomTFPolicy`) sebelum memulai pelatihan.\n",
        "\n",
        "**Membuat Dataset**:\n",
        "* Menggunakan `replay_buffer.as_dataset()` untuk membuat `tf.data.Dataset` dari *replay buffer*.\n",
        "* Parameter: `sample_batch_size`, `num_steps`, `num_parallel_calls`, `prefetch`.\n",
        "\n",
        "**Loop Pelatihan**:\n",
        "* Menggunakan `tf_agents.utils.common.function()` untuk mengonversi fungsi utama menjadi TensorFlow Functions untuk mempercepat pelatihan.\n",
        "* Loop memanggil `collect_driver.run()` untuk mengumpulkan pengalaman, lalu `agent.train()` untuk melatih model menggunakan *batch* dari dataset.\n",
        "* Kinerja agent diplot sebagai total *rewards* yang diperoleh per episode.\n",
        "\n",
        "**Algoritma RL Populer Lainnya**:\n",
        "* **Actor-Critic algorithms**: Menggabungkan Policy Gradients dengan Deep Q-Networks. Agent (*actor*) belajar dari nilai tindakan yang diestimasi oleh DQN (*critic*).\n",
        "* **Asynchronous Advantage Actor-Critic (A3C)**: Varian Actor-Critic di mana banyak agent belajar secara paralel dan tidak sinkron, memperbarui *master network*. DQN mengestimasi *advantage* setiap tindakan.\n",
        "* **Advantage Actor-Critic (A2C)**: Varian A3C tanpa asinkronisitas, dengan *updates* sinkron.\n",
        "* **Soft Actor-Critic (SAC)**: Belajar memaksimalkan *rewards* dan *entropy* tindakan (mencoba menjadi tidak dapat diprediksi), mendorong eksplorasi dan mempercepat pelatihan.\n",
        "* **Proximal Policy Optimization (PPO)**: Berbasis A2C yang membatasi fungsi *loss* untuk menghindari *updates* bobot yang terlalu besar dan ketidakstabilan pelatihan. Digunakan oleh OpenAI Five (Dota 2).\n",
        "* **Curiosity-based exploration**: Mengatasi kelangkaan *rewards* dengan membuat agent sangat ingin tahu untuk menjelajahi lingkungan, berdasarkan prediksi hasil tindakan dan mencari situasi di mana hasilnya tidak sesuai dengan prediksi. *Rewards* menjadi intrinsik bagi agent."
      ],
      "metadata": {
        "id": "LSdeFd0mwDly"
      }
    }
  ]
}