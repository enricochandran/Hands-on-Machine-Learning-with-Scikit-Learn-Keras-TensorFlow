{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Teknik *Unsupervised Learning*\n",
        "\n",
        "* **Clustering (Pengelompokan)**: Tujuannya adalah mengelompokkan *instance* yang serupa. Aplikasi *clustering* sangat luas, termasuk analisis data, segmentasi pelanggan, sistem rekomendasi, mesin pencari, segmentasi gambar, *semi-supervised learning*, dan *dimensionality reduction*. Beberapa algoritma *clustering* yang populer adalah K-Means dan DBSCAN.\n",
        "* **Anomaly Detection (Deteksi Anomali)**: Tujuannya adalah mempelajari seperti apa data \"normal\" dan kemudian menggunakannya untuk mengidentifikasi *instance* yang tidak normal, seperti produk cacat atau tren baru dalam *time series*. *Density estimation* sering digunakan untuk deteksi anomali, di mana *instance* di daerah dengan kepadatan sangat rendah kemungkinan besar adalah anomali.\n",
        "* **Density Estimation (Estimasi Kepadatan)**: Tugas ini adalah mengestimasi fungsi kepadatan probabilitas (PDF) dari proses acak yang menghasilkan *dataset*. Selain deteksi anomali, estimasi kepadatan juga berguna untuk analisis dan visualisasi data.\n",
        "\n",
        "### Clustering\n",
        "\n",
        "*Clustering* adalah tugas mengidentifikasi *instance* yang serupa dan menetapkannya ke dalam *cluster* atau kelompok. Setiap *instance* diberi kelompok, tetapi tidak seperti klasifikasi, *clustering* adalah tugas tanpa pengawasan.\n",
        "\n",
        "#### K-Means\n",
        "\n",
        "Algoritma K-Means adalah algoritma sederhana yang cepat dan efisien untuk mengelompokkan *dataset* yang memiliki struktur berbentuk gumpalan (*blob*).\n",
        "\n",
        "**Cara Kerja K-Means**:\n",
        "1.  **Inisialisasi Centroid**: Centroid ditempatkan secara acak (misalnya, dengan memilih $k$ *instance* secara acak dan menggunakan lokasinya sebagai centroid).\n",
        "2.  **Penetapan *Instance***: Setiap *instance* diberi label ke *cluster* yang centroidnya paling dekat.\n",
        "3.  **Pembaruan Centroid**: Centroid diperbarui dengan menghitung rata-rata *instance* untuk setiap *cluster*.\n",
        "4.  **Iterasi**: Langkah 2 dan 3 diulang sampai centroid berhenti bergerak.\n",
        "\n",
        "Algoritma ini dijamin akan konvergen dalam jumlah langkah yang terbatas.\n",
        "Meskipun K-Means cepat dan skalabel, ia memiliki beberapa kelemahan:\n",
        "* Perlu menjalankan algoritma beberapa kali untuk menghindari solusi suboptimal.\n",
        "* Perlu menentukan jumlah *cluster* ($k$) di awal.\n",
        "* Tidak bekerja dengan baik jika *cluster* memiliki ukuran yang bervariasi, kepadatan yang berbeda, atau bentuk non-sferis.\n",
        "* Penting untuk melakukan penskalaan fitur input sebelum menjalankan K-Means untuk menghindari *cluster* yang sangat meregang dan kinerja yang buruk.\n",
        "\n",
        "**Kode K-Means (dasar)**:\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "```\n",
        "Kode ini menginisialisasi model K-Means dengan 5 *cluster* dan kemudian melatihnya pada data `X`, mengembalikan label *cluster* yang diprediksi untuk setiap *instance*.\n",
        "\n",
        "**Centroid Initialization Methods**:\n",
        "Secara *default*, kelas `KMeans` menggunakan metode inisialisasi K-Means++. K-Means++ adalah langkah inisialisasi yang lebih cerdas yang cenderung memilih centroid yang berjauhan satu sama lain, mengurangi kemungkinan konvergensi ke solusi suboptimal.\n",
        "* **Inersia**: Metrik kinerja yang digunakan untuk memilih solusi terbaik di antara beberapa inisialisasi acak adalah inersia model, yaitu rata-rata kuadrat jarak antara setiap *instance* dan centroid terdekatnya. Nilai inersia yang lebih rendah menunjukkan model yang lebih baik.\n",
        "* **`n_init`**: Parameter ini mengontrol berapa kali algoritma dijalankan dengan inisialisasi acak yang berbeda. Secara *default*, nilainya adalah 10, dan Scikit-Learn akan menyimpan solusi dengan inersia terendah.\n",
        "\n",
        "**Accelerated K-Means and Mini-batch K-Means**:\n",
        "* **Accelerated K-Means**: Mempercepat algoritma dengan menghindari banyak perhitungan jarak yang tidak perlu, memanfaatkan ketidaksetaraan segitiga. Ini adalah algoritma yang digunakan oleh kelas `KMeans` secara *default*.\n",
        "* **Mini-batch K-Means**: Menggunakan *mini-batch* data pada setiap iterasi daripada seluruh *dataset*. Ini mempercepat algoritma secara signifikan dan memungkinkan pengelompokan *dataset* besar yang tidak muat dalam memori. Meskipun lebih cepat, inersianya umumnya sedikit lebih buruk dibandingkan K-Means reguler, terutama seiring dengan bertambahnya jumlah *cluster*.\n",
        "\n",
        "**Menemukan Jumlah *Cluster* Optimal ($k$)**:\n",
        "* **Elbow Method**: Memplot inersia sebagai fungsi $k$. Titik \"siku\" (*elbow*) pada kurva, di mana penurunan inersia melambat secara signifikan, seringkali merupakan pilihan yang baik untuk $k$.\n",
        "* **Silhouette Score**: Rata-rata koefisien siluet di semua *instance*. Koefisien siluet untuk suatu *instance* dihitung sebagai $(b-a)/\\max(a,b)$, di mana $a$ adalah jarak rata-rata ke *instance* lain dalam *cluster* yang sama, dan $b$ adalah jarak rata-rata ke *cluster* terdekat berikutnya. Nilai mendekati +1 menunjukkan *instance* yang terkelompok dengan baik, 0 berarti dekat batas *cluster*, dan -1 berarti mungkin salah ditetapkan. Skor siluet yang lebih tinggi menunjukkan *clustering* yang lebih baik.\n",
        "\n",
        "#### DBSCAN\n",
        "\n",
        "DBSCAN (*Density-Based Spatial Clustering of Applications with Noise*) mendefinisikan *cluster* sebagai wilayah kontinu dengan kepadatan tinggi.\n",
        "* **Epsilon ($\\epsilon$) Neighborhood**: Untuk setiap *instance*, algoritma menghitung berapa banyak *instance* yang terletak dalam jarak kecil $\\epsilon$ darinya.\n",
        "* **Core Instance**: Sebuah *instance* dianggap sebagai *core instance* jika memiliki setidaknya `min_samples` *instance* di dalam $\\epsilon$-*neighborhood*-nya (termasuk dirinya sendiri).\n",
        "* **Cluster Formation**: Semua *instance* di sekitar *core instance* termasuk dalam *cluster* yang sama. Urutan *core instance* yang berdekatan membentuk *cluster* tunggal.\n",
        "* **Anomali**: Setiap *instance* yang bukan *core instance* dan tidak memiliki *core instance* di sekitarnya dianggap sebagai anomali.\n",
        "\n",
        "Algoritma ini bekerja dengan baik jika semua *cluster* cukup padat dan dipisahkan dengan baik oleh wilayah berdensitas rendah. DBSCAN dapat mengidentifikasi *cluster* dengan bentuk arbitrer dan kuat terhadap *outlier*. Namun, jika kepadatan sangat bervariasi di seluruh *cluster*, mungkin sulit untuk menangkap semua *cluster* dengan benar.\n",
        "\n",
        "**Kode DBSCAN**:\n",
        "```python\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=1000, noise=0.05)\n",
        "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
        "dbscan.fit(X)\n",
        "```\n",
        "Kode ini membuat *dataset* berbentuk bulan dan kemudian menerapkan DBSCAN dengan parameter `eps` (radius *neighborhood*) dan `min_samples` (jumlah *instance* minimum dalam *neighborhood* agar menjadi *core instance*).\n",
        "\n",
        "DBSCAN tidak memiliki metode `predict()` untuk *instance* baru, tetapi Anda dapat melatih pengklasifikasi (misalnya, `KNeighborsClassifier`) pada *core instance* untuk memprediksi *cluster* *instance* baru.\n",
        "\n",
        "#### Algoritma Clustering Lainnya\n",
        "\n",
        "* **Agglomerative Clustering**: Membangun hierarki *cluster* dari bawah ke atas, secara bertahap menggabungkan pasangan *cluster* terdekat. Skalabel untuk jumlah *instance* atau *cluster* yang besar, terutama dengan matriks konektivitas.\n",
        "* **BIRCH (*Balanced Iterative Reducing and Clustering using Hierarchies*)**: Dirancang untuk *dataset* yang sangat besar, dapat lebih cepat dari K-Means *batch* dengan hasil serupa jika jumlah fitur tidak terlalu besar (<20).\n",
        "* **Mean-Shift**: Menemukan *local density maximum* dan mengelompokkan *instance* yang lingkarannya menyatu di tempat yang sama. Dapat menemukan sejumlah *cluster* dengan bentuk apa pun, tetapi kompleksitas komputasinya $O(m^2)$ membuatnya tidak cocok untuk *dataset* besar.\n",
        "* **Affinity Propagation**: Menggunakan sistem pemungutan suara di mana *instance* memilih *instance* serupa sebagai perwakilan mereka. Kompleksitas komputasinya juga $O(m^2)$.\n",
        "* **Spectral Clustering**: Mengambil matriks kesamaan antar *instance*, membuat *embedding* berdimensi rendah, lalu menggunakan algoritma *clustering* lain (misalnya, K-Means) di ruang berdimensi rendah ini. Tidak skalabel untuk jumlah *instance* yang besar dan tidak bekerja dengan baik ketika *cluster* memiliki ukuran yang sangat berbeda.\n",
        "\n",
        "### Gaussian Mixtures\n",
        "\n",
        "*Gaussian Mixture Model* (GMM) adalah model probabilistik yang mengasumsikan bahwa *instance* dihasilkan dari campuran beberapa distribusi Gaussian dengan parameter yang tidak diketahui. Setiap *cluster* dalam GMM dapat memiliki bentuk, ukuran, kepadatan, dan orientasi elipsoidal yang berbeda.\n",
        "\n",
        "**Cara Kerja GMM**:\n",
        "GMM menggunakan algoritma *Expectation-Maximization* (EM). EM mirip dengan K-Means:\n",
        "1.  **Inisialisasi Parameter Cluster**: Parameter *cluster* (bobot, rata-rata, dan matriks kovariansi) diinisialisasi secara acak.\n",
        "2.  **Expectation Step (Langkah E)**: Algoritma mengestimasi probabilitas setiap *instance* termasuk dalam setiap *cluster* (berdasarkan parameter *cluster* saat ini). Probabilitas ini disebut \"tanggung jawab\" *cluster* terhadap *instance*.\n",
        "3.  **Maximization Step (Langkah M)**: Setiap *cluster* diperbarui menggunakan semua *instance* dalam *dataset*, dengan setiap *instance* diberi bobot berdasarkan probabilitas estimasi bahwa ia termasuk dalam *cluster* tersebut.\n",
        "4.  **Iterasi**: Langkah E dan M diulang sampai konvergensi.\n",
        "\n",
        "Seperti K-Means, EM dapat konvergen ke solusi yang buruk, sehingga perlu dijalankan beberapa kali (parameter `n_init` di kelas `GaussianMixture` secara *default* adalah 1, tetapi disarankan untuk mengaturnya ke nilai yang lebih tinggi seperti 10).\n",
        "\n",
        "**Kode GMM**:\n",
        "```python\n",
        "from sklearn.mixture import GaussianMixture\n",
        "gm = GaussianMixture(n_components=3, n_init=10)\n",
        "gm.fit(X)\n",
        "```\n",
        "Kode ini melatih model GMM dengan 3 komponen Gaussian dan 10 inisialisasi.\n",
        "\n",
        "**Parameter yang Diestimasi**:\n",
        "* `gm.weights_`: Bobot relatif setiap *cluster*.\n",
        "* `gm.means_`: Rata-rata (pusat) setiap distribusi Gaussian.\n",
        "* `gm.covariances_`: Matriks kovariansi setiap distribusi Gaussian, yang mendefinisikan bentuk, ukuran, dan orientasi elipsoidal *cluster*.\n",
        "\n",
        "**Constraint pada Matriks Kovariansi**:\n",
        "Anda dapat membatasi bentuk dan orientasi *cluster* dengan mengatur parameter `covariance_type`:\n",
        "* `\"spherical\"`: Semua *cluster* harus berbentuk sferis dengan diameter yang berbeda.\n",
        "* `\"diag\"`: *Cluster* dapat berbentuk elipsoidal, tetapi sumbunya harus sejajar dengan sumbu koordinat (matriks kovariansi diagonal).\n",
        "* `\"tied\"`: Semua *cluster* memiliki bentuk, ukuran, dan orientasi elipsoidal yang sama (berbagi matriks kovariansi yang sama).\n",
        "* `\"full\"` (default): Setiap *cluster* dapat memiliki bentuk, ukuran, dan orientasi apa pun (memiliki matriks kovariansi yang tidak dibatasi sendiri).\n",
        "\n",
        "#### Anomaly Detection Menggunakan Gaussian Mixtures\n",
        "\n",
        "*Instance* yang terletak di wilayah berdensitas rendah dapat dianggap anomali. Anda dapat menggunakan metode `score_samples()` untuk mendapatkan log PDF dari setiap *instance*. Semakin besar skor, semakin tinggi kepadatannya. Dengan menentukan ambang batas kepadatan, *instance* di bawah ambang batas tersebut dapat ditandai sebagai anomali.\n",
        "\n",
        "**Kode Deteksi Anomali**:\n",
        "```python\n",
        "densities = gm.score_samples(X)\n",
        "density_threshold = np.percentile(densities, 4) # Misalnya, 4% anomali\n",
        "anomalies = X[densities < density_threshold]\n",
        "```\n",
        "Kode ini menghitung kepadatan untuk setiap *instance*, lalu menetapkan ambang batas pada persentil ke-4 dari kepadatan, menandai *instance* di bawah ambang batas tersebut sebagai anomali.\n",
        "\n",
        "#### Selecting the Number of Clusters (untuk GMM)\n",
        "\n",
        "Untuk GMM, inersia dan *silhouette score* tidak dapat diandalkan karena GMM menangani *cluster* yang tidak sferis atau memiliki ukuran yang berbeda. Sebagai gantinya, Anda dapat memilih model yang meminimalkan kriteria informasi teoretis seperti *Bayesian Information Criterion* (BIC) atau *Akaike Information Criterion* (AIC).\n",
        "\n",
        "* $BIC = \\log(m)p - 2 \\log(\\hat{L})$\n",
        "* $AIC = 2p - 2 \\log(\\hat{L})$\n",
        "\n",
        "Di mana:\n",
        "* $m$ adalah jumlah *instance*.\n",
        "* $p$ adalah jumlah parameter yang dipelajari oleh model.\n",
        "* $\\hat{L}$ adalah nilai maksimum fungsi *likelihood* model.\n",
        "\n",
        "BIC dan AIC memberikan penalti untuk model dengan lebih banyak parameter dan penghargaan untuk model yang cocok dengan data dengan baik. Nilai BIC atau AIC terendah menunjukkan jumlah *cluster* yang optimal.\n",
        "\n",
        "#### Bayesian Gaussian Mixture Models\n",
        "\n",
        "Kelas `BayesianGaussianMixture` dapat memberikan bobot nol (atau mendekati nol) kepada *cluster* yang tidak diperlukan. Anda dapat mengatur `n_components` ke nilai yang Anda yakini lebih besar dari jumlah *cluster* optimal, dan algoritma akan menghilangkan *cluster* yang tidak perlu secara otomatis. Dalam model ini, parameter *cluster* diperlakukan sebagai variabel acak laten, bukan parameter model tetap.\n",
        "\n",
        "**Kode Bayesian GMM**:\n",
        "```python\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "bgm = BayesianGaussianMixture(n_components=10, n_init=10)\n",
        "bgm.fit(X)\n",
        "np.round(bgm.weights_, 2)\n",
        "```\n",
        "Kode ini melatih Bayesian GMM dengan asumsi maksimal 10 komponen, dan kemudian mencetak bobot *cluster* yang terestimasi. Bobot yang mendekati nol menunjukkan *cluster* yang tidak digunakan oleh model.\n",
        "\n",
        "### Anomaly and Novelty Detection Lainnya\n",
        "\n",
        "* **PCA (*Principal Component Analysis*)**: Anomali cenderung memiliki kesalahan rekonstruksi yang jauh lebih besar dibandingkan *instance* normal setelah reduksi dimensi dan rekonstruksi.\n",
        "* **Fast-MCD (*Minimum Covariance Determinant*)**: Diimplementasikan oleh kelas `EllipticEnvelope`, berguna untuk deteksi *outlier*, terutama untuk membersihkan *dataset*. Diasumsikan *instance* normal berasal dari distribusi Gaussian tunggal, dan algoritma mengabaikan *outlier* saat mengestimasi parameter Gaussian.\n",
        "* **Isolation Forest**: Algoritma efisien untuk deteksi *outlier*, terutama di *dataset* berdimensi tinggi. Membangun *Random Forest* di mana *Decision Tree* dibuat secara acak, dan anomali cenderung terisolasi dalam langkah yang lebih sedikit.\n",
        "* **Local Outlier Factor (LOF)**: Membandingkan kepadatan *instance* di sekitar *instance* tertentu dengan kepadatan di sekitar tetangganya. Anomali seringkali lebih terisolasi.\n",
        "* **One-class SVM**: Lebih cocok untuk deteksi kebaruan (*novelty detection*). Algoritma ini mencoba memisahkan *instance* dalam ruang berdimensi tinggi dari titik asal, sehingga menghasilkan wilayah kecil yang mencakup semua *instance* normal. *Instance* baru yang tidak masuk dalam wilayah ini adalah anomali.\n",
        "\n",
        "### Menggunakan Clustering\n",
        "\n",
        "* **Image Segmentation**: Mengelompokkan piksel berdasarkan warna untuk mengurangi jumlah warna dalam gambar atau mengidentifikasi kontur objek.\n",
        "* **Preprocessing**: *Clustering* dapat digunakan untuk reduksi dimensi sebelum algoritma *supervised learning*. Misalnya, mengganti gambar dengan jaraknya ke sejumlah *cluster* dapat meningkatkan kinerja pengklasifikasi secara signifikan.\n",
        "* **Semi-Supervised Learning**: Dalam skenario dengan banyak *instance* tanpa label dan sedikit *instance* berlabel, *clustering* dapat digunakan untuk:\n",
        "    * Mengidentifikasi *instance* representatif (paling dekat dengan centroid *cluster*) dan memberi label secara manual hanya pada *instance* tersebut.\n",
        "    * *Label Propagation*: Menyebarkan label dari *instance* representatif ke semua *instance* lain dalam *cluster* yang sama. Dapat ditingkatkan dengan hanya menyebarkan label ke *instance* yang paling dekat dengan centroid untuk menghindari kesalahan pelabelan di dekat batas *cluster*.\n",
        "\n",
        "### Active Learning\n",
        "\n",
        "*Active learning* adalah ketika seorang ahli manusia berinteraksi dengan algoritma pembelajaran, memberikan label untuk *instance* tertentu ketika algoritma memintanya. Salah satu strategi umumnya adalah *uncertainty sampling*:\n",
        "1.  Model dilatih pada *instance* berlabel yang telah dikumpulkan.\n",
        "2.  Model memprediksi semua *instance* tanpa label.\n",
        "3.  *Instance* yang paling tidak pasti diprediksi oleh model (probabilitas estimasi terendah) diberikan kepada ahli untuk dilabeli.\n",
        "4.  Proses ini diulang sampai peningkatan kinerja tidak sebanding dengan upaya pelabelan."
      ],
      "metadata": {
        "id": "p-BIpeE_vIyZ"
      }
    }
  ]
}