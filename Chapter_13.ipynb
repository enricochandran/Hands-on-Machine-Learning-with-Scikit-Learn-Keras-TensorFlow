{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **The Data API**\n",
        "Seluruh Data API berkisar pada konsep _dataset_, yang merepresentasikan urutan item data. _Dataset_ dapat dibuat dari _tensor_ menggunakan `tf.data.Dataset.from_tensor_slices()`.\n",
        "\n",
        "```python\n",
        "x = tf.range(10) # any data tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
        "# dataset sekarang berisi item: tensor 0, 1, 2, ..., 9\n",
        "```\n",
        "\n",
        "Item _dataset_ dapat diulang menggunakan _loop_ `for`:\n",
        "```python\n",
        "for item in dataset:\n",
        "    print(item)\n",
        "```\n",
        "\n",
        "**Chaining Transformations**\n",
        "Setelah memiliki _dataset_, berbagai transformasi dapat diterapkan dengan memanggil metode transformasinya. Setiap metode mengembalikan _dataset_ baru, sehingga transformasi dapat dirantai.\n",
        "\n",
        "Contoh: `repeat(3)` dan `batch(7)`.\n",
        "```python\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)\n",
        "```\n",
        "* `repeat()`: Mengembalikan _dataset_ baru yang akan mengulang item dari _dataset_ asli sebanyak tiga kali. Ini tidak menyalin semua data ke dalam memori tiga kali. Jika dipanggil tanpa argumen, _dataset_ akan diulang selamanya.\n",
        "* `batch()`: Mengelompokkan item dari _dataset_ sebelumnya dalam _batch_ tujuh item. _Batch_ terakhir mungkin memiliki ukuran yang berbeda, tetapi dapat dihilangkan dengan `drop_remainder=True`.\n",
        "Metode _dataset_ tidak memodifikasi _dataset_ asli; mereka membuat yang baru.\n",
        "\n",
        "**Transformasi Item**\n",
        "* `map()`: Menerapkan transformasi ke setiap item. Misalnya, mendobel setiap item. Ini digunakan untuk _preprocessing_ data. Untuk mempercepat, argumen `num_parallel_calls` dapat diatur untuk _multithreading_. Fungsi yang diteruskan ke `map()` harus dapat diubah menjadi TF Function.\n",
        "    ```python\n",
        "    dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\n",
        "    ```\n",
        "* `apply()`: Menerapkan transformasi ke seluruh _dataset_. Contohnya adalah `tf.data.experimental.unbatch()`.\n",
        "* `filter()`: Menyaring _dataset_ berdasarkan suatu kondisi.\n",
        "* `take()`: Mengambil beberapa item dari _dataset_.\n",
        "\n",
        "**Shuffling Data**\n",
        "_Gradient Descent_ bekerja paling baik ketika _instance_ dalam _training set_ bersifat independen dan terdistribusi secara identik. Mengocok _instance_ dapat dilakukan dengan metode `shuffle()`.\n",
        "* `shuffle()`: Mengisi _buffer_ dengan item pertama dari _dataset_ sumber. Ketika item diminta, ia akan menarik satu secara acak dari _buffer_ dan menggantinya dengan yang baru dari _dataset_ sumber. Ukuran _buffer_ harus cukup besar agar _shuffling_ efektif.\n",
        "    ```python\n",
        "    dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
        "    dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "    for item in dataset:\n",
        "        print(item)\n",
        "    ```\n",
        "    Jika `repeat()` dipanggil pada _dataset_ yang di-_shuffle_, secara _default_ akan menghasilkan urutan baru di setiap iterasi. Ini dapat diubah dengan `reshuffle_each_iteration=False`.\n",
        "\n",
        "Untuk _dataset_ besar yang tidak muat di memori, _shuffling_ sumber data itu sendiri (misalnya, menggunakan perintah `shuf` di Linux) dapat meningkatkan _shuffling_. Pendekatan umum adalah membagi data sumber menjadi beberapa _file_, kemudian membacanya dalam urutan acak selama pelatihan. Untuk menghindari _instance_ dalam _file_ yang sama berakhir berdekatan, beberapa _file_ dapat dipilih secara acak dan dibaca secara bersamaan, dengan menginterleave catatan mereka. Kemudian _shuffling buffer_ dapat ditambahkan di atasnya.\n",
        "\n",
        "**Interleaving lines from multiple files**\n",
        "_Data API_ memungkinkan hal ini hanya dengan beberapa baris kode.\n",
        "1.  Buat _dataset_ berisi jalur _file_ menggunakan `tf.data.Dataset.list_files()`. Secara _default_, `list_files()` mengocok jalur _file_.\n",
        "    ```python\n",
        "    filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
        "    ```\n",
        "2.  Gunakan metode `interleave()` untuk membaca dari beberapa _file_ sekaligus dan menginterleave barisnya. _Callback_ fungsi (lambda dalam contoh) akan membuat _dataset_ baru untuk setiap jalur _file_ (misalnya, `tf.data.TextLineDataset`).\n",
        "    ```python\n",
        "    n_readers = 5\n",
        "    dataset = filepath_dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length=n_readers)\n",
        "    ```\n",
        "    Secara _default_, `interleave()` tidak menggunakan paralelisme. Untuk membaca _file_ secara paralel, atur argumen `num_parallel_calls` atau `tf.data.experimental.AUTOTUNE`.\n",
        "\n",
        "**Preprocessing the Data**\n",
        "Data perlu di-_parse_ dan di-_scale_. Contoh fungsi _preprocess_ untuk data CSV:\n",
        "```python\n",
        "X_mean, X_std = [...] # mean and scale of each feature in the training set\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return (x - X_mean) / X_std, y\n",
        "```\n",
        "* `tf.io.decode_csv()`: _Parse_ baris CSV. Argumen kedua adalah _array_ nilai _default_ untuk setiap kolom, yang juga memberi tahu TensorFlow jumlah kolom dan tipenya.\n",
        "* `tf.stack()`: Menggabungkan _scalar tensors_ menjadi _1D tensor arrays_.\n",
        "* Fitur _input_ diskalakan dengan mengurangi _mean_ dan membagi dengan _standard deviation_.\n",
        "\n",
        "**Putting Everything Together**\n",
        "Fungsi _helper_ `csv_reader_dataset` dapat dibuat untuk memuat, memproses, mengocok, mengulang, dan _batching_ data secara efisien.\n",
        "```python\n",
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
        "                       n_read_threads=None, shuffle_buffer_size=1000,\n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths)\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
        "    return dataset.batch(batch_size).prefetch(1)\n",
        "```\n",
        "\n",
        "**Prefetching**\n",
        "Memanggil `prefetch(1)` di akhir _pipeline_ akan membuat _dataset_ selalu selangkah lebih maju, menyiapkan _batch_ berikutnya secara paralel saat _batch_ saat ini sedang diproses oleh algoritma pelatihan. Ini dapat meningkatkan kinerja secara dramatis. Dengan _multithreaded loading_ dan _preprocessing_ (mengatur `num_parallel_calls` di `interleave()` dan `map()`), GPU dapat dimanfaatkan hampir 100%.\n",
        "\n",
        "Jika _dataset_ cukup kecil untuk muat di memori, `cache()` dapat mempercepat pelatihan secara signifikan dengan menyimpan konten ke RAM. Ini harus dilakukan setelah memuat dan memproses data, tetapi sebelum _shuffling_, _repeating_, _batching_, dan _prefetching_.\n",
        "\n",
        "**Using the Dataset with tf.keras**\n",
        "_Dataset_ yang dibuat dapat langsung diteruskan ke metode `fit()`, `evaluate()`, dan `predict()` pada model Keras.\n",
        "```python\n",
        "train_set = csv_reader_dataset(train_filepaths)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)\n",
        "\n",
        "model = keras.models.Sequential([...])\n",
        "model.compile([...])\n",
        "model.fit(train_set, epochs=10, validation_data=valid_set)\n",
        "```\n",
        "Keras akan menangani pengulangan _training dataset_.\n",
        "\n",
        "**The TFRecord Format**\n",
        "TFRecord adalah format biner pilihan TensorFlow untuk menyimpan data dalam jumlah besar dan membacanya secara efisien. Ini hanya berisi urutan catatan biner dengan ukuran bervariasi.\n",
        "* Membuat _file_ TFRecord menggunakan `tf.io.TFRecordWriter`.\n",
        "    ```python\n",
        "    with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "        f.write(b\"This is the first record\")\n",
        "        f.write(b\"And this is the second record\")\n",
        "    ```\n",
        "* Membaca _file_ TFRecord menggunakan `tf.data.TFRecordDataset`.\n",
        "    ```python\n",
        "    filepaths = [\"my_data.tfrecord\"]\n",
        "    dataset = tf.data.TFRecordDataset(filepaths)\n",
        "    for item in dataset:\n",
        "        print(item)\n",
        "    ```\n",
        "    Secara _default_, `TFRecordDataset` akan membaca _file_ satu per satu, tetapi dapat membaca beberapa _file_ secara paralel dan menginterleave catatannya dengan mengatur `num_parallel_reads`.\n",
        "\n",
        "**Compressed TFRecord Files**\n",
        "TFRecord _file_ dapat dikompresi, terutama jika perlu dimuat melalui koneksi jaringan.\n",
        "* Membuat _file_ TFRecord terkompresi dengan mengatur argumen `options`.\n",
        "    ```python\n",
        "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "    with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
        "        # ...\n",
        "    ```\n",
        "* Saat membaca _file_ TFRecord terkompresi, jenis kompresi perlu ditentukan.\n",
        "    ```python\n",
        "    dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
        "                                      compression_type=\"GZIP\")\n",
        "    ```\n",
        "\n",
        "**A Brief Introduction to Protocol Buffers**\n",
        "_File_ TFRecord biasanya berisi _serialized protocol buffers_ (protobufs). Ini adalah format biner yang portabel, _extensible_, dan efisien yang dikembangkan oleh Google. Protobufs didefinisikan menggunakan bahasa sederhana. TensorFlow menyertakan definisi protobuf khusus yang menyediakan operasi _parsing_.\n",
        "\n",
        "**TensorFlow Protobufs**\n",
        "Protobuf utama yang biasanya digunakan dalam _file_ TFRecord adalah _Example protobuf_, yang merepresentasikan satu _instance_ dalam _dataset_. Ini berisi daftar fitur bernama, di mana setiap fitur dapat berupa daftar _byte strings_, _floats_, atau _integers_.\n",
        "```python\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example\n",
        "\n",
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
        "                                                        b\"c@d.com\"]))\n",
        "        }))\n",
        "```\n",
        "Objek _Example_ dapat diserialisasi menggunakan `SerializeToString()` dan kemudian ditulis ke _file_ TFRecord.\n",
        "\n",
        "**Loading and Parsing Examples**\n",
        "Untuk memuat _serialized Example protobufs_, gunakan `tf.data.TFRecordDataset` dan _parse_ setiap _Example_ menggunakan `tf.io.parse_single_example()`. Ini adalah operasi TensorFlow dan dapat dimasukkan dalam TF Function. Ini membutuhkan data _serialized_ dan deskripsi setiap fitur. Deskripsi adalah _dictionary_ yang memetakan nama fitur ke _descriptor_ `tf.io.FixedLenFeature` (untuk fitur panjang tetap) atau `tf.io.VarLenFeature` (untuk fitur panjang bervariasi).\n",
        "```python\n",
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
        "}\n",
        "\n",
        "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
        "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
        "                                                feature_description)\n",
        "```\n",
        "Fitur panjang tetap di-_parse_ sebagai _regular tensors_, sedangkan fitur panjang variabel di-_parse_ sebagai _sparse tensors_. _Sparse tensor_ dapat diubah menjadi _dense tensor_ menggunakan `tf.sparse.to_dense()` atau nilai-nilainya dapat diakses langsung.\n",
        "\n",
        "`BytesList` dapat berisi data biner apa pun. Gambar dapat di-_encode_ menggunakan `tf.io.encode_jpeg()` dan kemudian di-_decode_ menggunakan `tf.io.decode_jpeg()` atau `tf.io.decode_image()`. _Tensor_ dapat diserialisasi dengan `tf.io.serialize_tensor()` dan di-_parse_ dengan `tf.io.parse_tensor()`.\n",
        "\n",
        "Untuk _parse_ _Example_ secara _batch_, gunakan `tf.io.parse_example()`.\n",
        "```python\n",
        "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\n",
        "for serialized_examples in dataset:\n",
        "    parsed_examples = tf.io.parse_example(serialized_examples,\n",
        "                                          feature_description)\n",
        "```\n",
        "\n",
        "**Handling Lists of Lists Using the SequenceExample Protobuf**\n",
        "_SequenceExample protobuf_ dirancang untuk kasus penggunaan dengan daftar-daftar (misalnya, dokumen sebagai daftar kalimat, setiap kalimat sebagai daftar kata).\n",
        "* _SequenceExample_ berisi objek `Features` untuk data kontekstual dan objek `FeatureLists` yang berisi satu atau lebih objek `FeatureList` bernama. Setiap `FeatureList` berisi daftar objek `Feature`.\n",
        "* _SequenceExample_ di-_parse_ menggunakan `tf.io.parse_single_sequence_example()` untuk _single example_ atau `tf.io.parse_sequence_example()` untuk _batch_. Fungsi-fungsi ini mengembalikan _tuple_ berisi fitur konteks (_dictionary_) dan daftar fitur (_dictionary_). Jika daftar fitur berisi urutan dengan ukuran bervariasi, dapat diubah menjadi _ragged tensors_ menggunakan `tf.RaggedTensor.from_sparse()`.\n",
        "\n",
        "**Preprocessing the Input Features**\n",
        "Mempersiapkan data untuk _neural network_ melibatkan konversi semua fitur menjadi fitur numerik, umumnya menormalisasikannya. Fitur kategorikal atau teks perlu dikonversi ke angka. Ini dapat dilakukan sebelumnya saat menyiapkan _file_ data, dalam _tf.data pipeline_, atau dalam _preprocessing layers_ di dalam model.\n",
        "\n",
        "**Standardization Layer (Custom)**\n",
        "Contoh _standardization layer_ menggunakan _Lambda layer_ atau _custom layer_.\n",
        "```python\n",
        "# Menggunakan Lambda layer:\n",
        "means = np.mean(X_train, axis=0, keepdims=True)\n",
        "stds = np.std(X_train, axis=0, keepdims=True)\n",
        "eps = keras.backend.epsilon()\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),\n",
        "    # ... other layers\n",
        "])\n",
        "\n",
        "# Custom layer:\n",
        "class Standardization(keras.layers.Layer):\n",
        "    def adapt(self, data_sample):\n",
        "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
        "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
        "    def call(self, inputs):\n",
        "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
        "\n",
        "# Penggunaan custom layer:\n",
        "std_layer = Standardization()\n",
        "std_layer.adapt(data_sample)\n",
        "model = keras.Sequential()\n",
        "model.add(std_layer)\n",
        "# ... create the rest of the model\n",
        "```\n",
        "_Keras_ kemungkinan akan menyediakan _keras.layers.Normalization_ layer yang bekerja serupa.\n",
        "\n",
        "**Encoding Categorical Features Using One-Hot Vectors**\n",
        "Untuk fitur kategorikal dengan sedikit kategori, _one-hot encoding_ dapat digunakan. Ini melibatkan pemetaan setiap kategori ke indeksnya menggunakan _lookup table_.\n",
        "```python\n",
        "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
        "indices = tf.range(len(vocab), dtype=tf.int64)\n",
        "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
        "num_oov_buckets = 2\n",
        "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
        "\n",
        "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
        "cat_indices = table.lookup(categories)\n",
        "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
        "```\n",
        "_Out-of-vocabulary (oov)_ _buckets_ digunakan untuk kategori yang tidak ada dalam _vocabulary_. `tf.one_hot()` mengonversi indeks menjadi _one-hot vectors_. _Keras_ kemungkinan akan menyertakan _keras.layers.TextVectorization_ yang dapat melakukan hal ini.\n",
        "\n",
        "**Encoding Categorical Features Using Embeddings**\n",
        "_Embedding_ adalah _dense vector_ yang dapat dilatih yang merepresentasikan sebuah kategori. _Embeddings_ diinisialisasi secara acak dan akan membaik selama pelatihan.\n",
        "* Ukuran _embedding_ adalah _hyperparameter_.\n",
        "* _Embedding matrix_ dibuat dengan satu baris per kategori (dan _oov bucket_) dan satu kolom per dimensi _embedding_.\n",
        "    ```python\n",
        "    embedding_dim = 2\n",
        "    embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
        "    embedding_matrix = tf.Variable(embed_init)\n",
        "    ```\n",
        "* `tf.nn.embedding_lookup()`: Mencari baris dalam _embedding matrix_ pada indeks yang diberikan.\n",
        "    ```python\n",
        "    tf.nn.embedding_lookup(embedding_matrix, cat_indices)\n",
        "    ```\n",
        "* _Keras_ menyediakan _keras.layers.Embedding_ layer yang menangani _embedding matrix_.\n",
        "    ```python\n",
        "    embedding = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets,\n",
        "                                        output_dim=embedding_dim)\n",
        "    embedding(cat_indices)\n",
        "    ```\n",
        "\n",
        "Model Keras dapat mengombinasikan _regular inputs_ dengan _categorical inputs_ yang di-_encode_ menggunakan _embeddings_.\n",
        "```python\n",
        "regular_inputs = keras.layers.Input(shape=[8])\n",
        "categories = keras.layers.Input(shape=[], dtype=tf.string)\n",
        "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
        "cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
        "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
        "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
        "model = keras.models.Model(inputs=[regular_inputs, categories],\n",
        "                           outputs=[outputs])\n",
        "```\n",
        "\n",
        "**Keras Preprocessing Layers**\n",
        "Tim TensorFlow sedang mengembangkan serangkaian _standard Keras preprocessing layers_.\n",
        "* `keras.layers.Normalization`: Melakukan standardisasi fitur.\n",
        "* `keras.layers.TextVectorization`: Meng-_encode_ setiap kata menjadi indeksnya dalam _vocabulary_.\n",
        "* `keras.layers.Discretization`: Memotong data kontinu menjadi beberapa _bin_ dan meng-_encode_ setiap _bin_ sebagai _one-hot vector_.\n",
        "    ```python\n",
        "    normalization = keras.layers.Normalization()\n",
        "    discretization = keras.layers.Discretization([...])\n",
        "    pipeline = keras.layers.PreprocessingStage([normalization, discretization])\n",
        "    pipeline.adapt(data_sample)\n",
        "    ```\n",
        "_Preprocessing layers_ model akan dibekukan selama pelatihan, sehingga parameternya tidak akan terpengaruh oleh _Gradient Descent_.\n",
        "\n",
        "`TextVectorization` juga akan memiliki opsi untuk menghasilkan _word-count vectors_ (_bag of words_). _Word counts_ dapat dinormalisasi menggunakan _Term-Frequency Inverse-Document-Frequency (TF-IDF)_.\n",
        "\n",
        "**TF Transform**\n",
        "Jika _preprocessing_ mahal secara komputasi, menanganinya sebelum pelatihan dapat memberikan _speedup_ yang signifikan. TF Transform memungkinkan untuk menulis satu fungsi _preprocessing_ yang dapat dijalankan dalam mode _batch_ pada _training set_ lengkap sebelum pelatihan, dan kemudian diekspor ke TF Function dan dimasukkan ke dalam model yang dilatih untuk menangani _preprocessing_ _instance_ baru saat _deployed_. Ini menghindari masalah _training/serving skew_.\n",
        "```python\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "def preprocess(inputs): # inputs a batch of input features\n",
        "    median_age = inputs[\"housing_median_age\"]\n",
        "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
        "    standardized_age = tft.scale_to_z_score(median_age)\n",
        "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
        "    return {\n",
        "        \"standardized_median_age\": standardized_age,\n",
        "        \"ocean_proximity_id\": ocean_proximity_id\n",
        "    }\n",
        "```\n",
        "TF Transform dapat menerapkan fungsi `preprocess()` ke seluruh _training set_ menggunakan Apache Beam. Ini juga akan menghitung statistik yang diperlukan (misalnya, _mean_, _standard deviation_, _vocabulary_). Selain itu, TF Transform akan menghasilkan TF Function yang setara yang dapat dicolokkan ke model yang akan _deployed_.\n",
        "\n",
        "**The TensorFlow Datasets (TFDS) Project**\n",
        "TFDS mempermudah pengunduhan _common dataset_.\n",
        "* Instal _tensorflow-datasets_ library.\n",
        "* Panggil `tfds.load()` untuk mengunduh data dan mengembalikan data sebagai _dictionary_ dari _datasets_.\n",
        "    ```python\n",
        "    import tensorflow_datasets as tfds\n",
        "\n",
        "    dataset = tfds.load(name=\"mnist\")\n",
        "    mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\n",
        "    ```\n",
        "* Dapat menerapkan transformasi apa pun (misalnya, _shuffling_, _batching_, dan _prefetching_).\n",
        "* Untuk model Keras, setiap item harus berupa _tuple_ berisi fitur dan _label_. Ini dapat diatur dengan `as_supervised=True` saat memanggil `load()`.\n",
        "    ```python\n",
        "    dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
        "    mnist_train = dataset[\"train\"].prefetch(1)\n",
        "\n",
        "    model = keras.models.Sequential([...])\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "    model.fit(mnist_train, epochs=5)\n",
        "    ```"
      ],
      "metadata": {
        "id": "eD8B8H1pwB8L"
      }
    }
  ]
}