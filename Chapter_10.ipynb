{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Pengantar Jaringan Saraf Tiruan (ANN)**\n",
        "\n",
        "ANN terinspirasi dari arsitektur otak biologis. Meskipun pada awalnya meniru neuron biologis, ANN telah berkembang dan kini cukup berbeda dari \"sepupu biologis\" mereka. ANN adalah inti dari Deep Learning, serbaguna, kuat, dan skalabel, sehingga ideal untuk tugas Machine Learning yang kompleks seperti klasifikasi gambar (misalnya, Google Images), pengenalan suara (misalnya, Apple's Siri), rekomendasi video (misalnya, YouTube), atau mengalahkan juara dunia Go (DeepMind's AlphaGo).\n",
        "\n",
        "**Sejarah Singkat ANN**\n",
        "\n",
        "ANN pertama kali diperkenalkan pada tahun 1943 oleh neurofisiolog Warren McCulloch dan matematikawan Walter Pitts. Mereka menyajikan model komputasi sederhana tentang bagaimana neuron biologis dapat bekerja sama untuk melakukan komputasi kompleks menggunakan logika proposisional. Kesuksesan awal ANN menyebabkan keyakinan luas bahwa mesin cerdas akan segera terwujud. Namun, di tahun 1960-an, janji ini tidak terpenuhi, sehingga pendanaan dialihkan dan ANN mengalami \"musim dingin\" yang panjang.\n",
        "\n",
        "Pada awal 1980-an, arsitektur baru dan teknik pelatihan yang lebih baik dikembangkan, memicu kebangkitan minat pada koneksionisme (studi tentang jaringan saraf). Namun, kemajuan lambat, dan pada 1990-an, teknik Machine Learning lain seperti Support Vector Machines ditemukan, yang menawarkan hasil lebih baik dan fondasi teoretis lebih kuat.\n",
        "\n",
        "Saat ini, kita menyaksikan gelombang minat baru pada ANN, yang dipercaya akan memiliki dampak lebih mendalam karena beberapa alasan:\n",
        "* Tersedianya data dalam jumlah besar untuk melatih jaringan saraf, dan ANN sering mengungguli teknik ML lain pada masalah yang sangat besar dan kompleks.\n",
        "* Peningkatan daya komputasi yang luar biasa sejak 1990-an, sebagian berkat Hukum Moore dan industri industri game yang memicu produksi kartu GPU.\n",
        "* Algoritma pelatihan telah ditingkatkan, meskipun hanya dengan sedikit perubahan dari tahun 1990-an, tetapi dengan dampak positif yang besar.\n",
        "* Beberapa batasan teoretis ANN ternyata jinak dalam praktiknya (misalnya, masalah *local optima* jarang terjadi).\n",
        "* ANN telah memasuki lingkaran kebajikan pendanaan dan kemajuan, di mana produk luar biasa berdasarkan ANN secara teratur menjadi berita utama, menarik lebih banyak perhatian dan pendanaan.\n",
        "\n",
        "**Neuron Biologis**\n",
        "Neuron biologis adalah sel unik yang ditemukan di otak hewan. Terdiri dari badan sel (mengandung nukleus dan komponen kompleks), banyak ekstensi bercabang yang disebut dendrit, dan satu ekstensi sangat panjang yang disebut akson. Akson bercabang menjadi telodendria, dan di ujung cabang ini terdapat struktur kecil yang disebut terminal sinaptik (atau sinapsis), yang terhubung ke dendrit atau badan sel neuron lain. Neuron biologis menghasilkan impuls listrik pendek yang disebut *action potentials* (APs atau sinyal), yang bergerak di sepanjang akson dan membuat sinapsis melepaskan sinyal kimia yang disebut neurotransmiter. Ketika neuron menerima neurotransmiter dalam jumlah yang cukup dalam beberapa milidetik, ia akan menembakkan impuls listriknya sendiri.\n",
        "\n",
        "**Komputasi Logika dengan Neuron**\n",
        "McCulloch dan Pitts mengusulkan model neuron biologis yang sangat sederhana, yang dikenal sebagai neuron tiruan. Neuron ini memiliki satu atau lebih input biner (on/off) dan satu output biner. Neuron tiruan mengaktifkan output-nya ketika lebih dari sejumlah inputnya aktif. Mereka menunjukkan bahwa bahkan dengan model sederhana seperti itu, jaringan neuron tiruan dapat dibangun untuk menghitung proposisi logis apa pun.\n",
        "\n",
        "Contoh jaringan ANN yang melakukan komputasi logis sederhana (dengan asumsi neuron diaktifkan ketika setidaknya dua inputnya aktif):\n",
        "* **Fungsi Identitas ($C=A$)**: Jika neuron A diaktifkan, neuron C juga aktif (menerima dua sinyal input dari A). Jika A mati, C juga mati.\n",
        "* **AND Logika ($C=A \\land B$)**: Neuron C diaktifkan hanya ketika neuron A dan B keduanya diaktifkan (satu sinyal input tidak cukup).\n",
        "* **OR Logika ($C=A \\lor B$)**: Neuron C diaktifkan jika neuron A atau B diaktifkan (atau keduanya).\n",
        "* **A AND NOT B ($C=A \\land \\neg B$)**: Jika koneksi input dapat menghambat aktivitas neuron (seperti pada neuron biologis), neuron C diaktifkan hanya jika neuron A aktif dan neuron B mati. Jika A selalu aktif, ini menjadi NOT logika ($C=\\neg B$).\n",
        "\n",
        "**Perceptron**\n",
        "Perceptron adalah salah satu arsitektur ANN paling sederhana, ditemukan pada tahun 1957 oleh Frank Rosenblatt. Ini didasarkan pada *threshold logic unit* (TLU) atau *linear threshold unit* (LTU). Input dan output TLU adalah angka, dan setiap koneksi input dikaitkan dengan bobot. TLU menghitung jumlah bobot dari inputnya ($z = w_1x_1 + w_2x_2 + \\dots + w_nx_n = \\mathbf{x}^T\\mathbf{w}$), lalu menerapkan fungsi langkah (*step function*) ke jumlah tersebut dan mengeluarkan hasilnya: $h_{\\mathbf{w}}(\\mathbf{x}) = \\text{step}(z)$, di mana $z = \\mathbf{x}^T\\mathbf{w}$.\n",
        "\n",
        "Fungsi langkah paling umum yang digunakan dalam Perceptron adalah fungsi langkah Heaviside:\n",
        "$$\\text{heaviside}(z) = \\begin{cases} 0 & \\text{if } z < 0 \\\\ 1 & \\text{if } z \\geq 0 \\end{cases}$$Terkadang fungsi *sign* digunakan sebagai gantinya:$$\\text{sgn}(z) = \\begin{cases} -1 & \\text{if } z < 0 \\\\ 0 & \\text{if } z = 0 \\\\ +1 & \\text{if } z > 0 \\end{cases}$$\n",
        "Sebuah TLU tunggal dapat digunakan untuk klasifikasi biner linier sederhana. Ini menghitung kombinasi linier input, dan jika hasilnya melebihi ambang batas, itu mengeluarkan kelas positif; jika tidak, itu mengeluarkan kelas negatif (mirip dengan Regresi Logistik atau pengklasifikasi SVM linier).\n",
        "\n",
        "Sebuah Perceptron hanya terdiri dari satu lapisan TLU, dengan setiap TLU terhubung ke semua input. Lapisan di mana semua neuron terhubung ke setiap neuron di lapisan sebelumnya (neuron inputnya) disebut *fully connected layer* atau *dense layer*. Input Perceptron dimasukkan ke neuron *passthrough* khusus yang disebut neuron input. Semua neuron input membentuk *input layer*. Selain itu, fitur bias ekstra umumnya ditambahkan ($x_0=1$), yang biasanya direpresentasikan menggunakan neuron bias khusus yang selalu mengeluarkan 1.\n",
        "\n",
        "Output dari sebuah lapisan neuron tiruan dapat dihitung secara efisien untuk beberapa instance sekaligus menggunakan aljabar linier:\n",
        "$$h_{\\mathbf{W},\\mathbf{b}}(\\mathbf{X}) = \\phi(\\mathbf{X}\\mathbf{W} + \\mathbf{b})$$\n",
        "Di mana:\n",
        "* $\\mathbf{X}$ adalah matriks fitur input (satu baris per instance, satu kolom per fitur).\n",
        "* $\\mathbf{W}$ adalah matriks bobot (satu baris per neuron input, satu kolom per neuron tiruan di lapisan).\n",
        "* $\\mathbf{b}$ adalah vektor bias (satu istilah bias per neuron tiruan).\n",
        "* $\\phi$ adalah fungsi aktivasi (misalnya, fungsi langkah untuk TLU).\n",
        "\n",
        "**Pelatihan Perceptron**\n",
        "Algoritma pelatihan Perceptron, yang diusulkan oleh Rosenblatt, sebagian besar terinspirasi oleh aturan Hebb. Aturan Hebb menyatakan bahwa ketika satu neuron biologis memicu neuron lain sering, koneksi antara kedua neuron ini menjadi lebih kuat. Perceptron dilatih menggunakan varian aturan ini yang mempertimbangkan kesalahan yang dibuat oleh jaringan saat membuat prediksi; aturan pembelajaran Perceptron memperkuat koneksi yang membantu mengurangi kesalahan.\n",
        "\n",
        "Lebih spesifik, Perceptron diberi satu instance pelatihan pada satu waktu, dan untuk setiap instance ia membuat prediksinya. Untuk setiap neuron output yang menghasilkan prediksi yang salah, ia memperkuat bobot koneksi dari input yang akan berkontribusi pada prediksi yang benar. Aturan pembaruan bobot Perceptron adalah:\n",
        "$$w_{i,j}^{\\text{(next step)}} = w_{i,j} + \\eta (y_j - \\hat{y}_j) x_i$$\n",
        "Di mana:\n",
        "* $w_{i,j}$ adalah bobot koneksi antara neuron input ke-$i$ dan neuron output ke-$j$.\n",
        "* $x_i$ adalah nilai input ke-$i$ dari instance pelatihan saat ini.\n",
        "* $\\hat{y}_j$ adalah output dari neuron output ke-$j$ untuk instance pelatihan saat ini.\n",
        "* $\\eta$ adalah *learning rate*.\n",
        "\n",
        "Batasan keputusan setiap neuron output adalah linier, sehingga Perceptron tidak mampu mempelajari pola kompleks. Namun, jika instance pelatihan dapat dipisahkan secara linier, algoritma ini akan konvergen ke solusi. Ini disebut teorema konvergensi Perceptron.\n",
        "\n",
        "**Contoh Implementasi Perceptron dengan Scikit-Learn**\n",
        "Scikit-Learn menyediakan kelas `Perceptron` yang mengimplementasikan jaringan TLU tunggal.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
        "y = (iris.target == 0).astype(np.int) # Iris setosa?\n",
        "\n",
        "per_clf = Perceptron()\n",
        "per_clf.fit(X, y)\n",
        "\n",
        "y_pred = per_clf.predict([[2, 0.5]])\n",
        "print(y_pred)\n",
        "```\n",
        "Kode ini melatih Perceptron untuk mengklasifikasikan bunga Iris Setosa berdasarkan panjang dan lebar kelopak. `Perceptron` Scikit-Learn setara dengan menggunakan `SGDClassifier` dengan `loss=\"perceptron\"`, `learning_rate=\"constant\"`, `eta0=1`, dan `penalty=None`. Perceptron tidak mengeluarkan probabilitas kelas, melainkan membuat prediksi berdasarkan ambang batas keras.\n",
        "\n",
        "**Multilayer Perceptron (MLP)**\n",
        "Kelemahan serius Perceptron, seperti ketidakmampuan memecahkan masalah XOR, dapat diatasi dengan menumpuk beberapa Perceptron. ANN yang dihasilkan disebut Multilayer Perceptron (MLP). MLP dapat memecahkan masalah XOR.\n",
        "\n",
        "Sebuah MLP terdiri dari satu *input layer* (passthrough), satu atau lebih lapisan TLU yang disebut *hidden layers*, dan satu lapisan TLU terakhir yang disebut *output layer*. Lapisan yang dekat dengan *input layer* disebut *lower layers*, dan yang dekat dengan output disebut *upper layers*. Setiap lapisan kecuali *output layer* mencakup neuron bias dan sepenuhnya terhubung ke lapisan berikutnya. Sinyal mengalir hanya dalam satu arah (dari input ke output), sehingga arsitektur ini adalah contoh *feedforward neural network* (FNN). Ketika ANN berisi tumpukan lapisan tersembunyi yang \"dalam\", itu disebut *deep neural network* (DNN).\n",
        "\n",
        "**Backpropagation Training Algorithm**\n",
        "Selama bertahun-tahun, para peneliti kesulitan menemukan cara untuk melatih MLP. Namun, pada tahun 1986, David Rumelhart, Geoffrey Hinton, dan Ronald Williams menerbitkan makalah yang memperkenalkan algoritma pelatihan *backpropagation*, yang masih digunakan hingga saat ini. Singkatnya, ini adalah Gradient Descent menggunakan teknik efisien untuk menghitung gradien secara otomatis. Dalam dua *pass* melalui jaringan (satu maju, satu mundur), algoritma *backpropagation* dapat menghitung gradien kesalahan jaringan terhadap setiap parameter model. Ini berarti ia dapat mengetahui bagaimana setiap bobot koneksi dan istilah bias harus diubah untuk mengurangi kesalahan. Setelah gradien ini diperoleh, ia melakukan langkah Gradient Descent biasa, dan seluruh proses diulang hingga jaringan konvergen ke solusi.\n",
        "\n",
        "Penghitungan gradien secara otomatis disebut *automatic differentiation* atau *autodiff*. Teknik *autodiff* yang digunakan oleh *backpropagation* disebut *reverse-mode autodiff*, yang cepat dan presisi, serta cocok ketika fungsi yang akan dibedakan memiliki banyak variabel (bobot koneksi) dan sedikit output (misalnya, satu *loss*).\n",
        "\n",
        "Detail algoritma *backpropagation*:\n",
        "* Menangani satu *mini-batch* pada satu waktu (misalnya, berisi 32 instance), dan melalui *training set* penuh beberapa kali. Setiap *pass* disebut *epoch*.\n",
        "* Setiap *mini-batch* dilewatkan ke *input layer* jaringan, yang mengirimkannya ke *hidden layer* pertama. Algoritma kemudian menghitung output semua neuron di lapisan ini (untuk setiap instance dalam *mini-batch*). Hasilnya diteruskan ke lapisan berikutnya, outputnya dihitung dan diteruskan ke lapisan berikutnya, dan seterusnya hingga output lapisan terakhir (*output layer*) diperoleh. Ini adalah *forward pass*: seperti membuat prediksi, kecuali semua hasil antara disimpan karena diperlukan untuk *backward pass*.\n",
        "* Selanjutnya, algoritma mengukur kesalahan output jaringan (menggunakan fungsi *loss* yang membandingkan output yang diinginkan dengan output aktual).\n",
        "* Kemudian ia menghitung seberapa besar kontribusi setiap koneksi output terhadap kesalahan. Ini dilakukan secara analitis dengan menerapkan *chain rule*, yang membuat langkah ini cepat dan presisi.\n",
        "* Algoritma kemudian mengukur seberapa banyak kontribusi kesalahan ini berasal dari setiap koneksi di lapisan bawah, lagi-lagi menggunakan *chain rule*, bekerja mundur hingga algoritma mencapai *input layer*. *Reverse pass* ini secara efisien mengukur gradien kesalahan di semua bobot koneksi dalam jaringan dengan menyebarkan gradien kesalahan mundur melalui jaringan.\n",
        "* Akhirnya, algoritma melakukan langkah Gradient Descent untuk menyesuaikan semua bobot koneksi dalam jaringan, menggunakan gradien kesalahan yang baru saja dihitung.\n",
        "\n",
        "Penting untuk menginisialisasi bobot koneksi *hidden layers* secara acak, atau pelatihan akan gagal. Jika semua bobot dan bias diinisialisasi ke nol, semua neuron dalam lapisan tertentu akan identik, dan *backpropagation* akan memengaruhinya dengan cara yang sama, sehingga mereka akan tetap identik. Dengan inisialisasi bobot secara acak, simetri rusak dan memungkinkan *backpropagation* melatih tim neuron yang beragam.\n",
        "\n",
        "Agar algoritma ini berfungsi dengan baik, para penulis membuat perubahan kunci pada arsitektur MLP: mereka mengganti fungsi langkah dengan fungsi logistik (sigmoid), $\\sigma(z) = 1/(1 + \\exp(-z))$. Ini penting karena fungsi langkah hanya berisi segmen datar, sehingga tidak ada gradien yang dapat digunakan oleh Gradient Descent, sementara fungsi logistik memiliki turunan nonzero yang terdefinisi dengan baik di mana-mana, memungkinkan Gradient Descent membuat kemajuan di setiap langkah.\n",
        "\n",
        "**Fungsi Aktivasi Populer Lainnya**\n",
        "* **Fungsi *hyperbolic tangent* ($\\tanh(z) = 2\\sigma(2z) - 1$)**: Mirip dengan fungsi logistik, S-shaped, kontinu, dan dapat dibedakan. Namun, nilai outputnya berkisar dari -1 hingga 1 (bukan 0 hingga 1), yang cenderung membuat output setiap lapisan lebih atau kurang terpusat di sekitar 0 pada awal pelatihan, seringkali membantu mempercepat konvergensi.\n",
        "* **Fungsi Rectified Linear Unit ($\\text{ReLU}(z) = \\max(0, z)$)**: Kontinu tetapi tidak dapat dibedakan pada $z=0$ (kemiringan berubah tiba-tiba), dan turunannya adalah 0 untuk $z<0$. Namun, dalam praktiknya, ia bekerja sangat baik dan memiliki keuntungan cepat dihitung, sehingga menjadi fungsi default. Yang terpenting, fakta bahwa ia tidak memiliki nilai output maksimum membantu mengurangi beberapa masalah selama Gradient Descent.\n",
        "\n",
        "Fungsi aktivasi diperlukan karena jika Anda merantai beberapa transformasi linier, semua yang Anda dapatkan adalah transformasi linier. Tanpa nonlinearitas antar lapisan, tumpukan lapisan yang dalam pun setara dengan satu lapisan, dan tidak dapat memecahkan masalah yang sangat kompleks. Sebaliknya, DNN yang cukup besar dengan aktivasi nonlinear secara teoritis dapat mendekati fungsi kontinu apa pun.\n",
        "\n",
        "**MLP untuk Regresi**\n",
        "MLP dapat digunakan untuk tugas regresi.\n",
        "* **Output neuron**: Untuk memprediksi nilai tunggal, Anda hanya perlu satu neuron output. Untuk regresi multivariat (memprediksi banyak nilai sekaligus), Anda memerlukan satu neuron output per dimensi output.\n",
        "* **Fungsi aktivasi output**: Umumnya, tidak ada fungsi aktivasi yang digunakan untuk neuron output agar dapat mengeluarkan rentang nilai apa pun. Jika output harus selalu positif, gunakan fungsi aktivasi ReLU atau softplus ($\\text{softplus}(z) = \\log(1 + \\exp(z))$). Jika prediksi harus jatuh dalam rentang nilai tertentu, gunakan fungsi logistik atau *hyperbolic tangent*, lalu skalakan label ke rentang yang sesuai.\n",
        "* **Fungsi *loss***: Biasanya *mean squared error* (MSE). Jika banyak *outlier* dalam *training set*, *mean absolute error* (MAE) lebih disukai. Alternatifnya, Huber loss, yang merupakan kombinasi keduanya (kuadratik saat kesalahan kecil, linier saat kesalahan besar), kurang sensitif terhadap *outlier* dan dapat konvergen lebih cepat serta lebih presisi daripada MAE.\n",
        "\n",
        "\n",
        "**MLP untuk Klasifikasi**\n",
        "MLP juga dapat digunakan untuk tugas klasifikasi.\n",
        "* **Klasifikasi biner**: Hanya perlu satu neuron output menggunakan fungsi aktivasi logistik. Outputnya adalah angka antara 0 dan 1, yang dapat diartikan sebagai probabilitas kelas positif.\n",
        "* **Klasifikasi biner multilabel**: Untuk tugas dengan banyak label biner independen (misalnya, email spam/ham dan urgent/nonurgent), Anda memerlukan satu neuron output untuk setiap kelas positif, keduanya menggunakan fungsi aktivasi logistik. Probabilitas output tidak harus berjumlah 1.\n",
        "* **Klasifikasi multikelas**: Jika setiap instance hanya dapat termasuk dalam satu kelas dari tiga atau lebih kelas yang mungkin (misalnya, klasifikasi digit), Anda perlu memiliki satu neuron output per kelas, dan Anda harus menggunakan fungsi aktivasi softmax untuk seluruh *output layer*. Fungsi softmax akan memastikan bahwa semua probabilitas yang diperkirakan berada antara 0 dan 1 dan bahwa probabilitas tersebut berjumlah 1 (diperlukan jika kelas-kelasnya eksklusif).\n",
        "* **Fungsi *loss***: *Cross-entropy loss* (juga disebut *log loss*) umumnya merupakan pilihan yang baik, karena kita memprediksi distribusi probabilitas.\n",
        "\n",
        "\n",
        "**Implementasi MLP dengan Keras**\n",
        "Keras adalah API Deep Learning tingkat tinggi yang memungkinkan Anda dengan mudah membangun, melatih, mengevaluasi, dan menjalankan semua jenis jaringan saraf. Keras bergantung pada *computation backend* (misalnya, TensorFlow, Microsoft Cognitive Toolkit (CNTK), dan Theano). Buku ini menggunakan `tf.keras`, implementasi Keras yang dibundel dengan TensorFlow, karena menawarkan fitur tambahan yang berguna.\n",
        "\n",
        "**Membangun Pengklasifikasi Gambar Menggunakan Sequential API**\n",
        "Pertama, muat dataset. Fashion MNIST adalah pengganti MNIST dengan format yang sama (70,000 gambar skala abu-abu $28 \\times 28$ piksel, 10 kelas), tetapi gambar mewakili item fashion.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocessing data\n",
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "# Creating the model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]), # Converts each input image into a 1D array\n",
        "    keras.layers.Dense(300, activation=\"relu\"), # First hidden layer with 300 neurons and ReLU\n",
        "    keras.layers.Dense(100, activation=\"relu\"), # Second hidden layer with 100 neurons and ReLU\n",
        "    keras.layers.Dense(10, activation=\"softmax\") # Output layer with 10 neurons (one per class) and softmax\n",
        "])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "```\n",
        "* `keras.models.Sequential()`: Model Keras paling sederhana untuk jaringan saraf yang terdiri dari tumpukan lapisan tunggal yang terhubung secara berurutan.\n",
        "* `keras.layers.Flatten(input_shape=[28, 28])`: Mengubah setiap gambar input menjadi array 1D (preprocessing sederhana tanpa parameter). `input_shape` harus ditentukan karena ini lapisan pertama.\n",
        "* `keras.layers.Dense(300, activation=\"relu\")`: Lapisan tersembunyi *Dense* pertama dengan 300 neuron dan fungsi aktivasi ReLU. Setiap lapisan Dense mengelola matriks bobot dan vektor bias.\n",
        "* `keras.layers.Dense(100, activation=\"relu\")`: Lapisan tersembunyi *Dense* kedua dengan 100 neuron dan ReLU.\n",
        "* `keras.layers.Dense(10, activation=\"softmax\")`: Lapisan output *Dense* dengan 10 neuron (satu per kelas) dan fungsi aktivasi softmax karena kelasnya eksklusif.\n",
        "\n",
        "**Mengompilasi Model**\n",
        "Setelah model dibuat, panggil metode `compile()` untuk menentukan fungsi *loss* dan *optimizer*.\n",
        "\n",
        "```python\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "```\n",
        "* `loss=\"sparse_categorical_crossentropy\"`: Digunakan karena memiliki label sparse (indeks kelas target untuk setiap instance) dan kelas-kelasnya eksklusif. Jika memiliki probabilitas target per kelas (vektor *one-hot*), gunakan `\"categorical_crossentropy\"`. Untuk klasifikasi biner, gunakan `\"binary_crossentropy\"` dengan fungsi aktivasi `\"sigmoid\"` di lapisan output.\n",
        "* `optimizer=\"sgd\"`: Melatih model menggunakan *Stochastic Gradient Descent* (Sistem Keras akan melakukan algoritma *backpropagation*). Penting untuk menyetel *learning rate* ketika menggunakan SGD.\n",
        "* `metrics=[\"accuracy\"]`: Berguna untuk mengukur akurasi selama pelatihan dan evaluasi.\n",
        "\n",
        "**Melatih dan Mengevaluasi Model**\n",
        "Panggil metode `fit()` untuk melatih model.\n",
        "\n",
        "```python\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "```\n",
        "* `epochs`: Jumlah *epoch* untuk melatih model.\n",
        "* `validation_data`: Set validasi opsional; Keras akan mengukur *loss* dan metrik tambahan pada set ini di akhir setiap *epoch*.\n",
        "\n",
        "Metode `fit()` mengembalikan objek `History` yang berisi parameter pelatihan, daftar *epoch*, dan kamus `history.history` yang berisi *loss* dan metrik tambahan yang diukur pada *training set* dan *validation set* di akhir setiap *epoch*.\n",
        "\n",
        "Visualisasi kurva pembelajaran:\n",
        "```python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()\n",
        "```\n",
        "Evaluasi model pada *test set* untuk memperkirakan *generalization error*.\n",
        "\n",
        "```python\n",
        "model.evaluate(X_test, y_test)\n",
        "```\n",
        "\n",
        "**Menggunakan Model untuk Membuat Prediksi**\n",
        "Gunakan metode `predict()` untuk membuat prediksi pada instance baru.\n",
        "\n",
        "```python\n",
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "print(y_proba.round(2))\n",
        "\n",
        "y_pred = model.predict_classes(X_new) # Use predict_classes for the class with the highest probability\n",
        "print(y_pred)\n",
        "print(np.array(class_names)[y_pred])\n",
        "```\n",
        "`predict()` menghasilkan probabilitas per kelas untuk setiap instance. `predict_classes()` mengembalikan kelas dengan probabilitas tertinggi.\n",
        "\n",
        "**Membangun MLP Regresi Menggunakan Sequential API**\n",
        "Mirip dengan klasifikasi, tetapi lapisan output memiliki satu neuron dan tidak menggunakan fungsi aktivasi, serta fungsi *loss* adalah *mean squared error*.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1) # Single output neuron for regression, no activation\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)\n",
        "```\n",
        "\n",
        "**Membangun Model Kompleks Menggunakan Functional API**\n",
        "Untuk topologi yang lebih kompleks, seperti Wide & Deep neural network (menghubungkan sebagian input langsung ke lapisan output untuk mempelajari pola dalam dan aturan sederhana), Keras menawarkan Functional API.\n",
        "\n",
        "```python\n",
        "input_ = keras.layers.Input(shape=X_train.shape[1:]) # Create an Input object\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_) # Pass input to the first hidden layer\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1) # Pass output of hidden1 to hidden2\n",
        "concat = keras.layers.Concatenate()([input_, hidden2]) # Concatenate input and hidden2 output\n",
        "output = keras.layers.Dense(1)(concat) # Output layer\n",
        "model = keras.Model(inputs=[input_], outputs=[output]) # Create Keras Model\n",
        "```\n",
        "* `keras.layers.Input()`: Menentukan jenis input yang akan diterima model, termasuk bentuk dan tipe data.\n",
        "* Setiap lapisan dipanggil seperti fungsi, meneruskan output lapisan sebelumnya sebagai input.\n",
        "* `keras.layers.Concatenate()`: Menggabungkan input dari beberapa lapisan.\n",
        "* `keras.Model(inputs=[input_], outputs=[output])`: Membuat model Keras dengan menentukan input dan output yang akan digunakan.\n",
        "\n",
        "**Penanganan Multiple Input**\n",
        "Untuk mengirim subset fitur melalui jalur yang berbeda (misalnya, beberapa fitur ke jalur *wide* dan beberapa ke jalur *deep*), gunakan banyak input.\n",
        "\n",
        "```python\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2]) # Concatenate input_A and hidden2 output\n",
        "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
        "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
        "\n",
        "# Compile and fit the model with multiple inputs\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
        "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
        "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3] # Corrected for batch dimension\n",
        "\n",
        "history = model.fit(\n",
        "    (X_train_A, X_train_B), y_train, epochs=20,\n",
        "    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
        "\n",
        "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
        "y_pred = model.predict((X_new_A, X_new_B))\n",
        "```\n",
        "Saat memanggil `fit()`, `evaluate()`, atau `predict()`, berikan pasangan matriks input.\n",
        "\n",
        "**Penanganan Multiple Output**\n",
        "Anda mungkin ingin memiliki banyak output untuk tugas yang menuntutnya (misalnya, menemukan koordinat objek dan mengklasifikasikannya), tugas independen yang berbeda berdasarkan data yang sama, atau sebagai teknik regularisasi (menambahkan output bantu untuk memastikan bagian jaringan belajar sesuatu yang berguna sendiri).\n",
        "\n",
        "```python\n",
        "# ... (same as Functional API example up to hidden2)\n",
        "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
        "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2) # Auxiliary output from hidden2\n",
        "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
        "\n",
        "# Compile with multiple losses and loss weights\n",
        "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\n",
        "\n",
        "# Fit with multiple labels\n",
        "history = model.fit(\n",
        "    [X_train_A, X_train_B], [y_train, y_train], epochs=20, # Provide labels for both outputs\n",
        "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
        "\n",
        "# Evaluate and predict with multiple outputs\n",
        "total_loss, main_loss, aux_loss = model.evaluate(\n",
        "    [X_test_A, X_test_B], [y_test, y_test])\n",
        "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
        "```\n",
        "Setiap output memerlukan fungsi *loss*-nya sendiri, jadi berikan daftar *loss* saat mengompilasi model. Anda dapat menetapkan bobot *loss* untuk setiap output. Saat melatih, berikan label untuk setiap output. `evaluate()` akan mengembalikan total *loss* serta *loss* individual, dan `predict()` akan mengembalikan prediksi untuk setiap output.\n",
        "\n",
        "**Menggunakan Subclassing API untuk Membangun Model Dinamis**\n",
        "Sequential API dan Functional API bersifat deklaratif (mendeklarasikan lapisan dan koneksinya sebelum memproses data). Namun, untuk model yang melibatkan loop, bentuk bervariasi, percabangan kondisional, dan perilaku dinamis lainnya, Subclassing API lebih cocok.\n",
        "\n",
        "Cukup *subclass* kelas `Model`, buat lapisan yang dibutuhkan di konstruktor, dan gunakan di metode `call()`.\n",
        "\n",
        "```python\n",
        "class WideAndDeepModel(keras.Model):\n",
        "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
        "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
        "        self.main_output = keras.layers.Dense(1)\n",
        "        self.aux_output = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_A, input_B = inputs\n",
        "        hidden1 = self.hidden1(input_B)\n",
        "        hidden2 = self.hidden2(hidden1)\n",
        "        concat = keras.layers.concatenate([input_A, hidden2])\n",
        "        main_output = self.main_output(concat)\n",
        "        aux_output = self.aux_output(hidden2)\n",
        "        return main_output, aux_output\n",
        "\n",
        "model = WideAndDeepModel()\n",
        "```\n",
        "Fleksibilitas ini datang dengan biaya: arsitektur model tersembunyi di dalam metode `call()`, sehingga Keras tidak dapat dengan mudah memeriksanya, menyimpannya, atau mengloningnya. `summary()` hanya memberikan daftar lapisan tanpa informasi koneksi.\n",
        "\n",
        "**Menyimpan dan Memulihkan Model**\n",
        "Saat menggunakan Sequential API atau Functional API, menyimpan model Keras yang terlatih sangat sederhana:\n",
        "\n",
        "```python\n",
        "model.save(\"my_keras_model.h5\")\n",
        "```\n",
        "Keras akan menggunakan format HDF5 untuk menyimpan arsitektur model (termasuk *hyperparameters* setiap lapisan) dan nilai semua parameter model (bobot koneksi dan bias), serta *optimizer*.\n",
        "\n",
        "Memuat model juga mudah:\n",
        "\n",
        "```python\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")\n",
        "```\n",
        "Ini tidak berfungsi dengan *model subclassing*. Anda dapat menggunakan `save_weights()` dan `load_weights()` untuk menyimpan dan memulihkan parameter model, tetapi Anda perlu menyimpan dan memulihkan hal lainnya sendiri.\n",
        "\n",
        "**Menggunakan Callbacks**\n",
        "Metode `fit()` menerima argumen `callbacks` yang memungkinkan Anda menentukan daftar objek yang akan dipanggil Keras pada waktu-waktu tertentu selama pelatihan.\n",
        "\n",
        "* `ModelCheckpoint`: Menyimpan *checkpoint* model pada interval reguler selama pelatihan, secara default di akhir setiap *epoch*. Dengan `save_best_only=True`, ia hanya akan menyimpan model saat performanya pada *validation set* adalah yang terbaik sejauh ini.\n",
        "\n",
        "```python\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb])\n",
        "model = keras.models.load_model(\"my_keras_model.h5\") # Roll back to best model\n",
        "```\n",
        "* `EarlyStopping`: Mengganggu pelatihan ketika tidak ada kemajuan pada *validation set* untuk sejumlah *epoch* (ditentukan oleh argumen `patience`), dan secara opsional akan mengembalikan model terbaik.\n",
        "\n",
        "```python\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "Anda juga dapat menulis *custom callbacks* Anda sendiri dengan mengimplementasikan metode seperti `on_epoch_end()`.\n",
        "\n",
        "**Menggunakan TensorBoard untuk Visualisasi**\n",
        "TensorBoard adalah alat visualisasi interaktif yang hebat untuk melihat kurva pembelajaran, membandingkan kurva pembelajaran dari beberapa *run*, memvisualisasikan grafik komputasi, menganalisis statistik pelatihan, dan lainnya.\n",
        "\n",
        "Untuk menggunakannya, program Anda harus mengeluarkan data yang ingin divisualisasikan ke file *log* biner khusus yang disebut *event files*. Server TensorBoard akan memantau direktori *log* dan secara otomatis mengambil perubahan untuk memperbarui visualisasi.\n",
        "\n",
        "```python\n",
        "import os\n",
        "import time\n",
        "\n",
        "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
        "\n",
        "def get_run_logdir():\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(root_logdir, run_id)\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[tensorboard_cb])\n",
        "```\n",
        "Setelah menjalankan kode, Anda dapat memulai server TensorBoard dari terminal:\n",
        "`$ tensorboard --logdir=./my_logs --port=6006`\n",
        "Atau di Jupyter:\n",
        "`%load_ext tensorboard`\n",
        "`%tensorboard --logdir=./my_logs --port=6006`\n",
        "Kemudian buka `http://localhost:6006` di *web browser*.\n",
        "\n",
        "**Menyetel Hyperparameters Jaringan Saraf**\n",
        "Banyak *hyperparameters* yang bisa disetel dalam MLP, seperti jumlah lapisan, neuron per lapisan, fungsi aktivasi, inisialisasi bobot, dll..\n",
        "\n",
        "Salah satu opsi adalah mencoba banyak kombinasi *hyperparameters* dan melihat mana yang bekerja paling baik pada *validation set* (atau menggunakan K-fold cross-validation). Ini dapat dilakukan dengan `GridSearchCV` atau `RandomizedSearchCV` dari Scikit-Learn. Untuk itu, model Keras perlu dibungkus dalam objek yang meniru *regressors* Scikit-Learn.\n",
        "\n",
        "```python\n",
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.InputLayer(input_shape=input_shape)\n",
        "    ])\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "print(rnd_search_cv.best_params_)\n",
        "print(rnd_search_cv.best_score_)\n",
        "model = rnd_search_cv.best_estimator_.model\n",
        "```\n",
        "* `build_model()`: Fungsi untuk membangun dan mengompilasi model Keras dengan *hyperparameters* tertentu.\n",
        "* `keras.wrappers.scikit_learn.KerasRegressor()`: Membungkus model Keras agar dapat digunakan dengan API Scikit-Learn.\n",
        "* `RandomizedSearchCV`: Melakukan pencarian acak di ruang *hyperparameter*.\n",
        "\n",
        "**Jumlah Hidden Layers**\n",
        "Untuk banyak masalah, satu lapisan tersembunyi dapat memberikan hasil yang memuaskan. MLP dengan hanya satu lapisan tersembunyi secara teoritis dapat memodelkan fungsi yang paling kompleks, asalkan memiliki cukup neuron. Namun, untuk masalah yang kompleks, jaringan yang dalam memiliki efisiensi parameter yang jauh lebih tinggi daripada jaringan dangkal: mereka dapat memodelkan fungsi kompleks menggunakan neuron yang secara eksponensial lebih sedikit daripada jaringan dangkal, memungkinkan mereka mencapai kinerja yang jauh lebih baik dengan jumlah data pelatihan yang sama. Data dunia nyata sering terstruktur secara hierarkis, dan jaringan saraf yang dalam secara otomatis memanfaatkan fakta ini.\n",
        "\n",
        "Arsitektur hierarkis ini tidak hanya membantu DNN konvergen lebih cepat ke solusi yang baik, tetapi juga meningkatkan kemampuan mereka untuk menggeneralisasi ke dataset baru. Ini juga memungkinkan *transfer learning*, di mana lapisan bawah dari jaringan yang sudah dilatih sebelumnya dapat digunakan kembali. Untuk masalah yang sangat kompleks, Anda dapat meningkatkan jumlah lapisan tersembunyi hingga Anda mulai mengalami *overfitting*.\n",
        "\n",
        "**Jumlah Neuron per Hidden Layer**\n",
        "Jumlah neuron di lapisan input dan output ditentukan oleh jenis input dan output yang dibutuhkan tugas Anda. Dulu, umum untuk mengatur ukuran lapisan tersembunyi menjadi bentuk piramida, dengan neuron yang semakin sedikit di setiap lapisan. Namun, praktik ini sebagian besar telah ditinggalkan karena tampaknya menggunakan jumlah neuron yang sama di semua lapisan tersembunyi bekerja sama baiknya dalam sebagian besar kasus, atau bahkan lebih baik, dan hanya ada satu *hyperparameter* yang harus disetel. Anda dapat mencoba meningkatkan jumlah neuron secara bertahap hingga jaringan mulai mengalami *overfitting*. Namun, seringkali lebih sederhana untuk memilih model dengan lebih banyak lapisan dan neuron daripada yang sebenarnya dibutuhkan, kemudian menggunakan *early stopping* dan teknik regularisasi lainnya untuk mencegah *overfitting*.\n",
        "\n",
        "**Learning Rate, Batch Size, dan Hyperparameters Lainnya**\n",
        "* **Learning rate**: Merupakan *hyperparameter* paling penting. *Optimal learning rate* adalah sekitar setengah dari *maximum learning rate* (titik di mana algoritma pelatihan mulai menyimpang).\n",
        "* **Optimizer**: Memilih *optimizer* yang lebih baik daripada *Mini-batch Gradient Descent* dan menyetel *hyperparameternya* juga cukup penting.\n",
        "* **Batch size**: Dapat memiliki dampak signifikan pada kinerja model dan waktu pelatihan. Manfaat utama penggunaan *batch size* besar adalah akselerator perangkat keras seperti GPU dapat memprosesnya secara efisien, sehingga algoritma pelatihan akan melihat lebih banyak instance per detik. Namun, *batch size* besar sering menyebabkan ketidakstabilan pelatihan dan model yang dihasilkan mungkin tidak menggeneralisasi sebaik model yang dilatih dengan *batch size* kecil.\n",
        "* **Activation function**: Fungsi aktivasi ReLU adalah default yang baik untuk semua lapisan tersembunyi. Untuk lapisan output, tergantung pada tugas Anda.\n",
        "* **Number of iterations**: Dalam kebanyakan kasus, tidak perlu menyetel jumlah iterasi pelatihan; cukup gunakan *early stopping* sebagai gantinya."
      ],
      "metadata": {
        "id": "g8m3RktcvJGx"
      }
    }
  ]
}