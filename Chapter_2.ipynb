{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zohaP76Glggs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Melihat Gambaran Besar (*Look at the Big Picture*)**\n",
        "    * **Tujuan Bisnis:** Model akan memprediksi harga rumah rata-rata di setiap distrik dan outputnya akan digunakan oleh sistem *Machine Learning* lain untuk menentukan kelayakan investasi di suatu area. Kesalahan estimasi harga manual saat ini bisa mencapai 20%.\n",
        "    * **Framing Masalah:** Ini adalah tugas *supervised learning* karena ada data berlabel (harga rumah rata-rata). Ini juga merupakan tugas *regression*, khususnya *multiple regression* (menggunakan banyak fitur untuk prediksi) dan *univariate regression* (memprediksi satu nilai per distrik). Karena data tidak terus-menerus mengalir, *batch learning* cocok untuk masalah ini.\n",
        "    * **Metrik Performa:** *Root Mean Square Error* (RMSE) adalah metrik umum untuk masalah regresi, memberikan bobot lebih tinggi pada kesalahan besar.\n",
        "        $$RMSE(X,h)=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^{2}}$$\n",
        "        Di mana:\n",
        "        * $m$: jumlah instansi dalam dataset.\n",
        "        * $X^{(i)}$: vektor nilai fitur untuk instansi ke-$i$.\n",
        "        * $y^{(i)}$: label (nilai output yang diinginkan) untuk instansi ke-$i$.\n",
        "        * $h$: fungsi prediksi sistem (*hypothesis*).\n",
        "        * $h(x^{(i)})$: nilai prediksi untuk instansi ke-$i$.\n",
        "        RMSE adalah $l_2$ norm, sedangkan *Mean Absolute Error* (MAE) adalah $l_1$ norm. RMSE lebih sensitif terhadap *outlier* daripada MAE.\n",
        "    * **Asumsi:** Penting untuk memeriksa asumsi di awal proyek. Dalam kasus ini, dipastikan bahwa sistem hilir membutuhkan harga aktual, bukan kategori.\n",
        "\n",
        "2.  **Mendapatkan Data (*Get the Data*)**\n",
        "    * Data yang digunakan adalah *California Housing Prices dataset* dari repositori StatLib, berdasarkan data sensus California tahun 1990.\n",
        "    * **Membuat *Workspace*:** Mengatur lingkungan Python dengan menginstal modul seperti Jupyter, NumPy, pandas, Matplotlib, dan Scikit-Learn. Disarankan menggunakan *virtual environment* untuk menghindari konflik versi *library*.\n",
        "        ```python\n",
        "        # Contoh kode untuk membuat virtual environment (dari buku)\n",
        "        # \\$ export ML_PATH=\"\\$HOME/ml\"\n",
        "        # \\$ mkdir -p \\$ML_PATH\n",
        "        # \\$ python3 -m pip install --user -U virtualenv\n",
        "        # \\$ cd \\$ML_PAΤΗ\n",
        "        # \\$ python3 -m virtualenv my_env\n",
        "        # \\$ source my_env/bin/activate\n",
        "        # \\$ python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn\n",
        "        # \\$ python3 -m ipykernel install --user --name python3\n",
        "        # \\$ jupyter notebook\n",
        "        ```\n",
        "    * **Mengunduh Data:** Membuat fungsi untuk mengunduh dan mengekstrak file `housing.tgz` yang berisi `housing.csv`.\n",
        "        ```python\n",
        "        import os\n",
        "        import tarfile\n",
        "        import urllib\n",
        "\n",
        "        DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "        HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "        HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "        def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "            os.makedirs(housing_path, exist_ok=True)\n",
        "            tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "            urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "            housing_tgz = tarfile.open(tgz_path)\n",
        "            housing_tgz.extractall(path=housing_path)\n",
        "            housing_tgz.close()\n",
        "\n",
        "        # fetch_housing_data() # Panggil fungsi ini untuk mengunduh data\n",
        "        ```\n",
        "    * **Melihat Struktur Data:** Menggunakan pandas untuk memuat data.\n",
        "        ```python\n",
        "        import pandas as pd\n",
        "\n",
        "        def load_housing_data(housing_path=HOUSING_PATH):\n",
        "            csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "            return pd.read_csv(csv_path)\n",
        "\n",
        "        housing = load_housing_data()\n",
        "        housing.head() # Menampilkan 5 baris pertama\n",
        "        housing.info() # Memberikan ringkasan data, termasuk jumlah baris, tipe atribut, dan nilai non-null\n",
        "        housing[\"ocean_proximity\"].value_counts() # Menghitung kategori untuk atribut 'ocean_proximity'\n",
        "        housing.describe() # Menampilkan ringkasan statistik atribut numerik\n",
        "        ```\n",
        "        Dataset ini memiliki 20,640 instansi, dan atribut `total_bedrooms` memiliki 207 nilai yang hilang. Atribut `ocean_proximity` adalah kategorikal.\n",
        "    * **Membuat *Test Set*:** Penting untuk membuat *test set* dan menyimpannya di awal untuk menghindari *data snooping bias*. Sekitar 20% data biasanya dialokasikan untuk *test set*.\n",
        "        * **Sampling Acak:**\n",
        "            ```python\n",
        "            import numpy as np\n",
        "\n",
        "            def split_train_test(data, test_ratio):\n",
        "                shuffled_indices = np.random.permutation(len(data))\n",
        "                test_set_size = int(len(data) * test_ratio)\n",
        "                test_indices = shuffled_indices[:test_set_size]\n",
        "                train_indices = shuffled_indices[test_set_size:]\n",
        "                return data.iloc[train_indices], data.iloc[test_indices]\n",
        "\n",
        "            # train_set, test_set = split_train_test(housing, 0.2)\n",
        "            ```\n",
        "            Kelemahan pendekatan ini adalah *test set* akan berbeda setiap kali program dijalankan. Solusi yang lebih baik adalah mengatur *random seed* atau menggunakan ID instansi untuk memastikan *split* yang stabil.\n",
        "        * **Stratified Sampling:** Jika dataset tidak terlalu besar atau ada atribut penting, *stratified sampling* direkomendasikan untuk memastikan *test set* representatif terhadap populasi. Dalam kasus ini, median income digunakan sebagai dasar stratifikasi.\n",
        "            ```python\n",
        "            from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "            housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                                            bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                                            labels=[1, 2, 3, 4, 5])\n",
        "            # housing[\"income_cat\"].hist() # Untuk memvisualisasikan kategori pendapatan\n",
        "\n",
        "            split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "            for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "                strat_train_set = housing.loc[train_index]\n",
        "                strat_test_set = housing.loc[test_index]\n",
        "\n",
        "            # strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set) # Memeriksa proporsi\n",
        "            ```\n",
        "            Setelah pembuatan *test set*, atribut `income_cat` harus dihapus agar data kembali ke keadaan semula.\n",
        "\n",
        "3.  **Menjelajahi dan Memvisualisasikan Data (*Discover and Visualize the Data to Gain Insights*)**\n",
        "    * Fokus pada *training set*.\n",
        "    * **Visualisasi Geografis:** Menggunakan *scatterplot* untuk memvisualisasikan lokasi distrik berdasarkan *latitude* dan *longitude*. Mengatur `alpha` menjadi 0.1 membantu melihat area dengan kepadatan tinggi. Menambahkan informasi populasi (ukuran lingkaran) dan harga rumah (warna) ke *scatterplot* dapat memberikan wawasan lebih lanjut, menunjukkan bahwa harga terkait dengan lokasi dan kepadatan populasi.\n",
        "    * **Mencari Korelasi:** Menghitung koefisien korelasi standar (*Pearson's r*) antar atribut.\n",
        "        ```python\n",
        "        corr_matrix = housing.corr()\n",
        "        corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
        "        ```\n",
        "        Median income memiliki korelasi positif yang kuat dengan harga rumah median.\n",
        "    * **Mengeksplorasi Kombinasi Atribut:** Membuat atribut baru yang mungkin lebih informatif, seperti `rooms_per_household`, `bedrooms_per_room`, dan `population_per_household`.\n",
        "        ```python\n",
        "        housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
        "        housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
        "        housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"]\n",
        "\n",
        "        # Periksa lagi korelasi setelah menambahkan atribut baru\n",
        "        # corr_matrix = housing.corr()\n",
        "        # corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
        "        ```\n",
        "        Atribut `bedrooms_per_room` dan `rooms_per_household` terbukti lebih berkorelasi dengan harga rumah daripada atribut aslinya.\n",
        "\n",
        "4.  **Mempersiapkan Data untuk Algoritma *Machine Learning* (*Prepare the Data for Machine Learning Algorithms*)**\n",
        "    * Membuat salinan *training set* yang bersih dan memisahkan prediktor dari label.\n",
        "    * **Pembersihan Data (*Data Cleaning*)**: Menangani nilai yang hilang. Ada tiga opsi:\n",
        "        1.  Menghilangkan distrik yang relevan (`dropna()`)\n",
        "        2.  Menghilangkan seluruh atribut (`drop()`)\n",
        "        3.  Mengisi nilai yang hilang dengan nilai tertentu (nol, rata-rata, median, dll.) (`fillna()`).\n",
        "        Scikit-Learn menyediakan `SimpleImputer` untuk menangani nilai yang hilang, biasanya dengan median untuk atribut numerik.\n",
        "        ```python\n",
        "        from sklearn.impute import SimpleImputer\n",
        "\n",
        "        housing_num = housing.drop(\"ocean_proximity\", axis=1) # Hanya atribut numerik\n",
        "        imputer = SimpleImputer(strategy=\"median\")\n",
        "        imputer.fit(housing_num) # Menghitung median untuk setiap atribut\n",
        "        X = imputer.transform(housing_num) # Mengisi nilai yang hilang\n",
        "        housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\n",
        "        ```\n",
        "    * **Menangani Atribut Teks dan Kategorikal:** Mengonversi atribut teks kategorikal menjadi angka.\n",
        "        * **`OrdinalEncoder`:** Mengonversi kategori teks menjadi angka integer. Masalahnya adalah algoritma ML akan menganggap nilai yang berdekatan lebih mirip, yang tidak selalu benar.\n",
        "            ```python\n",
        "            from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "            housing_cat = housing[[\"ocean_proximity\"]]\n",
        "            ordinal_encoder = OrdinalEncoder()\n",
        "            housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "            # ordinal_encoder.categories_ # Melihat kategori yang dipelajari\n",
        "            ```\n",
        "        * **`OneHotEncoder`:** Solusi yang lebih baik adalah *one-hot encoding*, membuat atribut biner untuk setiap kategori. Ini menghasilkan *sparse matrix* yang efisien untuk menyimpan banyak nol.\n",
        "            ```python\n",
        "            from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "            cat_encoder = OneHotEncoder()\n",
        "            housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "            # housing_cat_1hot.toarray() # Mengonversi ke dense NumPy array\n",
        "            ```\n",
        "    * ***Custom Transformers*:** Untuk operasi pembersihan kustom atau kombinasi atribut, Anda dapat membuat *transformer* sendiri dengan mengimplementasikan metode `fit()`, `transform()`, dan `fit_transform()` (dengan mewarisi `TransformerMixin` dan `BaseEstimator`).\n",
        "        ```python\n",
        "        from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "        rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 # Indeks kolom\n",
        "\n",
        "        class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "            def __init__(self, add_bedrooms_per_room=True):\n",
        "                self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "            def fit(self, X, y=None):\n",
        "                return self\n",
        "            def transform(self, X):\n",
        "                rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "                population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "                if self.add_bedrooms_per_room:\n",
        "                    bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "                    return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
        "                else:\n",
        "                    return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "        # attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "        # housing_extra_attribs = attr_adder.transform(housing.values)\n",
        "        ```\n",
        "    * ***Feature Scaling*:** Sebagian besar algoritma *Machine Learning* tidak berkinerja baik jika atribut numerik memiliki skala yang sangat berbeda.\n",
        "        * **Min-Max Scaling (Normalisasi):** Nilai diskalakan antara 0 dan 1. Digunakan `MinMaxScaler`.\n",
        "        * **Standardisasi:** Mengurangkan nilai rata-rata dan membagi dengan standar deviasi, sehingga distribusi memiliki *unit variance*. Digunakan `StandardScaler`. Standardisasi kurang terpengaruh oleh *outlier*.\n",
        "        Penting untuk hanya meng-*fit* *scaler* pada *training data*.\n",
        "    * ***Transformation Pipelines*:** `Pipeline` Scikit-Learn membantu mengeksekusi urutan transformasi secara berurutan.\n",
        "        ```python\n",
        "        from sklearn.pipeline import Pipeline\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "        num_pipeline = Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "            ('attribs_adder', CombinedAttributesAdder()),\n",
        "            ('std_scaler', StandardScaler()),\n",
        "        ])\n",
        "\n",
        "        # housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
        "        ```\n",
        "        `ColumnTransformer` (di Scikit-Learn 0.20+) memungkinkan penerapan transformasi yang berbeda ke kolom yang berbeda.\n",
        "        ```python\n",
        "        from sklearn.compose import ColumnTransformer\n",
        "\n",
        "        num_attribs = list(housing_num.columns) # Pastikan ini daftar nama kolom\n",
        "        cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "        full_pipeline = ColumnTransformer([\n",
        "            (\"num\", num_pipeline, num_attribs),\n",
        "            (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "        ])\n",
        "\n",
        "        housing_prepared = full_pipeline.fit_transform(housing)\n",
        "        ```\n",
        "\n",
        "5.  **Memilih dan Melatih Model (*Select and Train a Model*)**\n",
        "    * **Melatih dan Mengevaluasi pada *Training Set*:**\n",
        "        * **`LinearRegression`:**\n",
        "            ```python\n",
        "            from sklearn.linear_model import LinearRegression\n",
        "            from sklearn.metrics import mean_squared_error\n",
        "\n",
        "            lin_reg = LinearRegression()\n",
        "            lin_reg.fit(housing_prepared, housing_labels)\n",
        "\n",
        "            # Evaluasi\n",
        "            # housing_predictions = lin_reg.predict(housing_prepared)\n",
        "            # lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "            # lin_rmse = np.sqrt(lin_mse)\n",
        "            # print(f\"Linear Regression RMSE: {lin_rmse}\")\n",
        "            ```\n",
        "            RMSE yang tinggi menunjukkan *underfitting*.\n",
        "        * **`DecisionTreeRegressor`:** Model yang lebih kompleks.\n",
        "            ```python\n",
        "            from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "            tree_reg = DecisionTreeRegressor()\n",
        "            tree_reg.fit(housing_prepared, housing_labels)\n",
        "\n",
        "            # Evaluasi\n",
        "            # housing_predictions = tree_reg.predict(housing_prepared)\n",
        "            # tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "            # tree_rmse = np.sqrt(tree_mse)\n",
        "            # print(f\"Decision Tree RMSE: {tree_rmse}\") # Seringkali 0.0, menunjukkan overfitting parah\n",
        "            ```\n",
        "            RMSE 0.0 pada *training set* mengindikasikan *overfitting* yang parah.\n",
        "    * **Evaluasi yang Lebih Baik Menggunakan *Cross-Validation*:**\n",
        "        `K-fold cross-validation` membagi *training set* menjadi beberapa *fold* dan melatih serta mengevaluasi model berkali-kali, menggunakan *fold* yang berbeda untuk evaluasi setiap kali.\n",
        "        ```python\n",
        "        from sklearn.model_selection import cross_val_score\n",
        "\n",
        "        def display_scores(scores):\n",
        "            print(\"Scores:\", scores)\n",
        "            print(\"Mean:\", scores.mean())\n",
        "            print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "        # Untuk DecisionTreeRegressor\n",
        "        scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
        "                                 scoring=\"neg_mean_squared_error\", cv=10)\n",
        "        tree_rmse_scores = np.sqrt(-scores)\n",
        "        display_scores(tree_rmse_scores)\n",
        "\n",
        "        # Untuk LinearRegression\n",
        "        lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
        "                                     scoring=\"neg_mean_squared_error\", cv=10)\n",
        "        lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "        display_scores(lin_rmse_scores)\n",
        "        ```\n",
        "        Hasil *cross-validation* menunjukkan bahwa `DecisionTreeRegressor` *overfit* dan bahkan berkinerja lebih buruk daripada `LinearRegression`.\n",
        "        * **`RandomForestRegressor`:** Model *Ensemble Learning* yang seringkali memberikan hasil yang lebih baik.\n",
        "            ```python\n",
        "            from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "            forest_reg = RandomForestRegressor(random_state=42) # Tambahkan random_state agar reproducible\n",
        "            forest_reg.fit(housing_prepared, housing_labels)\n",
        "\n",
        "            # Evaluasi dengan cross-validation\n",
        "            forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
        "                                            scoring=\"neg_mean_squared_error\", cv=10)\n",
        "            forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "            display_scores(forest_rmse_scores)\n",
        "            ```\n",
        "            `RandomForestRegressor` menunjukkan hasil yang jauh lebih baik, tetapi masih ada indikasi *overfitting*. Disarankan untuk mencoba berbagai model lain dan menyimpan model yang menjanjikan.\n",
        "\n",
        "6.  **Menyesuaikan Model (*Fine-Tune Your Model*)**\n",
        "    * **`Grid Search`:** `GridSearchCV` Scikit-Learn secara otomatis mencari kombinasi *hyperparameter* terbaik menggunakan *cross-validation*.\n",
        "        ```python\n",
        "        from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "        param_grid = [\n",
        "            {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "            {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "        ]\n",
        "\n",
        "        forest_reg = RandomForestRegressor(random_state=42)\n",
        "        grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                                   scoring='neg_mean_squared_error',\n",
        "                                   return_train_score=True)\n",
        "\n",
        "        grid_search.fit(housing_prepared, housing_labels)\n",
        "\n",
        "        # grid_search.best_params_ # Mendapatkan hyperparameter terbaik\n",
        "        # grid_search.best_estimator_ # Mendapatkan estimator terbaik\n",
        "        # cvres = grid_search.cv_results_\n",
        "        # for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "        #    print(np.sqrt(-mean_score), params)\n",
        "        ```\n",
        "        `GridSearchCV` akan melatih model berkali-kali untuk setiap kombinasi *hyperparameter*.\n",
        "    * **`Randomized Search`:** Ketika ruang pencarian *hyperparameter* besar, `RandomizedSearchCV` lebih disukai. Ia mengevaluasi sejumlah kombinasi acak.\n",
        "\n",
        "7.  **Menganalisis Model Terbaik dan Kesalahannya (*Analyze the Best Models and Their Errors*)**\n",
        "    * Mengevaluasi `feature_importances_` dari `RandomForestRegressor` untuk memahami pentingnya setiap atribut dalam membuat prediksi yang akurat.\n",
        "        ```python\n",
        "        # feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "        # extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
        "        # cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
        "        # cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
        "        # attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
        "        # sorted(zip(feature_importances, attributes), reverse=True)\n",
        "        ```\n",
        "        Informasi ini dapat digunakan untuk menghilangkan fitur yang kurang berguna.\n",
        "\n",
        "8.  **Mengevaluasi Sistem pada *Test Set* (*Evaluate Your System on the Test Set*)**\n",
        "    * Setelah *fine-tuning* selesai, evaluasi model akhir pada *test set* menggunakan metode `transform()` pada *pipeline* data.\n",
        "        ```python\n",
        "        final_model = grid_search.best_estimator_\n",
        "\n",
        "        X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
        "        y_test = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "        X_test_prepared = full_pipeline.transform(X_test)\n",
        "        final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "        final_mse = mean_squared_error(y_test, final_predictions)\n",
        "        final_rmse = np.sqrt(final_mse) # Sekitar 47,730.2\n",
        "\n",
        "        # Menghitung interval kepercayaan 95%\n",
        "        # from scipy import stats\n",
        "        # confidence = 0.95\n",
        "        # squared_errors = (final_predictions - y_test) ** 2\n",
        "        # np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
        "        #                         loc=squared_errors.mean(),\n",
        "        #                         scale=stats.sem(squared_errors)))\n",
        "        ```\n",
        "    * Jika performa pada *test set* sedikit lebih buruk dari *cross-validation*, ini normal karena model mungkin terlalu disesuaikan dengan data validasi.\n",
        "\n",
        "9.  **Meluncurkan, Memantau, dan Memelihara Sistem (*Launch, Monitor, and Maintain Your System*)**\n",
        "    * **Penyebaran (*Deployment*):** Model yang telah dilatih dapat disimpan (misalnya, menggunakan `joblib`) dan dimuat di lingkungan produksi. Ini bisa berupa aplikasi web lokal atau layanan web khusus (misalnya, REST API). Penyebaran ke *cloud* seperti Google Cloud AI Platform juga merupakan pilihan populer.\n",
        "    * **Pemantauan (*Monitoring*):** Penting untuk memantau performa model secara langsung dan memicu peringatan jika performanya menurun. Model cenderung \"membusuk\" seiring waktu karena perubahan data. Pemantauan juga harus mencakup kualitas data input.\n",
        "    * **Pemeliharaan (*Maintenance*):** Proses pengumpulan data baru, pelabelan, pelatihan ulang model, dan penyebaran model baru harus otomatis jika data terus berkembang. Penting juga untuk memiliki *backup* model dan dataset untuk *rollback* jika terjadi kegagalan.\n",
        "\n",
        "Singkatnya, Bab 2 menekankan pentingnya proses *end-to-end* yang sistematis dalam proyek *Machine Learning*, mulai dari pemahaman masalah bisnis, persiapan data yang cermat, pemilihan dan pelatihan model, hingga *fine-tuning* dan aspek pemeliharaan sistem di lingkungan produksi. Ini juga memperkenalkan berbagai alat dan teknik penting dari Scikit-Learn."
      ],
      "metadata": {
        "id": "jMgQa4GAnkBY"
      }
    }
  ]
}