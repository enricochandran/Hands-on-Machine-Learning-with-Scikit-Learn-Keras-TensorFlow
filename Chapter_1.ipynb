{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Definisi Pembelajaran Mesin\n",
        "\n",
        "Pembelajaran Mesin adalah ilmu (dan seni) memprogram komputer sehingga mereka dapat belajar dari data. ML juga dapat didefinisikan sebagai bidang studi yang memberikan komputer kemampuan untuk belajar tanpa diprogram secara eksplisit. Definisi lain yang lebih berorientasi pada teknik adalah: Program komputer dikatakan belajar dari pengalaman E berkenaan dengan beberapa tugas T dan beberapa ukuran kinerja P, jika kinerjanya pada T, sebagaimana diukur oleh P, meningkat dengan pengalaman E.\n",
        "\n",
        "Contoh klasik adalah filter spam, sebuah program ML yang belajar untuk menandai email sebagai spam, diberi contoh email spam (ditandai oleh pengguna) dan email biasa (non-spam, juga disebut \"ham\"). Contoh-contoh yang digunakan sistem untuk belajar disebut *training set*, dan setiap contoh pelatihan disebut *training instance* atau *sample*. Dalam kasus ini, tugas (T) adalah menandai spam untuk email baru, pengalaman (E) adalah data pelatihan, dan ukuran kinerja (P) dapat didefinisikan, misalnya, sebagai rasio email yang diklasifikasikan dengan benar, yang disebut akurasi (*accuracy*) dan sering digunakan dalam tugas klasifikasi. Penting untuk dicatat bahwa mengunduh salinan Wikipedia bukanlah Pembelajaran Mesin karena komputer tidak menjadi lebih baik dalam tugas apa pun.\n",
        "\n",
        "### Mengapa Menggunakan Pembelajaran Mesin?\n",
        "\n",
        "Pembelajaran Mesin menawarkan keunggulan signifikan dibandingkan pemrograman tradisional, terutama dalam beberapa skenario:\n",
        "* **Memecahkan masalah kompleks**: ML dapat menemukan solusi untuk masalah yang terlalu rumit bagi pendekatan tradisional atau tidak memiliki algoritma yang diketahui, seperti pengenalan suara yang terlalu kompleks untuk aturan yang di-*hardcode*.\n",
        "* **Menangani lingkungan yang berfluktuasi**: Sistem ML dapat beradaptasi secara otomatis terhadap data baru dan perubahan. Sebagai contoh, filter spam berbasis ML dapat secara otomatis belajar untuk memblokir pola spam baru tanpa intervensi manual.\n",
        "* **Menyederhanakan kode dan pemeliharaan**: Untuk masalah yang memerlukan banyak penyetelan halus atau daftar aturan yang panjang, algoritma ML dapat menyederhanakan kode dan seringkali berkinerja lebih baik.\n",
        "* **Memperoleh wawasan**: Algoritma ML dapat diinspeksi untuk melihat apa yang telah mereka pelajari, mengungkapkan korelasi atau tren yang tidak terduga dalam jumlah data yang besar. Proses ini disebut *data mining*.\n",
        "\n",
        "### Jenis Sistem Pembelajaran Mesin\n",
        "\n",
        "Sistem Pembelajaran Mesin dapat dikategorikan berdasarkan beberapa kriteria:\n",
        "\n",
        "#### Berdasarkan Pengawasan:\n",
        "\n",
        "* **Supervised Learning (Pembelajaran Terawasi)**: Algoritma dilatih dengan *training set* yang mencakup solusi yang diinginkan, yang disebut *labels* (label). Tugas umum meliputi klasifikasi (misalnya, filter spam mengklasifikasikan email sebagai spam atau ham) dan regresi (misalnya, memprediksi harga mobil berdasarkan fitur-fitur). Algoritma penting dalam kategori ini termasuk k-Nearest Neighbors, Linear Regression, Logistic Regression, Support Vector Machines (SVMs), Decision Trees dan Random Forests, serta Neural networks.\n",
        "\n",
        "* **Unsupervised Learning (Pembelajaran Tanpa Pengawasan)**: Data pelatihan tidak berlabel, dan sistem mencoba belajar tanpa \"guru\". Tugas umum meliputi:\n",
        "    * **Clustering (Pengelompokan)**: Mendeteksi kelompok-kelompok *instance* yang serupa, seperti mengelompokkan pengunjung blog. Algoritma yang digunakan antara lain K-Means, DBSCAN, dan Hierarchical Cluster Analysis (HCA).\n",
        "    * **Anomaly Detection dan Novelty Detection**: Mengidentifikasi *instance* yang tidak biasa atau baru, contohnya mendeteksi penipuan kartu kredit atau menangkap cacat manufaktur. Algoritma yang relevan termasuk One-class SVM dan Isolation Forest.\n",
        "    * **Visualization dan Dimensionality Reduction (Visualisasi dan Pengurangan Dimensi)**: Menyederhanakan data tanpa kehilangan terlalu banyak informasi, seringkali dengan menggabungkan fitur-fitur yang berkorelasi menjadi satu (*feature extraction*). Ini dapat membuat algoritma ML berjalan lebih cepat dan menggunakan lebih sedikit memori. Algoritma dalam kategori ini meliputi Principal Component Analysis (PCA), Kernel PCA, Locally Linear Embedding (LLE), dan t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
        "    * **Association Rule Learning**: Menemukan hubungan yang menarik antara atribut dalam jumlah data yang besar, misalnya, menemukan bahwa \"orang yang membeli saus barbekyu dan keripik kentang juga cenderung membeli steak\". Algoritma yang digunakan adalah Apriori dan Eclat.\n",
        "\n",
        "* **Semisupervised Learning (Pembelajaran Semiterawasi)**: Menggunakan data yang sebagian berlabel (banyak *instance* tidak berlabel, sedikit berlabel). Contohnya adalah layanan *photo-hosting* yang mengelompokkan foto orang yang sama dan hanya meminta pengguna untuk melabeli satu atau dua foto untuk mengidentifikasi semua foto lainnya. Algoritma ini seringkali merupakan kombinasi algoritma tak terawasi dan terawasi.\n",
        "\n",
        "* **Reinforcement Learning (Pembelajaran Penguatan)**: Sistem, yang disebut *agent*, mengamati lingkungan, melakukan tindakan, dan menerima *reward* (atau *penalty*). Tujuannya adalah belajar strategi terbaik, yang disebut *policy*, untuk memaksimalkan *reward* dari waktu ke waktu. Contoh terkenal adalah program AlphaGo DeepMind yang mengalahkan juara dunia Go.\n",
        "\n",
        "#### Berdasarkan Pembelajaran Inkremental:\n",
        "\n",
        "* **Batch Learning (Pembelajaran Batch)**: Sistem dilatih menggunakan semua data yang tersedia secara *offline* dan tidak dapat belajar secara inkremental. Jika ada data baru, sistem harus dilatih ulang dari awal dengan seluruh dataset. Proses ini bisa memakan waktu dan sumber daya komputasi yang besar.\n",
        "\n",
        "* **Online Learning (Pembelajaran Online)**: Sistem dilatih secara inkremental dengan memberi makan *instance* data secara berurutan, baik secara individual atau dalam kelompok kecil yang disebut *mini-batches*. Setiap langkah pembelajaran cepat dan murah, memungkinkan sistem untuk belajar data baru dengan cepat saat data tiba. Ini cocok untuk sistem yang menerima aliran data berkelanjutan (misalnya, harga saham) dan perlu beradaptasi dengan cepat atau secara otomatis. Ini juga dapat digunakan untuk melatih sistem pada dataset besar yang tidak muat dalam memori utama (disebut *out-of-core learning*). Parameter penting adalah *learning rate*, yang menentukan seberapa cepat sistem harus beradaptasi dengan data yang berubah. Tantangan utamanya adalah risiko penurunan kinerja jika data yang buruk dimasukkan ke dalam sistem.\n",
        "\n",
        "#### Berdasarkan Cara Generalisasi:\n",
        "\n",
        "* **Instance-Based Learning (Pembelajaran Berbasis Instansi)**: Sistem belajar contoh-contoh dengan hati, lalu menggeneralisasi ke kasus-kasus baru dengan menggunakan ukuran kesamaan untuk membandingkannya dengan contoh-contoh yang telah dipelajari (atau sebagian dari mereka). Sebagai contoh, filter spam dapat menandai email yang sangat mirip dengan email spam yang diketahui. Algoritma k-Nearest Neighbors adalah contohnya.\n",
        "\n",
        "* **Model-Based Learning (Pembelajaran Berbasis Model)**: Sistem membangun model dari contoh-contoh dan kemudian menggunakan model itu untuk membuat prediksi. Ini melibatkan pemilihan model (misalnya, model linear), menentukan fungsi biaya yang mengukur seberapa buruk model tersebut, dan kemudian melatih model untuk menemukan nilai parameter yang meminimalkan fungsi biaya.\n",
        "\n",
        "### Tantangan Utama Pembelajaran Mesin\n",
        "\n",
        "Dua hal utama yang dapat menyebabkan masalah dalam proyek Pembelajaran Mesin adalah \"algoritma yang buruk\" dan \"data yang buruk\".\n",
        "\n",
        "#### Data yang Buruk:\n",
        "\n",
        "* **Kuantitas Data Pelatihan yang Tidak Cukup**: Sebagian besar algoritma ML membutuhkan data dalam jumlah besar untuk berfungsi dengan baik, seringkali ribuan contoh untuk masalah sederhana dan jutaan untuk masalah kompleks seperti pengenalan gambar atau suara.\n",
        "* **Data Pelatihan Non-Representatif**: Data pelatihan harus representatif dari kasus-kasus baru yang ingin digeneralisasi oleh model. Sampel yang terlalu kecil dapat menyebabkan *sampling noise* (data non-representatif akibat kebetulan), dan metode pengambilan sampel yang cacat dapat menyebabkan *sampling bias*. Contoh historis terkenal adalah jajak pendapat presiden AS tahun 1936 oleh Literary Digest.\n",
        "* **Data Berkualitas Buruk**: Data pelatihan yang penuh dengan kesalahan, *outlier*, dan *noise* akan membuat sistem lebih sulit mendeteksi pola yang mendasarinya dan mengurangi kinerja. Penting untuk meluangkan waktu membersihkan data pelatihan. Ini termasuk menangani *outlier* dan nilai yang hilang.\n",
        "* **Fitur Tidak Relevan (*Irrelevant Features*)**: Sistem hanya dapat belajar jika data pelatihan mengandung fitur yang cukup relevan dan tidak terlalu banyak fitur yang tidak relevan (*garbage in, garbage out*). *Feature engineering* (rekayasa fitur) adalah proses penting untuk membuat fitur yang baik, meliputi *feature selection* (memilih fitur yang paling berguna), *feature extraction* (menggabungkan fitur yang ada untuk menghasilkan yang lebih berguna), dan menciptakan fitur baru dengan mengumpulkan data baru.\n",
        "\n",
        "#### Algoritma yang Buruk:\n",
        "\n",
        "* **Overfitting Training Data (Overfitting Data Pelatihan)**: Ini terjadi ketika model berkinerja baik pada data pelatihan tetapi tidak menggeneralisasi dengan baik ke kasus-kasus baru. Ini sering terjadi ketika model terlalu kompleks relatif terhadap jumlah dan kebisingan data pelatihan. Solusi meliputi menyederhanakan model (misalnya, model linear daripada model polinomial tingkat tinggi), mengurangi jumlah atribut, mengumpulkan lebih banyak data pelatihan, atau mengurangi *noise*. *Regularization* (regularisasi) adalah proses membatasi model untuk membuatnya lebih sederhana dan mengurangi risiko *overfitting*. Jumlah regularisasi dikendalikan oleh *hyperparameter*.\n",
        "* **Underfitting Training Data (Underfitting Data Pelatihan)**: Ini adalah kebalikan dari *overfitting*, di mana model terlalu sederhana untuk mempelajari struktur data yang mendasari. Ini berarti prediksi model cenderung tidak akurat, bahkan pada contoh pelatihan. Solusi meliputi memilih model yang lebih kuat, memberikan fitur yang lebih baik, atau mengurangi batasan pada model (misalnya, mengurangi *hyperparameter* regularisasi).\n",
        "\n",
        "### Pengujian dan Validasi\n",
        "\n",
        "Untuk mengetahui seberapa baik model akan menggeneralisasi ke kasus-kasus baru, model harus diuji pada kasus-kasus baru. Umumnya, data dibagi menjadi *training set* dan *test set*. *Training set* digunakan untuk melatih model, dan *test set* digunakan untuk menguji kinerja model. Tingkat kesalahan pada kasus-kasus baru disebut *generalization error* atau *out-of-sample error*. Jika kesalahan pelatihan rendah tetapi kesalahan generalisasi tinggi, berarti model mengalami *overfitting*.\n",
        "\n",
        "Untuk pemilihan model dan penyetelan *hyperparameter*, *holdout validation* adalah solusi umum. Sebagian dari *training set* dipegang sebagai *validation set* atau *dev set*. Model dilatih pada *training set* yang dikurangi (yaitu, *training set* penuh dikurangi *validation set*), dan model terbaik dipilih berdasarkan kinerjanya pada *validation set*. Setelah itu, model terbaik dilatih pada *training set* lengkap (termasuk *validation set*) untuk menghasilkan model akhir, yang kemudian dievaluasi pada *test set* untuk estimasi kesalahan generalisasi.\n",
        "\n",
        "Tantangan lainnya adalah *data mismatch*, di mana data pelatihan mungkin tidak sepenuhnya representatif dari data yang akan digunakan dalam produksi. Solusi yang dapat digunakan adalah dengan memisahkan sebagian dari data pelatihan web ke dalam *train-dev set*. Jika model berkinerja baik pada *train-dev set* tetapi buruk pada *validation set*, masalahnya berasal dari *data mismatch*.\n",
        "\n",
        "### Teorema No Free Lunch (NFL)\n",
        "\n",
        "Teorema No Free Lunch menyatakan bahwa jika tidak ada asumsi yang dibuat tentang data, maka tidak ada alasan untuk memilih satu model di atas yang lain. Dalam praktiknya, asumsi yang masuk akal tentang data dibuat, dan hanya beberapa model yang masuk akal yang dievaluasi."
      ],
      "metadata": {
        "id": "B_GoLymSkJX5"
      }
    }
  ]
}