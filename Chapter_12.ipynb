{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow adalah *library* yang kuat untuk komputasi numerik, khususnya sangat cocok untuk *Machine Learning* skala besar. TensorFlow dikembangkan oleh tim Google Brain dan mendukung banyak layanan skala besar Google, seperti Google Cloud Speech, Google Photos, dan Google Search. TensorFlow menjadi *open source* pada November 2015 dan kini merupakan *library Deep Learning* paling populer.\n",
        "\n",
        "Berikut adalah fitur utama TensorFlow:\n",
        "* Intinya sangat mirip dengan NumPy, tetapi dengan dukungan GPU.\n",
        "* Mendukung komputasi terdistribusi di berbagai perangkat dan server.\n",
        "* Mencakup *compiler just-in-time* (JIT) yang mengoptimalkan komputasi untuk kecepatan dan penggunaan memori. Ini bekerja dengan mengekstrak *computation graph* dari fungsi Python, mengoptimalkannya, dan menjalankannya secara efisien (misalnya, menjalankan operasi independen secara paralel).\n",
        "* *Computation graph* dapat diekspor ke format portabel, memungkinkan model TensorFlow dilatih di satu lingkungan dan dijalankan di lingkungan lain.\n",
        "* Mengimplementasikan *autodiff* dan menyediakan *optimizer* yang sangat baik seperti RMSProp dan Nadam, yang memudahkan minimasi berbagai fungsi *loss*.\n",
        "* Menawarkan banyak fitur lain di atas fitur intinya, termasuk `tf.keras`, `tf.data` (untuk *loading* dan *preprocessing* data), `tf.image` (untuk *image processing*), `tf.signal` (untuk *signal processing*), dan lainnya.\n",
        "* Menyertakan API *Deep Learning* lain bernama Estimators API, tetapi tim TensorFlow merekomendasikan penggunaan `tf.keras` sebagai gantinya.\n",
        "\n",
        "**Perbedaan antara TensorFlow dan NumPy:**\n",
        "\n",
        "* **Dukungan GPU:** TensorFlow mendukung komputasi GPU untuk mempercepat operasi, sementara NumPy tidak.\n",
        "* **Komputasi Terdistribusi:** TensorFlow mendukung komputasi terdistribusi di berbagai perangkat dan server, fitur yang tidak ada di NumPy.\n",
        "* ***Computation Graphs*:** TensorFlow dapat membuat dan mengoptimalkan *computation graphs* untuk eksekusi yang efisien dan portabilitas. NumPy tidak memiliki konsep *computation graphs*.\n",
        "* ***Autodiff* dan *Optimizers*:** TensorFlow memiliki *autodiff* bawaan untuk menghitung *gradient* secara otomatis dan menyediakan *optimizer* yang canggih untuk meminimalkan fungsi *loss*. NumPy tidak memiliki fungsionalitas ini secara *built-in*.\n",
        "* **Tipe Data:** NumPy secara *default* menggunakan presisi 64-bit, sedangkan TensorFlow menggunakan presisi 32-bit karena umumnya cukup untuk *neural network*, lebih cepat, dan menggunakan lebih sedikit RAM.\n",
        "* **Konversi Tipe:** TensorFlow tidak melakukan konversi tipe secara otomatis dan akan menimbulkan *exception* jika mencoba melakukan operasi pada *tensor* dengan tipe yang tidak kompatibel. Ini untuk menghindari masalah performa dan *bug* yang tidak disengaja. NumPy lebih fleksibel dalam konversi tipe.\n",
        "* **Immutability vs. Mutability:** *Tensor* TensorFlow (tf.Tensor) bersifat *immutable* (tidak dapat diubah setelah dibuat). Untuk nilai yang dapat diubah (seperti *weight* model), TensorFlow menyediakan `tf.Variable`. *Array* NumPy bersifat *mutable*.\n",
        "* **Operasi Transposisi:** Dalam TensorFlow, `tf.transpose(t)` membuat *tensor* baru dengan salinan data yang ditransposisi, sementara di NumPy, `t.T` hanyalah *view* yang ditransposisi pada data yang sama.\n",
        "\n",
        "**Struktur Data TensorFlow Selain *Regular Tensors***:\n",
        "\n",
        "* **Sparse Tensors (`tf.SparseTensor`):** Digunakan untuk merepresentasikan *tensor* yang sebagian besar berisi nilai nol secara efisien. Paket `tf.sparse` berisi operasi untuk *sparse tensors*.\n",
        "* **Tensor Arrays (`tf.TensorArray`):** Merupakan daftar *tensor*. Mereka memiliki ukuran tetap secara *default* tetapi dapat dibuat dinamis. Semua *tensor* yang dikandungnya harus memiliki *shape* dan tipe data yang sama.\n",
        "* **Ragged Tensors (`tf.RaggedTensor`):** Merepresentasikan daftar *list* *tensor* statis, di mana setiap *tensor* memiliki *shape* dan tipe data yang sama. Paket `tf.ragged` berisi operasi untuk *ragged tensors*.\n",
        "* **String Tensors (`tf.string`):** Merupakan *tensor* reguler bertipe `tf.string` yang merepresentasikan *byte string*, bukan *Unicode string*. Jika *Unicode string* digunakan, mereka akan di-*encode* secara otomatis ke UTF-8. Paket `tf.strings` berisi operasi untuk *byte string* dan *Unicode string*. Penting untuk dicatat bahwa `tf.string` bersifat atomik, artinya panjangnya tidak muncul di *shape tensor*.\n",
        "* **Sets (direpresentasikan sebagai *regular tensors* atau *sparse tensors*):** Setiap *set* direpresentasikan oleh sebuah vektor di sumbu terakhir *tensor*. Operasi untuk memanipulasi *set* tersedia di paket `tf.sets`.\n",
        "* **Queues:** Digunakan untuk menyimpan *tensor* di beberapa *step*. TensorFlow menawarkan berbagai jenis *queue*, seperti FIFO queues (`FIFOQueue`), *priority queues* (`PriorityQueue`), *shuffling queues* (`RandomShuffleQueue`), dan *batching queues` (`PaddingFIFOQueue`). Kelas-kelas ini berada di paket `tf.queue`.\n",
        "\n",
        "**Perbandingan `tf.range(10)` dan `tf.constant(np.arange(10))`:**\n",
        "\n",
        "Tidak, hasilnya tidak sama persis, meskipun keduanya menghasilkan *tensor* dengan nilai yang sama.\n",
        "\n",
        "* `tf.range(10)`: Akan menghasilkan `tf.Tensor` dengan nilai `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]` dan `dtype` *default* `tf.int32` atau `tf.int64` tergantung pada arsitektur sistem.\n",
        "* `tf.constant(np.arange(10))`: `np.arange(10)` akan menghasilkan *NumPy array* dengan nilai `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]` dan `dtype` *default* `int64` (untuk sebagian besar sistem). Ketika ini dikonversi ke `tf.constant()`, TensorFlow akan membuat `tf.Tensor` dengan nilai yang sama, tetapi *dtype*-nya akan menjadi `tf.int64` karena *NumPy array* sumbernya adalah `int64`.\n",
        "\n",
        "Perbedaan utama ada pada `dtype` bawaan. NumPy menggunakan presisi 64-bit secara *default*, sedangkan TensorFlow cenderung menggunakan 32-bit untuk *neural network* karena alasan performa.\n",
        "\n",
        "**Kapan Menggunakan Fungsi atau Subclass untuk *Custom Loss Function***:\n",
        "\n",
        "* **Menggunakan Fungsi:**\n",
        "    * Ketika *loss function* tidak memiliki *hyperparameter* yang perlu disimpan. Fungsi sederhana yang menerima `y_true` dan `y_pred` sebagai argumen sudah cukup.\n",
        "    * Untuk kesederhanaan dan kemudahan implementasi. Ini adalah cara tercepat untuk membuat *custom loss* jika tidak ada persyaratan tambahan.\n",
        "    * Ketika *loss* dihitung hanya berdasarkan *label* dan *prediksi*.\n",
        "    * **Contoh:**\n",
        "        ```python\n",
        "        def huber_fn(y_true, y_pred):\n",
        "            error = y_true - y_pred\n",
        "            is_small_error = tf.abs(error) < 1\n",
        "            squared_loss = tf.square(error) / 2\n",
        "            linear_loss = tf.abs(error) - 0.5\n",
        "            return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "        ```\n",
        "        Untuk menyimpan model dengan fungsi ini, Anda perlu menyediakan kamus yang memetakan nama fungsi ke fungsi sebenarnya saat memuat model:\n",
        "        ```python\n",
        "        model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
        "                                        custom_objects={\"huber_fn\": huber_fn})\n",
        "        ```\n",
        "        Jika *loss function* perlu dikonfigurasi (misalnya, dengan *threshold*), Anda bisa membuat fungsi pembungkus:\n",
        "        ```python\n",
        "        def create_huber(threshold=1.0):\n",
        "            def huber_fn(y_true, y_pred):\n",
        "                error = y_true - y_pred\n",
        "                is_small_error = tf.abs(error) < threshold\n",
        "                squared_loss = tf.square(error) / 2\n",
        "                linear_loss = threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "                return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "            return huber_fn\n",
        "        ```\n",
        "        Namun, *threshold* tidak akan disimpan dengan model, sehingga perlu ditentukan ulang saat memuat:\n",
        "        ```python\n",
        "        model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
        "                                        custom_objects={\"huber_fn\": create_huber(2.0)})\n",
        "        ```\n",
        "* **Mensubklaskan `keras.losses.Loss`:**\n",
        "    * Ketika *loss function* memiliki *hyperparameter* yang perlu disimpan bersama model. Dengan mengimplementasikan metode `get_config()`, *hyperparameter* ini akan otomatis disimpan dan dimuat.\n",
        "    * Ketika Anda ingin *loss* menjadi portabel ke implementasi Keras lainnya. Meskipun API Keras saat ini hanya secara eksplisit menentukan *subclassing* untuk *layer*, *model*, *callback*, dan *regularizer*, ada kemungkinan akan diperbarui untuk semua komponen.\n",
        "    * Ketika Anda membutuhkan kontrol lebih atas inisialisasi atau perilaku internal.\n",
        "    * **Contoh:**\n",
        "        ```python\n",
        "        class HuberLoss(keras.losses.Loss):\n",
        "            def __init__(self, threshold=1.0, **kwargs):\n",
        "                self.threshold = threshold\n",
        "                super().__init__(**kwargs)\n",
        "\n",
        "            def call(self, y_true, y_pred):\n",
        "                error = y_true - y_pred\n",
        "                is_small_error = tf.abs(error) < self.threshold\n",
        "                squared_loss = tf.square(error) / 2\n",
        "                linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
        "                return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "            def get_config(self):\n",
        "                base_config = super().get_config()\n",
        "                return {**base_config, \"threshold\": self.threshold}\n",
        "        ```\n",
        "        Model dapat dikompilasi dengan *loss* ini:\n",
        "        ```python\n",
        "        model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
        "        ```\n",
        "        Saat memuat model, Anda hanya perlu memetakan nama kelas ke kelas itu sendiri:\n",
        "        ```python\n",
        "        model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
        "                                        custom_objects={\"HuberLoss\": HuberLoss})\n",
        "        ```\n",
        "\n",
        "**Kapan Menggunakan Fungsi atau Subclass untuk *Custom Metric***:\n",
        "\n",
        "* **Menggunakan Fungsi:**\n",
        "    * Ketika metrik dapat dihitung sebagai rata-rata dari hasil setiap *batch*. Keras secara otomatis akan menghitung rata-rata metrik yang dikembalikan oleh fungsi untuk setiap *batch* selama *epoch*.\n",
        "    * Jika metrik tidak perlu mempertahankan *state* di seluruh *batch* (bukan *streaming metric*). Contohnya adalah *Mean Squared Error* (MSE) atau *Mean Absolute Error* (MAE) sebagai metrik, yang dapat dihitung rata-ratanya secara langsung.\n",
        "    * **Contoh (menggunakan `create_huber` sebagai metrik):**\n",
        "        ```python\n",
        "        model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
        "        ```\n",
        "        Di sini, `create_huber(2.0)` akan dipanggil untuk setiap *batch*, dan Keras akan melacak rata-rata *Huber loss* selama *epoch*.\n",
        "\n",
        "* **Mensubklaskan `keras.metrics.Metric`:**\n",
        "    * Ketika metrik adalah \"streaming metric\" (atau *stateful metric*) yang perlu memperbarui *state*-nya di seluruh *batch* dan menghitung hasil akhir berdasarkan *state* akumulatif. Contoh klasik adalah *precision*, di mana rata-rata *precision* per *batch* tidak memberikan hasil yang benar untuk *precision* keseluruhan. Anda perlu melacak *true positives* dan *false positives* secara kumulatif.\n",
        "    * Ketika metrik memiliki *hyperparameter* yang perlu disimpan bersama model. Mirip dengan *custom loss*, metode `get_config()` memungkinkan penyimpanan *hyperparameter*.\n",
        "    * Ketika Anda ingin mengontrol bagaimana *state* metrik diperbarui (`update_state`) dan bagaimana hasil akhir dihitung (`result`).\n",
        "    * **Contoh:**\n",
        "        ```python\n",
        "        class HuberMetric(keras.metrics.Metric):\n",
        "            def __init__(self, threshold=1.0, **kwargs):\n",
        "                super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
        "                self.threshold = threshold\n",
        "                self.huber_fn = create_huber(threshold)\n",
        "                self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
        "                self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
        "\n",
        "            def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "                metric = self.huber_fn(y_true, y_pred)\n",
        "                self.total.assign_add(tf.reduce_sum(metric))\n",
        "                self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "\n",
        "            def result(self):\n",
        "                return self.total / self.count\n",
        "\n",
        "            def get_config(self):\n",
        "                base_config = super().get_config()\n",
        "                return {**base_config, \"threshold\": self.threshold}\n",
        "        ```\n",
        "        Dalam kelas ini, `update_state` melacak total *Huber loss* dan jumlah *instance*, dan `result` menghitung rata-ratanya.\n",
        "\n",
        "**Kapan Membuat *Custom Layer* versus *Custom Model***:\n",
        "\n",
        "* **Membuat *Custom Layer* (`keras.layers.Layer`):**\n",
        "    * Ketika Anda ingin membuat blok bangunan yang dapat digunakan kembali dalam model. *Layer* adalah unit komputasi dasar dalam jaringan saraf. Contohnya termasuk *layer* Dense, Convolutional, Batch Normalization, atau bahkan blok *layer* yang sering diulang.\n",
        "    * Ketika *layer* tersebut tidak memiliki *fit()*, *evaluate()*, atau *predict()* metodenya sendiri. *Layer* beroperasi pada *tensor* input dan menghasilkan *tensor* output.\n",
        "    * Ketika Anda ingin mengelola *weight* dan *state* tertentu dalam operasi komputasi. Metode `build()` digunakan untuk membuat *weight* *layer*.\n",
        "    * **Contoh:** Implementasi sederhana dari *layer* Dense, atau *layer* yang menambahkan *Gaussian noise*.\n",
        "        ```python\n",
        "        class MyDense(keras.layers.Layer):\n",
        "            def __init__(self, units, activation=None, **kwargs):\n",
        "                super().__init__(**kwargs)\n",
        "                self.units = units\n",
        "                self.activation = keras.activations.get(activation)\n",
        "\n",
        "            def build(self, batch_input_shape):\n",
        "                self.kernel = self.add_weight(\n",
        "                    name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
        "                    initializer=\"glorot_normal\")\n",
        "                self.bias = self.add_weight(\n",
        "                    name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
        "                super().build(batch_input_shape)\n",
        "\n",
        "            def call(self, X):\n",
        "                return self.activation(X @ self.kernel + self.bias)\n",
        "\n",
        "            def compute_output_shape(self, batch_input_shape):\n",
        "                return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
        "\n",
        "            def get_config(self):\n",
        "                base_config = super().get_config()\n",
        "                return {**base_config, \"units\": self.units,\n",
        "                        \"activation\": keras.activations.serialize(self.activation)}\n",
        "        ```\n",
        "\n",
        "* **Membuat *Custom Model* (`keras.Model`):**\n",
        "    * Ketika Anda ingin membangun arsitektur jaringan saraf secara keseluruhan, yang memiliki metode *training*, *evaluation*, dan *prediction* sendiri. `keras.Model` adalah kelas yang lebih tinggi yang mengumpulkan *layer* menjadi model yang dapat dilatih.\n",
        "    * Ketika model memiliki arsitektur yang kompleks, seperti *multiple inputs*, *multiple outputs*, atau *skip connections*, atau *loops* yang sulit direpresentasikan dengan API Sequential atau Functional saja.\n",
        "    * Ketika Anda perlu menambahkan *loss* atau *metric* berdasarkan internal model, bukan hanya *output* dan *label*. Model memiliki metode `add_loss()` dan `add_metric()`.\n",
        "    * **Contoh:** Model *Regressor* dengan blok residual, atau model dengan *auxiliary output* dan *reconstruction loss*.\n",
        "        ```python\n",
        "        class ResidualBlock(keras.layers.Layer):\n",
        "            def __init__(self, n_layers, n_neurons, **kwargs):\n",
        "                super().__init__(**kwargs)\n",
        "                self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
        "                                                  kernel_initializer=\"he_normal\")\n",
        "                               for _ in range(n_layers)]\n",
        "            def call(self, inputs):\n",
        "                Z = inputs\n",
        "                for layer in self.hidden:\n",
        "                    Z = layer(Z)\n",
        "                return inputs + Z\n",
        "\n",
        "        class ResidualRegressor(keras.Model):\n",
        "            def __init__(self, output_dim, **kwargs):\n",
        "                super().__init__(**kwargs)\n",
        "                self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
        "                                                  kernel_initializer=\"he_normal\")\n",
        "                self.block1 = ResidualBlock(2, 30)\n",
        "                self.block2 = ResidualBlock(2, 30)\n",
        "                self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "            def call(self, inputs):\n",
        "                Z = self.hidden1(inputs)\n",
        "                for _ in range(1 + 3): # 1 initial + 3 additional applications\n",
        "                    Z = self.block1(Z)\n",
        "                Z = self.block2(Z)\n",
        "                return self.out(Z)\n",
        "        ```\n",
        "        Secara umum, bedakan antara komponen internal yang dapat digunakan kembali (*layer*) dari model yang akan Anda latih (*model*).\n",
        "\n",
        "**Kasus Penggunaan yang Membutuhkan Penulisan *Custom Training Loop***:\n",
        "\n",
        "Meskipun metode `fit()` Keras sangat fleksibel dan mencakup sebagian besar kasus penggunaan, ada beberapa skenario langka di mana Anda mungkin perlu menulis *custom training loop* Anda sendiri:\n",
        "\n",
        "* **Penggunaan *Optimizer* Berbeda untuk Bagian Jaringan yang Berbeda:** Beberapa arsitektur model canggih, seperti model *Wide & Deep*, menggunakan *optimizer* yang berbeda untuk bagian *wide* dan *deep* dari jaringan. Karena metode `fit()` hanya menggunakan satu *optimizer* yang ditentukan saat kompilasi model, hal ini memerlukan *custom training loop*.\n",
        "* **Transformasi atau Batasan *Gradient* yang Spesial:** Jika Anda perlu menerapkan transformasi atau batasan yang kompleks pada *gradient* (selain *clipping* sederhana) sebelum menerapkannya ke *weight* model, *custom training loop* memberikan kontrol yang diperlukan.\n",
        "* **Kontrol Penuh atas Proses *Training*:** Terkadang, *developer* mungkin ingin memiliki kendali eksplisit atas setiap langkah dalam *training loop* untuk memahami secara pasti apa yang terjadi atau untuk tujuan *debugging* yang mendalam, meskipun ini membuat kode lebih panjang dan rentan kesalahan.\n",
        "* **Penggunaan *Loss Function* atau Metrik yang Sangat Kompleks dan Interaktif:** Meskipun `add_loss()` dan `add_metric()` sudah sangat fleksibel, ada kemungkinan kasus di mana Anda perlu kontrol lebih lanjut atas bagaimana *loss* atau metrik dihitung dan diintegrasikan ke dalam proses *training*, terutama jika mereka bergantung pada interaksi yang tidak standar antar komponen model.\n",
        "* **Penyesuaian Aliran Data dan *Sampling* yang Sangat Spesifik:** Meskipun `tf.data` sangat kuat, jika Anda memiliki kebutuhan *sampling* atau *preprocessing* data yang sangat spesifik dan non-standar yang tidak dapat dengan mudah diakomodasi oleh API *data* Keras, *custom training loop* memungkinkan integrasi logika tersebut secara langsung.\n",
        "\n",
        "**Komponen Keras Kustom dan Kode Python Arbitrer vs. TF Functions**:\n",
        "\n",
        "Secara umum, **komponen Keras kustom (seperti fungsi *loss*, metrik, *layer*, atau *model*) harus dapat dikonversi ke TF Functions**. Ini berarti bahwa kode di dalamnya harus ditulis menggunakan operasi TensorFlow (`tf.`) sebanyak mungkin, dan menghindari penggunaan *library* Python eksternal (seperti NumPy) jika operasinya dimaksudkan untuk menjadi bagian dari *computation graph*.\n",
        "\n",
        "**Mengapa demikian?**\n",
        "* **Performa:** TF Functions mengonversi kode Python menjadi *computation graph* yang dapat dioptimalkan dan dijalankan dengan sangat efisien oleh TensorFlow, terutama di GPU atau TPU. Jika kode Python arbitrer (non-TensorFlow) disertakan, TensorFlow tidak dapat mengoptimalkan bagian tersebut, yang dapat menghambat performa.\n",
        "* **Portabilitas:** *Computation graph* TensorFlow dapat diekspor dan dijalankan di berbagai platform (misalnya, Android, iOS, *browser*, server) tanpa Python. Jika ada kode Python arbitrer, portabilitas akan berkurang karena platform target harus memiliki Python dan *library* yang relevan terinstal.\n",
        "* **Automatic Graph Generation (AutoGraph):** Keras secara otomatis mengonversi fungsi Python Anda menjadi TF Functions ketika Anda menggunakannya dalam model. Proses ini, yang disebut AutoGraph, menganalisis kode sumber Python dan mengganti *control flow statement* (seperti `for` *loop* dan `if` *statement*) dengan operasi TensorFlow yang setara. Jika ada kode yang tidak dapat di-*parse* atau dikonversi oleh AutoGraph, hal itu dapat menyebabkan kesalahan atau fungsionalitas yang terbatas.\n",
        "* **Side Effects:** *Side effects* dari kode Python non-TensorFlow (seperti *logging* atau memperbarui *counter* Python) hanya akan terjadi saat fungsi di-*trace* (yaitu, saat *graph* dibangun), bukan setiap kali TF Function dipanggil.\n",
        "\n",
        "**Namun, ada beberapa pengecualian atau skenario di mana kode Python arbitrer dapat muncul:**\n",
        "* **Selama Tahap Tracing:** Kode Python yang tidak berbasis TensorFlow akan dieksekusi selama tahap *tracing* (yaitu, saat *computation graph* dibuat), tetapi tidak akan menjadi bagian dari *graph* yang dieksekusi selanjutnya. Ini berguna jika Anda menggunakan Python untuk tujuan membangun *graph* itu sendiri (misalnya, menentukan arsitektur model berdasarkan parameter Python).\n",
        "* **Menggunakan `tf.py_function()`:** Anda dapat secara eksplisit membungkus kode Python arbitrer dalam operasi `tf.py_function()`. Namun, ini akan mengorbankan performa dan portabilitas karena TensorFlow tidak dapat mengoptimalkan bagian kode ini dan memerlukan Python di lingkungan eksekusi.\n",
        "\n",
        "**Aturan Utama untuk Fungsi yang Dapat Dikonversi ke TF Function**:\n",
        "\n",
        "Agar fungsi Python Anda dapat dikonversi dengan lancar menjadi TF Function oleh AutoGraph, ikuti aturan berikut:\n",
        "\n",
        "1.  **Gunakan Operasi TensorFlow:** Gunakan operasi TensorFlow (`tf.`) sebanyak mungkin untuk komputasi Anda (misalnya, `tf.reduce_sum()` alih-alih `np.sum()`, `tf.sort()` alih-alih `sorted()`). Kode yang memanggil *library* eksternal atau standar Python hanya akan berjalan selama *tracing* dan tidak akan menjadi bagian dari *graph*.\n",
        "2.  **Perhatikan Efek Samping (Side Effects):** Efek samping dari kode Python non-TensorFlow (misalnya, *logging*, memperbarui *counter* Python) hanya akan terjadi saat fungsi di-*trace* (yaitu, saat *graph* dibangun), bukan setiap kali TF Function dipanggil.\n",
        "3.  **Memanggil Fungsi Lain:** Anda dapat memanggil fungsi Python atau TF Function lain, tetapi mereka juga harus mengikuti aturan yang sama, karena TensorFlow akan menangkap operasi mereka dalam *computation graph*. Fungsi lain ini tidak perlu dihiasi dengan `@tf.function`.\n",
        "4.  **Pembuatan Variabel:** Jika fungsi membuat `tf.Variable` (atau objek TensorFlow *stateful* lainnya seperti *dataset* atau *queue*), itu harus dilakukan pada panggilan pertama dan hanya sekali itu saja. Jika tidak, Anda akan mendapatkan *exception*. Sebaiknya buat *variabel* di luar TF Function (misalnya, di metode `build()` dari *custom layer*).\n",
        "5.  **Penugasan Variabel:** Saat menetapkan nilai baru ke variabel TensorFlow, selalu gunakan metode `assign()` variabel tersebut (misalnya, `variable.assign(new_value)`) daripada operator penugasan Python (`=`).\n",
        "6.  **Ketersediaan Kode Sumber:** Kode sumber fungsi Python Anda harus tersedia untuk TensorFlow. Jika tidak tersedia (misalnya, jika didefinisikan di *Python shell* atau hanya menyebarkan file `.pyc`), proses pembuatan *graph* mungkin gagal atau fungsionalitasnya terbatas.\n",
        "7.  **Loop Iterasi Tensor/Dataset:** TensorFlow hanya akan menangkap *for loop* yang berulang melalui *tensor* atau *dataset* (`tf.range()`). Gunakan `for i in tf.range(x)` daripada `for i in range(x)`, atau *loop* tidak akan ditangkap dalam *graph* dan hanya akan berjalan selama *tracing*.\n",
        "8.  **Vektorisasi:** Untuk alasan performa, selalu pilih implementasi *vectorized* daripada menggunakan *loop* jika memungkinkan.\n",
        "\n",
        "**Kapan Anda Perlu Membuat *Dynamic Keras Model*? Bagaimana Caranya? Mengapa Tidak Semua Model Dibuat Dinamis?**:\n",
        "\n",
        "**Kapan Anda Perlu Membuat *Dynamic Keras Model*?**\n",
        "Secara *default*, Keras secara otomatis mengonversi fungsi Python Anda (termasuk *custom layer*, *loss*, dan *model*) menjadi TF Functions, yang menghasilkan *computation graph* statis. Ini bagus untuk performa dan portabilitas. Namun, ada kasus di mana *graph* statis mungkin tidak cukup:\n",
        "\n",
        "* **Logika Kondisional atau *Loop* yang Bergantung pada Data:** Jika logika model Anda (misalnya, arsitektur yang berubah, jumlah *layer* yang bergantung pada input) sangat bergantung pada nilai data aktual (bukan hanya *shape* atau *dtype*), *graph* statis mungkin tidak dapat menanganinya karena *graph* dibuat sebelum data aktual dieksekusi.\n",
        "* ***Debugging* Eagerly:** Saat mengembangkan dan melakukan *debugging*, mode *eager execution* (di mana operasi dieksekusi segera tanpa membangun *graph*) seringkali lebih mudah karena Anda bisa menggunakan *debugger* Python standar. Membuat model dinamis secara efektif membuat Keras beroperasi dalam mode *eager*.\n",
        "* **Operasi Python Arbitrer yang Tidak Dapat Dikonversi:** Meskipun tidak direkomendasikan untuk performa atau portabilitas, jika Anda memiliki kebutuhan yang sangat spesifik untuk menjalankan kode Python arbitrer (non-TensorFlow) yang tidak dapat dibungkus dalam `tf.py_function()` di dalam *forward pass* model Anda, model dinamis mungkin satu-satunya cara.\n",
        "\n",
        "**Bagaimana Anda Membuat *Dynamic Keras Model*?**\n",
        "Ada dua cara utama untuk mencegah Keras mengonversi fungsi Python Anda ke TF Functions dan memaksa mode *eager execution*:\n",
        "\n",
        "1.  **Set `dynamic=True` Saat Membuat *Custom Layer* atau *Custom Model*:**\n",
        "    Jika Anda membuat *custom layer* atau *custom model* dengan mensubklaskan `keras.layers.Layer` atau `keras.Model`, Anda bisa meneruskan `dynamic=True` di konstruktor:\n",
        "    ```python\n",
        "    class MyDynamicLayer(keras.layers.Layer):\n",
        "        def __init__(self, **kwargs):\n",
        "            super().__init__(dynamic=True, **kwargs)\n",
        "            # ...\n",
        "\n",
        "        def call(self, inputs):\n",
        "            # Logika yang mungkin bergantung pada nilai data aktual\n",
        "            # atau mencakup operasi Python non-TensorFlow\n",
        "            pass\n",
        "\n",
        "    class MyDynamicModel(keras.Model):\n",
        "        def __init__(self, **kwargs):\n",
        "            super().__init__(dynamic=True, **kwargs)\n",
        "            # ...\n",
        "\n",
        "        def call(self, inputs):\n",
        "            # ...\n",
        "            pass\n",
        "    ```\n",
        "2.  **Set `run_eagerly=True` Saat Memanggil Metode `compile()` Model:**\n",
        "    Ini adalah cara yang lebih umum untuk membuat seluruh model (termasuk *layer* bawaan Keras dan *custom component*) berjalan dalam mode *eager*:\n",
        "    ```python\n",
        "    model = keras.Sequential([...])\n",
        "    model.compile(loss=\"mse\", optimizer=\"adam\", run_eagerly=True)\n",
        "    ```\n",
        "    Ketika `run_eagerly=True` diatur, Keras tidak akan membangun *graph* komputasi untuk *forward pass* dan *backward pass*, melainkan akan mengeksekusi operasi secara langsung.\n",
        "\n",
        "**Mengapa Tidak Membuat Semua Model Dinamis?**\n",
        "Meskipun model dinamis memberikan fleksibilitas tambahan, ada beberapa alasan kuat mengapa Anda tidak boleh membuat semua model Anda dinamis kecuali memang diperlukan:\n",
        "\n",
        "* **Penurunan Performa yang Signifikan:** Ini adalah alasan utama. TF Functions (dan *graph* yang mendasarinya) dioptimalkan secara signifikan untuk kecepatan dan penggunaan memori. Mereka dapat menjalankan operasi secara paralel, melakukan *pruning* node yang tidak terpakai, dan menyederhanakan ekspresi. Mode *eager execution* jauh lebih lambat karena setiap operasi dieksekusi secara individual, dengan *overhead* Python yang lebih tinggi.\n",
        "* **Kurangnya Portabilitas:** Model dinamis tidak dapat dengan mudah diekspor ke format portabel (seperti SavedModel yang dapat dijalankan oleh TensorFlow Lite atau TensorFlow.js) karena mereka bergantung pada eksekusi kode Python.\n",
        "* **Kesulitan Debugging Gradien:** Meskipun *debugging* kode Python lebih mudah dalam mode *eager*, *debugging* masalah yang terkait dengan *gradient* (misalnya, *gradient* menjadi `None` atau `NaN`) bisa menjadi lebih sulit tanpa manfaat dari *graph* yang dioptimalkan dan fitur *autodiff* bawaan TensorFlow.\n",
        "* **Peningkatan Penggunaan Memori:** Membuat *graph* baru untuk setiap set *input shape* dan *dtype* yang berbeda saat menggunakan nilai Python numerik sebagai argumen dapat menyebabkan banyak *graph* dihasilkan, yang menghabiskan banyak RAM.\n",
        "\n",
        "Singkatnya, gunakan model dinamis hanya ketika *fit()* tidak cukup fleksibel untuk kebutuhan spesifik Anda, seperti *debugging* atau logika *control flow* yang sangat bergantung pada nilai data. Untuk sebagian besar kasus, model non-dinamis (berbasis *graph*) adalah pilihan yang lebih baik karena performa, portabilitas, dan kemudahan penggunaan yang disediakan oleh Keras dan TensorFlow.\n",
        "\n",
        "**Implementasi *Custom Layer* Normalisasi *Layer***\n",
        "\n",
        "Berikut adalah implementasi *custom layer* Layer Normalization, yang membandingkannya dengan `keras.layers.LayerNormalization` untuk memverifikasi hasilnya:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "class MyLayerNormalization(keras.layers.Layer):\n",
        "    def __init__(self, epsilon=0.001, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.epsilon = epsilon # smoothing term to avoid division by zero\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        # a (gamma) should be initialized with 1s\n",
        "        self.alpha = self.add_weight(\n",
        "            name=\"alpha\",\n",
        "            shape=batch_input_shape[-1:], # Shape should match the last dimension of inputs\n",
        "            initializer=\"ones\"\n",
        "        )\n",
        "        # b (beta) should be initialized with 0s\n",
        "        self.beta = self.add_weight(\n",
        "            name=\"beta\",\n",
        "            shape=batch_input_shape[-1:], # Shape should match the last dimension of inputs\n",
        "            initializer=\"zeros\"\n",
        "        )\n",
        "        super().build(batch_input_shape) # Must be at the end\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Compute mean and standard deviation of each instance's features\n",
        "        # axes=-1 means normalize over the last axis (features axis)\n",
        "        # keepdims=True ensures the output shape allows for broadcasting\n",
        "        mean, variance = tf.nn.moments(inputs, axes=-1, keepdims=True)\n",
        "        std_dev = tf.sqrt(variance + self.epsilon)\n",
        "\n",
        "        # Compute and return alpha * (X - mu) / (sigma + epsilon) + beta\n",
        "        return self.alpha * (inputs - mean) / std_dev + self.beta\n",
        "\n",
        "    def compute_output_shape(self, batch_input_shape):\n",
        "        # Layer Normalization preserves the input shape\n",
        "        return batch_input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, \"epsilon\": self.epsilon}\n",
        "\n",
        "# Verifikasi dengan membandingkan dengan keras.layers.LayerNormalization\n",
        "# Buat data dummy\n",
        "X = tf.constant(np.random.rand(2, 5).astype(np.float32)) # Batch size 2, 5 features\n",
        "\n",
        "# Buat instance custom layer\n",
        "my_norm = MyLayerNormalization()\n",
        "my_output = my_norm(X)\n",
        "\n",
        "# Buat instance Keras LayerNormalization\n",
        "keras_norm = keras.layers.LayerNormalization()\n",
        "keras_output = keras_norm(X)\n",
        "\n",
        "print(\"Input X:\\n\", X.numpy())\n",
        "print(\"\\nOutput dari MyLayerNormalization:\\n\", my_output.numpy())\n",
        "print(\"\\nOutput dari Keras LayerNormalization:\\n\", keras_output.numpy())\n",
        "\n",
        "# Periksa apakah hasilnya sangat mirip\n",
        "np.testing.assert_allclose(my_output.numpy(), keras_output.numpy(), rtol=1e-5, atol=1e-5)\n",
        "print(\"\\nVerifikasi berhasil: Output dari custom layer sangat mirip dengan Keras LayerNormalization.\")\n",
        "\n",
        "# Output contoh:\n",
        "# Input X:\n",
        "#  [[0.34217143 0.2831874  0.03859665 0.3297598  0.06553835]\n",
        "#  [0.8521035  0.7224213  0.22384666 0.1508216  0.6427328 ]]\n",
        "#\n",
        "# Output dari MyLayerNormalization:\n",
        "#  [[-0.04655648 -0.3204936  -1.5034606   0.26462746 -1.2131175 ]\n",
        "#  [ 1.1077757   0.5510659  -1.050672    1.3323143  -0.9404838 ]]\n",
        "#\n",
        "# Output dari Keras LayerNormalization:\n",
        "#  [[-0.0465564  -0.3204935  -1.5034604   0.26462746 -1.2131175 ]\n",
        "#  [ 1.1077757   0.551066   -1.050672    1.3323143  -0.9404838 ]]\n",
        "#\n",
        "# Verifikasi berhasil: Output dari custom layer sangat mirip dengan Keras LayerNormalization.\n",
        "```\n",
        "\n",
        "**Penjelasan Kode:**\n",
        "* **`__init__(self, epsilon=0.001, **kwargs)`:**\n",
        "    * Konstruktor menginisialisasi `epsilon`, sebuah konstanta kecil untuk menghindari pembagian dengan nol saat menghitung deviasi standar.\n",
        "    * `super().__init__(**kwargs)` memanggil konstruktor kelas induk `keras.layers.Layer` untuk menangani argumen standar seperti `name` atau `dtype`.\n",
        "* **`build(self, batch_input_shape)`:**\n",
        "    * Metode ini dipanggil saat *layer* pertama kali digunakan dan bentuk input diketahui.\n",
        "    * `self.alpha` (sering disebut *gamma*) dan `self.beta` adalah *weight* yang dapat dilatih oleh *layer*. `add_weight()` digunakan untuk mendaftarkan *variabel* ini dengan Keras.\n",
        "    * `shape=batch_input_shape[-1:]` mengatur *shape* *weight* agar sesuai dengan dimensi fitur terakhir dari input (misalnya, jika input adalah `(batch_size, num_features)`, *shape* akan menjadi `(num_features,)`).\n",
        "    * `alpha` diinisialisasi dengan satu dan `beta` dengan nol.\n",
        "    * `super().build(batch_input_shape)` harus dipanggil di akhir metode ini untuk memberi tahu Keras bahwa *layer* sudah dibuat (`self.built=True`).\n",
        "* **`call(self, inputs)`:**\n",
        "    * Metode ini mendefinisikan *forward pass* dari *layer*.\n",
        "    * `tf.nn.moments(inputs, axes=-1, keepdims=True)` menghitung *mean* ($\\mu$) dan *variance* ($\\sigma^2$) dari *feature* setiap *instance* di sepanjang sumbu terakhir (`axes=-1`). `keepdims=True` memastikan *shape output* mempertahankan dimensi yang dihilangkan sebagai `1`, yang penting untuk *broadcasting*.\n",
        "    * Deviasi standar ($\\sigma$) dihitung dari *variance*, dengan menambahkan `epsilon` untuk stabilitas numerik.\n",
        "    * Rumus normalisasi *layer* diterapkan: $\\alpha \\otimes (X - \\mu) / (\\sigma + \\epsilon) + \\beta$. Di sini $\\otimes$ menunjukkan perkalian *element-wise*.\n",
        "* **`compute_output_shape(self, batch_input_shape)`:**\n",
        "    * Untuk Normalisasi *Layer*, *shape output* sama dengan *shape input*.\n",
        "    * Keras seringkali dapat menyimpulkan bentuk *output* secara otomatis, tetapi menyediakannya secara eksplisit dapat membantu dalam kasus tertentu atau implementasi Keras lainnya.\n",
        "* **`get_config(self)`:**\n",
        "    * Metode ini memungkinkan *hyperparameter* *layer* (`epsilon`) untuk disimpan dan dimuat bersama dengan model.\n",
        "\n",
        "**Melatih Model Menggunakan *Custom Training Loop* untuk Dataset Fashion MNIST**:\n",
        "\n",
        "Pertama, siapkan dataset Fashion MNIST dan preprocess:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Muat dan siapkan dataset Fashion MNIST\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Pisahkan dataset menjadi training dan validation\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "# Flatten gambar\n",
        "X_train_scaled = X_train.reshape(-1, 28 * 28).astype(np.float32)\n",
        "X_valid_scaled = X_valid.reshape(-1, 28 * 28).astype(np.float32)\n",
        "X_test_scaled = X_test.reshape(-1, 28 * 28).astype(np.float32)\n",
        "\n",
        "# Buat model sederhana\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256, activation=\"relu\", input_shape=[28 * 28]),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Fungsi untuk mengambil batch acak\n",
        "def random_batch(X, y, batch_size=32):\n",
        "    idx = np.random.randint(len(X), size=batch_size)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "# Fungsi untuk menampilkan status bar (disederhanakan dari buku)\n",
        "def print_status_bar(iteration, total, loss, metrics=None):\n",
        "    metrics_str = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
        "                              for m in [loss] + (metrics or [])])\n",
        "    end = \"\" if iteration < total else \"\\n\"\n",
        "    print(f\"\\r{iteration}/{total} - {metrics_str}\", end=end)\n",
        "\n",
        "# Definisikan hyperparameter dan komponen training\n",
        "n_epochs = 10\n",
        "batch_size = 32\n",
        "n_steps_per_epoch = len(X_train_scaled) // batch_size\n",
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=learning_rate)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy() # Untuk klasifikasi, label int\n",
        "mean_loss = keras.metrics.Mean(name=\"mean_loss\")\n",
        "mean_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"mean_accuracy\")\n",
        "\n",
        "# Loop training kustom\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "    for step in range(1, n_steps_per_epoch + 1):\n",
        "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            main_loss = loss_fn(y_batch, y_pred)\n",
        "            loss = tf.add_n([main_loss] + model.losses) # Tambahkan regularization losses jika ada\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        # Update metrics for training\n",
        "        mean_loss(loss)\n",
        "        mean_accuracy(y_batch, y_pred)\n",
        "\n",
        "        # Tampilkan status bar\n",
        "        print_status_bar(step, n_steps_per_epoch, mean_loss, [mean_accuracy])\n",
        "\n",
        "    # Di akhir epoch, hitung validasi loss dan accuracy\n",
        "    # Reset state metrics training sebelum validasi (opsional, tergantung keinginan)\n",
        "    # mean_loss.reset_states()\n",
        "    # mean_accuracy.reset_states()\n",
        "\n",
        "    # Hitung validasi loss dan accuracy\n",
        "    val_loss = keras.metrics.Mean(name=\"val_loss\")\n",
        "    val_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"val_accuracy\")\n",
        "\n",
        "    # Iterate over validation set in batches\n",
        "    n_valid_steps = len(X_valid_scaled) // batch_size\n",
        "    for val_step in range(1, n_valid_steps + 1):\n",
        "        X_val_batch, y_val_batch = random_batch(X_valid_scaled, y_valid, batch_size)\n",
        "        y_val_pred = model(X_val_batch) # training=False by default for model()\n",
        "        val_main_loss = loss_fn(y_val_batch, y_val_pred)\n",
        "        val_loss(val_main_loss)\n",
        "        val_accuracy(y_val_batch, y_val_pred)\n",
        "\n",
        "    # Tampilkan hasil akhir epoch termasuk validasi\n",
        "    print_status_bar(n_steps_per_epoch, n_steps_per_epoch, mean_loss,\n",
        "                     [mean_accuracy, val_loss, val_accuracy])\n",
        "\n",
        "    # Reset metrics untuk epoch berikutnya\n",
        "    mean_loss.reset_states()\n",
        "    mean_accuracy.reset_states()\n",
        "    val_loss.reset_states()\n",
        "    val_accuracy.reset_states()\n",
        "```\n",
        "\n",
        "**Penjelasan Kode *Custom Training Loop*:**\n",
        "* **Persiapan Data dan Model:** Memuat dataset Fashion MNIST, melakukan *preprocessing* dasar (normalisasi dan *flattening*), dan mendefinisikan model Sequential sederhana.\n",
        "* **`random_batch()`:** Fungsi pembantu untuk mengambil *batch* data pelatihan secara acak.\n",
        "* **`print_status_bar()`:** Fungsi untuk menampilkan *progress training* pada satu baris.\n",
        "* **Definisi Hyperparameter dan Komponen Training:** Menentukan jumlah *epoch*, ukuran *batch*, *optimizer* (`Nadam`), fungsi *loss* (`SparseCategoricalCrossentropy` karena label adalah bilangan bulat), dan *metrics* (`Mean` untuk *loss* rata-rata, `SparseCategoricalAccuracy` untuk *accuracy* rata-rata).\n",
        "* **Loop Epoch:** Iterasi melalui setiap *epoch*.\n",
        "* **Loop Step (Batch):** Iterasi melalui setiap *batch* dalam satu *epoch*.\n",
        "    * **Pengambilan *Batch*:** Mengambil *batch* acak dari data pelatihan.\n",
        "    * **`tf.GradientTape()`:** Digunakan untuk merekam operasi yang melibatkan *variabel* yang dapat dilatih untuk perhitungan *gradient* otomatis.\n",
        "    * **Forward Pass:** Model dipanggil dengan *batch* input (`model(X_batch, training=True)`). `training=True` penting untuk *layer* yang berperilaku berbeda selama pelatihan (misalnya, `Dropout`, `BatchNormalization`).\n",
        "    * **Perhitungan *Loss*:** *Main loss* dihitung menggunakan `loss_fn`. Jika ada *regularization loss* dari *layer* (seperti `kernel_regularizer` dalam model), mereka ditambahkan ke *main loss* menggunakan `tf.add_n()` dan `model.losses`.\n",
        "    * **Perhitungan *Gradient*:** `tape.gradient()` menghitung *gradient* dari *loss* terhadap semua *variabel* model yang dapat dilatih (`model.trainable_variables`).\n",
        "    * **Penerapan *Gradient*:** `optimizer.apply_gradients()` menggunakan *gradient* untuk memperbarui *weight* model. `zip()` digunakan untuk memasangkan *gradient* dengan *variabel* yang sesuai.\n",
        "    * **Pembaruan Metrik:** Metrik `mean_loss` dan `mean_accuracy` diperbarui dengan hasil *batch* saat ini.\n",
        "    * **Tampilan Status:** `print_status_bar()` menampilkan *progress training*.\n",
        "* **Validasi (Akhir Epoch):**\n",
        "    * Setelah loop *training* *batch* selesai, *loss* dan *accuracy* validasi dihitung dengan cara yang sama.\n",
        "    * `model(X_val_batch)` secara *default* akan berjalan dalam mode *inference* (setara dengan `training=False`).\n",
        "    * Hasil validasi ditampilkan di *status bar*.\n",
        "* **Reset Metrik:** *State* dari semua metrik direset di akhir setiap *epoch* agar dapat menghitung metrik baru untuk *epoch* berikutnya.\n",
        "\n",
        "**Mencoba *Optimizer* Berbeda dengan *Learning Rate* Berbeda untuk *Upper* dan *Lower Layers***:\n",
        "\n",
        "Ini adalah skenario yang memerlukan *custom training loop* karena `model.compile()` hanya mendukung satu *optimizer*. Untuk mengimplementasikan ini, kita perlu membagi *variabel* yang dapat dilatih model menjadi kelompok *upper layer* dan *lower layer* dan menerapkan *optimizer* terpisah untuk setiap kelompok.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Muat dan siapkan dataset Fashion MNIST (sama seperti sebelumnya)\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "X_train_scaled = X_train.reshape(-1, 28 * 28).astype(np.float32)\n",
        "X_valid_scaled = X_valid.reshape(-1, 28 * 28).astype(np.float32)\n",
        "X_test_scaled = X_test.reshape(-1, 28 * 28).astype(np.float32)\n",
        "\n",
        "# Fungsi untuk mengambil batch acak\n",
        "def random_batch(X, y, batch_size=32):\n",
        "    idx = np.random.randint(len(X), size=batch_size)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "# Fungsi untuk menampilkan status bar (disederhanakan dari buku)\n",
        "def print_status_bar(iteration, total, loss, metrics=None):\n",
        "    metrics_str = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
        "                              for m in [loss] + (metrics or [])])\n",
        "    end = \"\" if iteration < total else \"\\n\"\n",
        "    print(f\"\\r{iteration}/{total} - {metrics_str}\", end=end)\n",
        "\n",
        "# Buat model (misalnya, dengan 3 Dense layers)\n",
        "# Kita akan menganggap layer pertama sebagai \"lower layer\" dan sisanya \"upper layers\"\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256, activation=\"relu\", input_shape=[28 * 28], name=\"lower_layer_1\"),\n",
        "    keras.layers.Dense(128, activation=\"relu\", name=\"upper_layer_1\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\", name=\"upper_output_layer\")\n",
        "])\n",
        "\n",
        "# Pisahkan variabel yang dapat dilatih berdasarkan layer\n",
        "lower_layers_vars = model.get_layer(\"lower_layer_1\").trainable_variables\n",
        "upper_layers_vars = []\n",
        "for layer_name in [\"upper_layer_1\", \"upper_output_layer\"]:\n",
        "    upper_layers_vars.extend(model.get_layer(layer_name).trainable_variables)\n",
        "\n",
        "print(\"Lower layer variables:\", [v.name for v in lower_layers_vars])\n",
        "print(\"Upper layers variables:\", [v.name for v in upper_layers_vars])\n",
        "\n",
        "\n",
        "# Definisikan hyperparameter dan komponen training\n",
        "n_epochs = 10\n",
        "batch_size = 32\n",
        "n_steps_per_epoch = len(X_train_scaled) // batch_size\n",
        "\n",
        "# Optimizer terpisah dengan learning rate berbeda\n",
        "lower_optimizer = keras.optimizers.Nadam(learning_rate=0.005) # Lebih kecil untuk lower layers\n",
        "upper_optimizer = keras.optimizers.Nadam(learning_rate=0.01) # Lebih besar untuk upper layers\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "mean_loss = keras.metrics.Mean(name=\"mean_loss\")\n",
        "mean_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"mean_accuracy\")\n",
        "\n",
        "# Loop training kustom\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "    for step in range(1, n_steps_per_epoch + 1):\n",
        "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape: # persistent=True karena kita akan memanggil tape.gradient dua kali\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            main_loss = loss_fn(y_batch, y_pred)\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "\n",
        "        # Hitung gradient untuk lower layers dan terapkan\n",
        "        lower_gradients = tape.gradient(loss, lower_layers_vars)\n",
        "        lower_optimizer.apply_gradients(zip(lower_gradients, lower_layers_vars))\n",
        "\n",
        "        # Hitung gradient untuk upper layers dan terapkan\n",
        "        upper_gradients = tape.gradient(loss, upper_layers_vars)\n",
        "        upper_optimizer.apply_gradients(zip(upper_gradients, upper_layers_vars))\n",
        "\n",
        "        del tape # Hapus tape setelah digunakan\n",
        "\n",
        "        # Update metrics for training\n",
        "        mean_loss(loss)\n",
        "        mean_accuracy(y_batch, y_pred)\n",
        "\n",
        "        # Tampilkan status bar\n",
        "        print_status_bar(step, n_steps_per_epoch, mean_loss, [mean_accuracy])\n",
        "\n",
        "    # Validasi di akhir epoch (sama seperti sebelumnya)\n",
        "    val_loss = keras.metrics.Mean(name=\"val_loss\")\n",
        "    val_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"val_accuracy\")\n",
        "\n",
        "    n_valid_steps = len(X_valid_scaled) // batch_size\n",
        "    for val_step in range(1, n_valid_steps + 1):\n",
        "        X_val_batch, y_val_batch = random_batch(X_valid_scaled, y_valid, batch_size)\n",
        "        y_val_pred = model(X_val_batch)\n",
        "        val_main_loss = loss_fn(y_val_batch, y_val_pred)\n",
        "        val_loss(val_main_loss)\n",
        "        val_accuracy(y_val_batch, y_val_pred)\n",
        "\n",
        "    print_status_bar(n_steps_per_epoch, n_steps_per_epoch, mean_loss,\n",
        "                     [mean_accuracy, val_loss, val_accuracy])\n",
        "\n",
        "    mean_loss.reset_states()\n",
        "    mean_accuracy.reset_states()\n",
        "    val_loss.reset_states()\n",
        "    val_accuracy.reset_states()\n",
        "```\n",
        "\n",
        "**Penjelasan Perubahan untuk *Multiple Optimizers***:\n",
        "* **Identifikasi *Trainable Variables*:** Kita perlu mengidentifikasi *variabel* mana yang termasuk dalam \"lower layers\" dan \"upper layers\". Di sini, saya memberikan nama pada *layer* Dense di model dan menggunakan `model.get_layer(name).trainable_variables` untuk mendapatkan *variabel* yang terkait.\n",
        "* **Dua *Optimizer*:** Buat dua *optimizer* terpisah, masing-masing dengan *learning rate* yang berbeda.\n",
        "* **`tf.GradientTape(persistent=True)`:** Karena kita perlu memanggil `tape.gradient()` lebih dari sekali (satu kali untuk *lower layers* dan satu kali untuk *upper layers*), kita harus mengatur `persistent=True` saat membuat `tf.GradientTape()`. Setelah kedua panggilan `gradient()`, penting untuk menghapus `tape` (`del tape`) untuk membebaskan sumber daya memori.\n",
        "* **Penerapan *Gradient* Terpisah:**\n",
        "    * `lower_gradients = tape.gradient(loss, lower_layers_vars)` menghitung *gradient* *loss* hanya terhadap *variabel* *lower layer*.\n",
        "    * `lower_optimizer.apply_gradients(zip(lower_gradients, lower_layers_vars))` kemudian menerapkan *gradient* ini menggunakan *optimizer* `lower_optimizer`.\n",
        "    * Proses yang sama diulang untuk *upper layers*.\n",
        "\n",
        "Dengan pendekatan ini, Anda mendapatkan kontrol granular atas proses *training*, memungkinkan skenario yang tidak didukung secara langsung oleh API `fit()` Keras. Namun, seperti yang disebutkan sebelumnya, kompleksitas dan potensi kesalahan meningkat secara signifikan."
      ],
      "metadata": {
        "id": "dkXEwcPbwBvE"
      }
    }
  ]
}